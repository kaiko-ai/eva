{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p> User Guide \u2022   Datasets \u2022   Reference API </p>"},{"location":"#introduction","title":"Introduction","text":"<p>Oncology FM Evaluation Framework by kaiko.ai</p> <p>With the first release, eva supports performance evaluation for vision Foundation Models (\"FMs\") and supervised machine learning models on WSI-patch-level image classification- and radiology (CT-scans) segmentation tasks.</p> <p>The goal of this project is to provide the open-source community with an easy-to-use framework that follows industry best practices to deliver a robust, reproducible and fair evaluation benchmark across FMs of different sizes and architectures.</p> <p>Support for additional modalities and tasks will be added in future releases.</p>"},{"location":"#use-cases","title":"Use cases","text":""},{"location":"#1-evaluate-your-own-fms-on-public-benchmark-datasets","title":"1. Evaluate your own FMs on public benchmark datasets","text":"<p>With a specified FM as input, you can run eva on several publicly available datasets &amp; tasks. One evaluation run will download and preprocess the relevant data, compute embeddings, fit and evaluate a downstream head and report the mean and standard deviation of the relevant performance metrics.</p> <p>Supported datasets &amp; tasks include:</p> <p>WSI patch-level pathology datasets</p> <ul> <li>Patch Camelyon: binary breast cancer classification</li> <li>BACH: multiclass breast cancer classification</li> <li>CRC: multiclass colorectal cancer classification</li> <li>MHIST: binary colorectal polyp cancer classification</li> </ul> <p>Radiology datasets</p> <ul> <li>TotalSegmentator: radiology/CT-scan for segmentation of anatomical structures</li> </ul> <p>More datasets &amp; downstream task types will be added in future releases.</p> <p>To evaluate FMs, eva provides support for different model-formats, including models trained with PyTorch, models available on HuggingFace and ONNX-models. For other formats custom wrappers can be implemented.</p>"},{"location":"#2-evaluate-ml-models-on-your-own-dataset-task","title":"2. Evaluate ML models on your own dataset &amp; task","text":"<p>If you have your own labelled dataset, all that is needed is to implement a dataset class tailored to your source data. Start from one of our out-of-the box provided dataset classes, adapt it to your data and run eva to see how different FMs perform on your task.</p>"},{"location":"#evaluation-results","title":"Evaluation results","text":"<p>We evaluated the following FMs on the 4 supported WSI-patch-level image classification tasks:</p> FM-backbone pretraining PCam - val* PCam - test* BACH - val* CRC - val* MHIST - val* DINO ViT-S16 N/A 0.765 (\u00b10.004) 0.726 (\u00b10.003) 0.416 (\u00b10.014) 0.643 (\u00b10.005) 0.551 (\u00b10.017) DINO ViT-S16 ImageNet 0.871 (\u00b10.004) 0.856 (\u00b10.005) 0.673 (\u00b10.005) 0.936 (\u00b10.001) 0.823 (\u00b10.006) DINO ViT-B8 ImageNet 0.872 (\u00b10.004) 0.854 (\u00b10.002) 0.704 (\u00b10.008) 0.942 (\u00b10.001) 0.813 (\u00b10.003) Lunit - ViT-S16 TCGA 0.89 (\u00b10.001) 0.897 (\u00b10.003) 0.765 (\u00b10.011) 0.936 (\u00b10.001) 0.762 (\u00b10.004) Owkin - iBOT ViT-B16 TCGA 0.914 (\u00b10.002) 0.919 (\u00b10.009) 0.717 (\u00b10.004) 0.938 (\u00b10.001) 0.799 (\u00b10.003) kaiko.ai - DINO ViT-S16 TCGA 0.911 (\u00b10.002) 0.899 (\u00b10.002) 0.773 (\u00b10.007) 0.954 (\u00b10.002) 0.829 (\u00b10.004) kaiko.ai - DINO ViT-B8 TCGA 0.902 (\u00b10.002) 0.887 (\u00b10.004) 0.798 (\u00b10.007) 0.950 (\u00b10.003) 0.803 (\u00b10.004) kaiko.ai - DINOv2 ViT-L14 TCGA 0.900 (\u00b10.002) 0.896 (\u00b10.001) 0.768 (\u00b10.006) 0.945 (\u00b10.001) 0.777 (\u00b10.008) <p>* Metric in table: Balanced Accuracy (for binary &amp; multiclass). The table shows the average performance &amp; standard deviation over 5 runs.</p> <p>The runs use the default setup described in the section below.</p> <p>eva trains the decoder on the \"train\" split and uses the \"validation\" split for monitoring, early stopping and checkpoint selection. Evaluation results are reported on the \"validation\" split and, if available, on the \"test\" split.</p> <p>For more details on the FM-backbones and instructions to replicate the results, please refer to the Replicate evaluations.</p>"},{"location":"#evaluation-setup","title":"Evaluation setup","text":"<p>Note that the current version of eva implements the task- &amp; model-independent and fixed default set up following the standard evaluation protocol proposed by [1] and described in the table below. We selected this approach to prioritize reliable, robust and fair FM-evaluation while being in line with common literature. Additionally, with future versions we are planning to allow the use of cross-validation and hyper-parameter tuning to find the optimal setup to achieve best possible performance on the implemented downstream tasks.</p> <p>With a provided FM, eva computes embeddings for all input images (WSI patches) which are then used to train a downstream head consisting of a single linear layer in a supervised setup for each of the benchmark datasets. We use early stopping with a patience of 5% of the maximal number of epochs.</p> Backbone frozen Hidden layers none Dropout 0.0 Activation function none Number of steps 12,500 Base Batch size 4,096 Batch size dataset specific* Base learning rate 0.01 Learning Rate [Base learning rate] * [Batch size] / [Base batch size] Max epochs [Number of samples] * [Number of steps] /  [Batch size] Early stopping 5% * [Max epochs] Optimizer SGD Momentum 0.9 Weight Decay 0.0 Nesterov momentum true LR Schedule Cosine without warmup <p>* For smaller datasets (e.g. BACH with 400 samples) we reduce the batch size to 256 and scale the learning rate accordingly.</p> <ul> <li>[1]: Virchow: A Million-Slide Digital Pathology Foundation Model, 2024</li> </ul>"},{"location":"#license","title":"License","text":"<p>eva is distributed under the terms of the Apache-2.0 license.</p>"},{"location":"#next-steps","title":"Next steps","text":"<p>Check out the User Guide to get started with eva</p> <p></p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or  advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic  address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a  professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at conduct@kaiko.ai. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"},{"location":"CONTRIBUTING/","title":"Contributing to eva","text":"<p>eva is open source and community contributions are welcome!</p>"},{"location":"CONTRIBUTING/#contribution-process","title":"Contribution Process","text":""},{"location":"CONTRIBUTING/#github-issues","title":"GitHub Issues","text":"<p>The eva contribution process generally starts with filing a GitHub issue.</p> <p>eva defines four categories of issues: feature requests, bug reports, documentation fixes, and installation issues. In general, we recommend waiting for feedback from a eva maintainer or community member before proceeding to implement a feature or patch.</p>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>After you have agreed upon an implementation strategy for your feature or patch with an eva maintainer, the next step is to introduce your changes as a pull request against the eva repository.</p> <p>Steps to make a pull request:</p> <ul> <li>Fork https://github.com/kaiko-ai/eva</li> <li>Implement your feature as a branch off of the <code>main</code> branch</li> <li>Create a pull request into the <code>main</code> branch of https://github.com/kaiko-ai/eva</li> </ul> <p>Once your pull request has been merged, your changes will be automatically included in the next eva release!</p>"},{"location":"DEVELOPER_GUIDE/","title":"Developer Guide","text":""},{"location":"DEVELOPER_GUIDE/#setting-up-a-dev-environment","title":"Setting up a DEV environment","text":"<p>We use PDM as a package and dependency manager. You can set up a local python environment for development as follows:  1. Install package and dependency manager PDM following the instructions here. 2. Install system dependencies     - For MacOS: <code>brew install Cmake</code>     - For Linux (Debian): <code>sudo apt-get install build-essential cmake</code> 3. Run <code>pdm install -G dev</code> to install the python dependencies. This will create a virtual environment in <code>eva/.venv</code>.</p>"},{"location":"DEVELOPER_GUIDE/#adding-new-dependencies","title":"Adding new dependencies","text":"<p>Add a new dependency to the <code>core</code> submodule: <code>pdm add &lt;package_name&gt;</code></p> <p>Add a new dependency to the <code>vision</code> submodule: <code>pdm add -G vision &lt;package_name&gt;</code></p> <p>After adding a new dependency, you also need to update the <code>pdm.lock</code> file: <code>pdm update</code></p> <p>For more information about managing dependencies please look here.</p>"},{"location":"DEVELOPER_GUIDE/#continuous-integration-ci","title":"Continuous Integration (CI)","text":"<p>For testing automation, we use <code>nox</code>.</p> <p>Installation: - with brew: <code>brew install nox</code> - with pip: <code>pip install --user --upgrade nox</code> (this way, you might need to run nox commands with <code>python -m nox</code> or specify an alias)</p> <p>Commands: - <code>nox</code> to run all the automation tests.  - <code>nox -s fmt</code> to run the code formatting tests. - <code>nox -s lint</code> to run the code lining tests. - <code>nox -s check</code> to run the type-annotation tests. - <code>nox -s test</code> to run the unit tests.   - <code>nox -s test -- tests/eva/metrics/test_average_loss.py</code> to run specific tests</p>"},{"location":"STYLE_GUIDE/","title":"eva Style Guide","text":"<p>This document contains our style guides used in <code>eva</code>.</p> <p>Our priority is consistency, so that developers can quickly ingest and understand the entire codebase without being distracted by style idiosyncrasies.</p>"},{"location":"STYLE_GUIDE/#general-coding-principles","title":"General coding principles","text":"<p>Q: How to keep code readable and maintainable? - Don't Repeat Yourself (DRY) - Use the lowest possible visibility for a variable or method (i.e. make private if possible) -- see Information Hiding / Encapsulation</p> <p>Q: How big should a function be? - Single Level of Abstraction Principle (SLAP) - High Cohesion and Low Coupling</p> <pre><code>TL;DR: functions should usually be quite small, and _do one thing_\n</code></pre>"},{"location":"STYLE_GUIDE/#python-style-guide","title":"Python Style Guide","text":"<p>In general we follow the following regulations: PEP8, the  Google Python Style Guide and we expect type hints/annotations.</p>"},{"location":"STYLE_GUIDE/#docstrings","title":"Docstrings","text":"<p>Our docstring style is derived from Google Python style.</p> <pre><code>def example_function(variable: int, optional: str | None = None) -&gt; str:\n    \"\"\"An example docstring that explains what this functions do.\n\n    Docs sections can be referenced via :ref:`custom text here &lt;anchor-link&gt;`.\n\n    Classes can be referenced via :class:`eva.data.datamodules.DataModule`.\n\n    Functions can be referenced via :func:`eva.data.datamodules.call.call_method_if_exists`.\n\n    Example:\n\n        &gt;&gt;&gt; from torch import nn\n        &gt;&gt;&gt; import eva\n        &gt;&gt;&gt; eva.models.modules.HeadModule(\n        &gt;&gt;&gt;     head=nn.Linear(10, 2),\n        &gt;&gt;&gt;     criterion=nn.CrossEntropyLoss(),\n        &gt;&gt;&gt; )\n\n    Args:\n        variable: A required argument.\n        optional: An optional argument.\n\n    Returns:\n        A description of the output string.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"STYLE_GUIDE/#module-docstrings","title":"Module docstrings","text":"<p>PEP-8 and PEP-257 indicate docstrings should have very specific syntax:</p> <pre><code>\"\"\"One line docstring that shouldn't wrap onto next line.\"\"\"\n</code></pre> <pre><code>\"\"\"First line of multiline docstring that shouldn't wrap.\n\nSubsequent line or paragraphs.\n\"\"\"\n</code></pre>"},{"location":"STYLE_GUIDE/#constants-docstrings","title":"Constants docstrings","text":"<p>Public constants should usually have docstrings. Optional on private constants. Docstrings on constants go underneath</p> <pre><code>SOME_CONSTANT = 3\n\"\"\"Either a single-line docstring or multiline as per above.\"\"\"\n</code></pre>"},{"location":"STYLE_GUIDE/#function-docstrings","title":"Function docstrings","text":"<p>All public functions should have docstrings following the pattern shown below.</p> <p>Each section can be omitted if there are no inputs, outputs, or no notable exceptions raised, respectively.</p> <pre><code>def fake_datamodule(\n    n_samples: int, random: bool = True\n) -&gt; eva.data.datamodules.DataModule:\n    \"\"\"Generates a fake DataModule.\n\n    It builds a :class:`eva.data.datamodules.DataModule` by generating\n    a fake dataset with generated data while fixing the seed. It can\n    be useful for debugging purposes.\n\n    Args:\n        n_samples: The number of samples of the generated datasets.\n        random: Whether to generated randomly.\n\n    Returns:\n        A :class:`eva.data.datamodules.DataModule` with generated random data.\n\n    Raises:\n        ValueError: If `n_samples` is `0`.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"STYLE_GUIDE/#class-docstrings","title":"Class docstrings","text":"<p>All public classes should have class docstrings following the pattern shown below.</p> <pre><code>class DataModule(pl.LightningDataModule):\n    \"\"\"DataModule encapsulates all the steps needed to process data.\n\n    It will initialize and create the mapping between dataloaders and\n    datasets. During the `prepare_data`, `setup` and `teardown`, the\n    datamodule will call the respectively methods from all the datasets,\n    given that they are defined.\n    \"\"\"\n\n    def __init__(\n        self,\n        datasets: schemas.DatasetsSchema | None = None,\n        dataloaders: schemas.DataloadersSchema | None = None,\n    ) -&gt; None:\n        \"\"\"Initializes the datamodule.\n\n        Args:\n            datasets: The desired datasets. Defaults to `None`.\n            dataloaders: The desired dataloaders. Defaults to `None`.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"datasets/","title":"Datasets","text":"<p>eva provides native support for several public datasets. When possible, the corresponding dataset classes facilitate automatic download to disk, if not possible, this documentation provides download instructions.</p>"},{"location":"datasets/#vision-datasets-overview","title":"Vision Datasets Overview","text":""},{"location":"datasets/#whole-slide-wsi-and-microscopy-image-datasets","title":"Whole Slide (WSI) and microscopy image datasets","text":"Dataset #Patches Patch Size Magnification (\u03bcm/px) Task Cancer Type BACH 400 2048x1536 20x (0.5) Classification (4 classes) Breast CRC 107,180 224x224 20x (0.5) Classification (9 classes) Colorectal PatchCamelyon 327,680 96x96 10x (1.0) * Classification (2 classes) Breast MHIST 3,152 224x224 5x (2.0) * Classification (2 classes) Colorectal Polyp <p>* Downsampled from 40x (0.25 \u03bcm/px) to increase the field of view.</p>"},{"location":"datasets/#radiology-datasets","title":"Radiology datasets","text":"Dataset #Images Image Size Task Download provided TotalSegmentator 1228 ~300 x ~300 x ~350 * Multilabel Classification (117 classes) Yes <p>* 3D images of varying sizes</p>"},{"location":"datasets/bach/","title":"BACH","text":"<p>The BACH dataset consists of microscopy and WSI images, of which we use only the microscopy images. These are 408 labelled images from 4 classes (\"Normal\", \"Benign\", \"Invasive\", \"InSitu\"). This dataset was used for the \"BACH Grand Challenge on Breast Cancer Histology images\".</p>"},{"location":"datasets/bach/#raw-data","title":"Raw data","text":""},{"location":"datasets/bach/#key-stats","title":"Key stats","text":"Modality Vision (microscopy images) Task Multiclass classification (4 classes) Cancer type Breast Data size total: 10.4GB / data in use: 7.37 GB (18.9 MB per image) Image dimension 1536 x 2048 x 3 Magnification (\u03bcm/px) 20x (0.42) Files format <code>.tif</code> images Number of images 408 (102 from each class) Splits in use one labelled split"},{"location":"datasets/bach/#organization","title":"Organization","text":"<p>The data <code>ICIAR2018_BACH_Challenge.zip</code> from zenodo is organized as follows:</p> <pre><code>ICAR2018_BACH_Challenge\n\u251c\u2500\u2500 Photos                    # All labelled patches used by eva\n\u2502   \u251c\u2500\u2500 Normal\n\u2502   \u2502   \u251c\u2500\u2500 n032.tif\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 Benign\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 Invasive\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 InSitu\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 WSI                       # WSIs, not in use\n\u2502   \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/bach/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The <code>BACH</code> dataset class supports downloading the data during runtime by setting the init argument <code>download=True</code>.</p> <p>Note that in the provided <code>BACH</code>-config files the download argument is set to <code>false</code>. To enable automatic download you will need to open the config and set <code>download: true</code>.</p> <p>The splits are created from the indices specified in the BACH dataset class. These indices were picked to prevent data  leakage due to images belonging to the same patient. Because the small dataset in combination with the patient ID constraint  does not allow to split the data three-ways with sufficient amount of data in each split, we only create a train and val  split and leave it to the user to submit predictions on the official test split to the BACH Challenge Leaderboard.</p> Splits Train Validation #Samples 268 (67%) 132 (33%)"},{"location":"datasets/bach/#relevant-links","title":"Relevant links","text":"<ul> <li>BACH dataset on zenodo</li> <li>BACH Challenge website</li> <li>BACH Challenge Leaderboard</li> <li>Patient ID information (Link provided on BACH challenge website)</li> <li>Reference API Vision dataset classes</li> </ul>"},{"location":"datasets/bach/#license","title":"License","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International</p>"},{"location":"datasets/crc/","title":"CRC","text":"<p>The CRC-HE dataset consists of labelled patches (9 classes) from colorectal cancer (CRC) and normal tissue. We use the <code>NCT-CRC-HE-100K</code> dataset for training and validation and the <code>CRC-VAL-HE-7K for testing</code>. </p> <p>The <code>NCT-CRC-HE-100K-NONORM</code> consists of 100,000 images without applied color normalization. The <code>CRC-VAL-HE-7K</code> consists of 7,180 image patches from 50 patients without overlap with <code>NCT-CRC-HE-100K-NONORM</code>.</p> <p>The tissue classes are: Adipose (ADI), background (BACK), debris (DEB), lymphocytes (LYM), mucus (MUC), smooth muscle (MUS), normal colon mucosa (NORM), cancer-associated stroma (STR) and colorectal adenocarcinoma epithelium (TUM)</p>"},{"location":"datasets/crc/#raw-data","title":"Raw data","text":""},{"location":"datasets/crc/#key-stats","title":"Key stats","text":"Modality Vision (WSI patches) Task Multiclass classification (9 classes) Cancer type Colorectal Data size total: 11.7GB (train), 800MB (val) Image dimension 224 x 224 x 3 Magnification (\u03bcm/px) 20x (0.5) Files format <code>.tif</code> images Number of images 107,180 (100k train, 7.2k val) Splits in use NCT-CRC-HE-100K (train), CRC-VAL-HE-7K (val)"},{"location":"datasets/crc/#splits","title":"Splits","text":"<p>We use the splits according to the data sources:</p> <ul> <li>Train split: <code>NCT-CRC-HE-100K</code></li> <li>Validation split: <code>CRC-VAL-HE-7K</code></li> </ul> Splits Train Validation #Samples 100,000 (93.3%) 7,180 (6.7%) <p>A test split is not provided. Because the patient information for the training data is not available, dividing the  training data in a train/val split (and using the given val split as test split) is not possible without risking data leakage. eva therefore reports evaluation results for CRC HE on the validation split.</p>"},{"location":"datasets/crc/#organization","title":"Organization","text":"<p>The data <code>NCT-CRC-HE-100K.zip</code>, <code>NCT-CRC-HE-100K-NONORM.zip</code> and <code>CRC-VAL-HE-7K.zip</code> from zenodo are organized as follows:</p> <pre><code>NCT-CRC-HE-100K                # All images used for training\n\u251c\u2500\u2500 ADI                        # All labelled patches belonging to the 1st class\n\u2502   \u251c\u2500\u2500 ADI-AAAFLCLY.tif\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 BACK                       # All labelled patches belonging to the 2nd class\n\u2502   \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n\nNCT-CRC-HE-100K-NONORM         # All images used for training\n\u251c\u2500\u2500 ADI                        # All labelled patches belonging to the 1st class\n\u2502   \u251c\u2500\u2500 ADI-AAAFLCLY.tif\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 BACK                       # All labelled patches belonging to the 2nd class\n\u2502   \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n\nCRC-VAL-HE-7K                  # All images used for validation\n\u251c\u2500\u2500 ...                        # identical structure as for NCT-CRC-HE-100K-NONORM\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/crc/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The <code>CRC</code> dataset class supports downloading the data during runtime by setting the init argument <code>download=True</code>.</p> <p>Note that in the provided <code>CRC</code>-config files the download argument is set to <code>false</code>. To enable automatic download you will need to open the config and set <code>download: true</code>.</p>"},{"location":"datasets/crc/#relevant-links","title":"Relevant links","text":"<ul> <li>CRC datasets on zenodo</li> <li>Reference API Vision dataset classes</li> </ul>"},{"location":"datasets/crc/#license","title":"License","text":"<p>CC BY 4.0 LEGAL CODE</p>"},{"location":"datasets/mhist/","title":"MHIST","text":"<p>MHIST is a binary classification task which comprises of 3,152 hematoxylin and eosin (H&amp;E)-stained Formalin Fixed Paraffin-Embedded (FFPE) fixed-size images (224 by 224 pixels) of colorectal polyps from the Department of Pathology and Laboratory Medicine at Dartmouth-Hitchcock Medical Center (DHMC).</p> <p>The tissue classes are: Hyperplastic Polyp (HP), Sessile Serrated Adenoma (SSA). This classification task focuses on the clinically-important binary distinction between HPs and SSAs, a challenging problem with considerable inter-pathologist variability. HPs are typically benign, while sessile serrated adenomas are precancerous lesions that can turn into cancer if left untreated and require sooner follow-up examinations. Histologically, HPs have a superficial serrated architecture and elongated crypts, whereas SSAs are characterized by broad-based crypts, often with complex structure and heavy serration.</p>"},{"location":"datasets/mhist/#raw-data","title":"Raw data","text":""},{"location":"datasets/mhist/#key-stats","title":"Key stats","text":"Modality Vision (WSI patches) Task Binary classification (2 classes) Cancer type Colorectal Polyp Data size 354 MB Image dimension 224 x 224 x 3 Magnification (\u03bcm/px) 5x (2.0) * Files format <code>.png</code> images Number of images 3,152 (2,175 train, 977 test) Splits in use annotations.csv (train / test) <p>* Downsampled from 40x to increase the field of view.</p>"},{"location":"datasets/mhist/#organization","title":"Organization","text":"<p>The contents from <code>images.zip</code> and the file <code>annotations.csv</code> from bmirds are organized as follows:</p> <pre><code>mhist                           # Root folder\n\u251c\u2500\u2500 images                      # All the dataset images\n\u2502   \u251c\u2500\u2500 MHIST_aaa.png\n\u2502   \u251c\u2500\u2500 MHIST_aab.png\n\u2502   \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 annotations.csv             # The dataset annotations file\n</code></pre>"},{"location":"datasets/mhist/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>To download the dataset, please visit the access portal on BMIRDS and follow the instructions. You will then receive an email with all the relative links that you can use to download the data (<code>images.zip</code>, <code>annotations.csv</code>, <code>Dataset Research Use Agreement.pdf</code> and <code>MD5SUMs.txt</code>). </p> <p>Please create a root folder, e.g. <code>mhist</code>, and download all the files there, which unzipping the contents of <code>images.zip</code> to a directory named <code>images</code> inside your root folder (i.e. <code>mhist/images</code>). Afterwards, you can (optionally) delete the <code>images.zip</code> file.</p>"},{"location":"datasets/mhist/#splits","title":"Splits","text":"<p>We work with the splits provided by the data source. Since no \"validation\" split is provided, we use the \"test\" split as validation split.</p> <ul> <li>Train split: <code>annotations.csv</code> :: \"Partition\" == \"train\"</li> <li>Validation split: <code>annotations.csv</code> :: \"Partition\" == \"test\"</li> </ul> Splits Train Validation #Samples 2,175 (69%) 977 (31%)"},{"location":"datasets/mhist/#relevant-links","title":"Relevant links","text":"<ul> <li>Accessing MHIST Dataset (BMIRDS)</li> <li>Paper: A Petri Dish for Histopathology Image Analysis</li> </ul>"},{"location":"datasets/patch_camelyon/","title":"PatchCamelyon","text":"<p>The PatchCamelyon benchmark is a image classification dataset with 327,680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue.</p>"},{"location":"datasets/patch_camelyon/#raw-data","title":"Raw data","text":""},{"location":"datasets/patch_camelyon/#key-stats","title":"Key stats","text":"Modality Vision (WSI patches) Task Binary classification Cancer type Breast Data size 8 GB Image dimension 96 x 96 x 3 Magnification (\u03bcm/px) 10x (1.0) * Files format <code>h5</code> Number of images 327,680 (50% of each class) <p>* The slides were acquired and digitized at 2 different medical centers using a 40x objective but under-sampled to 10x to increase the field of view.</p>"},{"location":"datasets/patch_camelyon/#splits","title":"Splits","text":"<p>The datasource provides train/validation/test splits</p> Splits Train Validation Test #Samples 262,144 (80%) 32,768 (10%) 32,768 (10%)"},{"location":"datasets/patch_camelyon/#organization","title":"Organization","text":"<p>The PatchCamelyon data from zenodo is organized as follows:</p> <pre><code>\u251c\u2500\u2500 camelyonpatch_level_2_split_train_x.h5.gz               # train images\n\u251c\u2500\u2500 camelyonpatch_level_2_split_train_y.h5.gz               # train labels\n\u251c\u2500\u2500 camelyonpatch_level_2_split_valid_x.h5.gz               # val images\n\u251c\u2500\u2500 camelyonpatch_level_2_split_valid_y.h5.gz               # val labels\n\u251c\u2500\u2500 camelyonpatch_level_2_split_test_x.h5.gz                # test images\n\u251c\u2500\u2500 camelyonpatch_level_2_split_test_y.h5.gz                # test labels\n</code></pre>"},{"location":"datasets/patch_camelyon/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The dataset class <code>PatchCamelyon</code> supports downloading the data during runtime by setting the init argument <code>download=True</code>.</p> <p>Note that in the provided <code>PatchCamelyon</code>-config files the download argument is set to <code>false</code>. To enable automatic download you will need to open the config and set <code>download: true</code>.</p> <p>Labels are provided by source files, splits are given by file names.</p>"},{"location":"datasets/patch_camelyon/#relevant-links","title":"Relevant links","text":"<ul> <li>PatchCamelyon dataset on zenodo</li> <li>GitHub repository</li> <li>Reference API Vision dataset classes</li> </ul>"},{"location":"datasets/patch_camelyon/#citation","title":"Citation","text":"<pre><code>@misc{b_s_veeling_j_linmans_j_winkens_t_cohen_2018_2546921,\n  author       = {B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling},\n  title        = {Rotation Equivariant CNNs for Digital Pathology},\n  month        = sep,\n  year         = 2018,\n  doi          = {10.1007/978-3-030-00934-2_24},\n  url          = {https://doi.org/10.1007/978-3-030-00934-2_24}\n}\n</code></pre>"},{"location":"datasets/patch_camelyon/#license","title":"License","text":"<p>Creative Commons Zero v1.0 Universal</p>"},{"location":"datasets/total_segmentator/","title":"TotalSegmentator","text":"<p>The TotalSegmentator dataset is a radiology image-segmentation dataset with 1228 3D images and corresponding masks with 117 different anatomical structures. It can be used for segmentation and multilabel classification tasks.</p>"},{"location":"datasets/total_segmentator/#raw-data","title":"Raw data","text":""},{"location":"datasets/total_segmentator/#key-stats","title":"Key stats","text":"Modality Vision (radiology, CT scans) Task Segmentation / multilabel classification (117 classes) Data size total: 23.6GB Image dimension ~300 x ~300 x ~350 (number of slices) x 1 (grey scale) * Files format <code>.nii</code> (\"NIFTI\") images Number of images 1228 Splits in use one labelled split <p>/* image resolution and number of slices per image vary</p>"},{"location":"datasets/total_segmentator/#organization","title":"Organization","text":"<p>The data <code>Totalsegmentator_dataset_v201.zip</code> from zenodo is organized as follows:</p> <pre><code>Totalsegmentator_dataset_v201\n\u251c\u2500\u2500 s0011                               # one image\n\u2502   \u251c\u2500\u2500 ct.nii.gz                       # CT scan\n\u2502   \u251c\u2500\u2500 segmentations                   # directory with segmentation masks\n\u2502   \u2502   \u251c\u2500\u2500 adrenal_gland_left.nii.gz   # segmentation mask 1st anatomical structure\n\u2502   \u2502   \u251c\u2500\u2500 adrenal_gland_right.nii.gz  # segmentation mask 2nd anatomical structure\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/total_segmentator/#download-and-preprocessing","title":"Download and preprocessing","text":"<ul> <li>The dataset class <code>TotalSegmentator</code> supports download the data on runtime with the initialized argument <code>download: bool = True</code>. </li> <li>For the multilabel classification task, every mask with at least one positive pixel is gets the label \"1\", all others get the label \"0\".</li> <li>For the multilabel classification task, the <code>TotalSegmentator</code> class creates a manifest file with one row/slice and the columns: <code>path</code>, <code>slice</code>, <code>split</code> and additional 117 columns for each class.</li> <li>The 3D images are treated as 2D. Every 25th slice is sampled and treated as individual image</li> <li>The splits with the following sizes are created after ordering images by filename:</li> </ul> Splits Train Validation Test #Samples 737 (60%) 246 (20%) 245 (20%)"},{"location":"datasets/total_segmentator/#relevant-links","title":"Relevant links","text":"<ul> <li>TotalSegmentator dataset on zenodo</li> <li>TotalSegmentator small subset (102 images) on zenodo</li> <li>Reference API TotalSegmentator dataset class</li> </ul>"},{"location":"datasets/total_segmentator/#license","title":"License","text":"<p>Creative Commons Attribution 4.0 International</p>"},{"location":"reference/","title":"Reference API","text":"<p>Here is the Reference API, describing the classes, functions, parameters and attributes of the eva package.</p> <p>To learn how to use eva, however, its best to get started with the User Guide</p>"},{"location":"reference/core/callbacks/","title":"Callbacks","text":""},{"location":"reference/core/callbacks/#writers","title":"Writers","text":""},{"location":"reference/core/callbacks/#eva.core.callbacks.writers.EmbeddingsWriter","title":"<code>eva.core.callbacks.writers.EmbeddingsWriter</code>","text":"<p>             Bases: <code>BasePredictionWriter</code></p> <p>Callback for writing generated embeddings to disk.</p> <p>This callback writes the embedding files in a separate process to avoid blocking the main process where the model forward pass is executed.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>The directory where the embeddings will be saved.</p> required <code>backbone</code> <code>Module | None</code> <p>A model to be used as feature extractor. If <code>None</code>, it will be expected that the input batch returns the features directly.</p> <code>None</code> <code>dataloader_idx_map</code> <code>Dict[int, str] | None</code> <p>A dictionary mapping dataloader indices to their respective names (e.g. train, val, test).</p> <code>None</code> <code>group_key</code> <code>str | None</code> <p>The metadata key to group the embeddings by. If specified, the embedding files will be saved in subdirectories named after the group_key. If specified, the key must be present in the metadata of the input batch.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output directory. Defaults to True.</p> <code>True</code> Source code in <code>src/eva/core/callbacks/writers/embeddings.py</code> <pre><code>def __init__(\n    self,\n    output_dir: str,\n    backbone: nn.Module | None = None,\n    dataloader_idx_map: Dict[int, str] | None = None,\n    group_key: str | None = None,\n    overwrite: bool = True,\n) -&gt; None:\n    \"\"\"Initializes a new EmbeddingsWriter instance.\n\n    This callback writes the embedding files in a separate process to avoid blocking the\n    main process where the model forward pass is executed.\n\n    Args:\n        output_dir: The directory where the embeddings will be saved.\n        backbone: A model to be used as feature extractor. If `None`,\n            it will be expected that the input batch returns the features directly.\n        dataloader_idx_map: A dictionary mapping dataloader indices to their respective\n            names (e.g. train, val, test).\n        group_key: The metadata key to group the embeddings by. If specified, the\n            embedding files will be saved in subdirectories named after the group_key.\n            If specified, the key must be present in the metadata of the input batch.\n        overwrite: Whether to overwrite the output directory. Defaults to True.\n    \"\"\"\n    super().__init__(write_interval=\"batch\")\n\n    self._output_dir = output_dir\n    self._backbone = backbone\n    self._dataloader_idx_map = dataloader_idx_map or {}\n    self._group_key = group_key\n    self._overwrite = overwrite\n\n    self._write_queue: multiprocessing.Queue\n    self._write_process: eva_multiprocessing.Process\n</code></pre>"},{"location":"reference/core/interface/","title":"Interface API","text":"<p>Reference information for the <code>Interface</code> API.</p>"},{"location":"reference/core/interface/#eva.Interface","title":"<code>eva.Interface</code>","text":"<p>A high-level interface for training and validating a machine learning model.</p> <p>This class provides a convenient interface to connect a model, data, and trainer to train and validate a model.</p>"},{"location":"reference/core/interface/#eva.Interface.fit","title":"<code>fit</code>","text":"<p>Perform model training and evaluation out-of-place.</p> <p>This method uses the specified trainer to fit the model using the provided data.</p> <p>Example use cases:</p> <ul> <li>Using a model consisting of a frozen backbone and a head, the backbone will generate   the embeddings on the fly which are then used as input features to train the head on   the downstream task specified by the given dataset.</li> <li>Fitting only the head network using a dataset that loads pre-computed embeddings.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The base trainer to use but not modify.</p> required <code>model</code> <code>ModelModule</code> <p>The model module to use but not modify.</p> required <code>data</code> <code>DataModule</code> <p>The data module.</p> required Source code in <code>src/eva/core/interface/interface.py</code> <pre><code>def fit(\n    self,\n    trainer: eva_trainer.Trainer,\n    model: modules.ModelModule,\n    data: datamodules.DataModule,\n) -&gt; None:\n    \"\"\"Perform model training and evaluation out-of-place.\n\n    This method uses the specified trainer to fit the model using the provided data.\n\n    Example use cases:\n\n    - Using a model consisting of a frozen backbone and a head, the backbone will generate\n      the embeddings on the fly which are then used as input features to train the head on\n      the downstream task specified by the given dataset.\n    - Fitting only the head network using a dataset that loads pre-computed embeddings.\n\n    Args:\n        trainer: The base trainer to use but not modify.\n        model: The model module to use but not modify.\n        data: The data module.\n    \"\"\"\n    trainer.run_evaluation_session(model=model, datamodule=data)\n</code></pre>"},{"location":"reference/core/interface/#eva.Interface.predict","title":"<code>predict</code>","text":"<p>Perform model prediction out-of-place.</p> <p>This method performs inference with a pre-trained foundation model to compute embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The base trainer to use but not modify.</p> required <code>model</code> <code>ModelModule</code> <p>The model module to use but not modify.</p> required <code>data</code> <code>DataModule</code> <p>The data module.</p> required Source code in <code>src/eva/core/interface/interface.py</code> <pre><code>def predict(\n    self,\n    trainer: eva_trainer.Trainer,\n    model: modules.ModelModule,\n    data: datamodules.DataModule,\n) -&gt; None:\n    \"\"\"Perform model prediction out-of-place.\n\n    This method performs inference with a pre-trained foundation model to compute embeddings.\n\n    Args:\n        trainer: The base trainer to use but not modify.\n        model: The model module to use but not modify.\n        data: The data module.\n    \"\"\"\n    eva_trainer.infer_model(\n        base_trainer=trainer,\n        base_model=model,\n        datamodule=data,\n        return_predictions=False,\n    )\n</code></pre>"},{"location":"reference/core/interface/#eva.Interface.predict_fit","title":"<code>predict_fit</code>","text":"<p>Combines the predict and fit commands in one method.</p> <p>This method performs the following two steps: 1. predict: perform inference with a pre-trained foundation model to compute embeddings. 2. fit: training the head network using the embeddings generated in step 1.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The base trainer to use but not modify.</p> required <code>model</code> <code>ModelModule</code> <p>The model module to use but not modify.</p> required <code>data</code> <code>DataModule</code> <p>The data module.</p> required Source code in <code>src/eva/core/interface/interface.py</code> <pre><code>def predict_fit(\n    self,\n    trainer: eva_trainer.Trainer,\n    model: modules.ModelModule,\n    data: datamodules.DataModule,\n) -&gt; None:\n    \"\"\"Combines the predict and fit commands in one method.\n\n    This method performs the following two steps:\n    1. predict: perform inference with a pre-trained foundation model to compute embeddings.\n    2. fit: training the head network using the embeddings generated in step 1.\n\n    Args:\n        trainer: The base trainer to use but not modify.\n        model: The model module to use but not modify.\n        data: The data module.\n    \"\"\"\n    self.predict(trainer=trainer, model=model, data=data)\n    self.fit(trainer=trainer, model=model, data=data)\n</code></pre>"},{"location":"reference/core/data/dataloaders/","title":"Dataloaders","text":"<p>Reference information for the <code>Dataloader</code> classes.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader","title":"<code>eva.data.DataLoader</code>  <code>dataclass</code>","text":"<p>The <code>DataLoader</code> combines a dataset and a sampler.</p> <p>It provides an iterable over the given dataset.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.batch_size","title":"<code>batch_size: int | None = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How many samples per batch to load.</p> <p>Set to <code>None</code> for iterable dataset where dataset produces batches.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.shuffle","title":"<code>shuffle: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to shuffle the data at every epoch.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.sampler","title":"<code>sampler: samplers.Sampler | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Defines the strategy to draw samples from the dataset.</p> <p>Can be any Iterable with <code>__len__</code> implemented. If specified, shuffle must not be specified.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.batch_sampler","title":"<code>batch_sampler: samplers.Sampler | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Like <code>sampler</code>, but returns a batch of indices at a time.</p> <p>Mutually exclusive with <code>batch_size</code>, <code>shuffle</code>, <code>sampler</code> and <code>drop_last</code>.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.num_workers","title":"<code>num_workers: int = multiprocessing.cpu_count()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How many workers to use for loading the data.</p> <p>By default, it will use the number of CPUs available.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.collate_fn","title":"<code>collate_fn: Callable | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The batching process.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.pin_memory","title":"<code>pin_memory: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Will copy Tensors into CUDA pinned memory before returning them.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.drop_last","title":"<code>drop_last: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Drops the last incomplete batch.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.persistent_workers","title":"<code>persistent_workers: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Will keep the worker processes after a dataset has been consumed once.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.prefetch_factor","title":"<code>prefetch_factor: int | None = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches loaded in advance by each worker.</p>"},{"location":"reference/core/data/datamodules/","title":"Datamodules","text":"<p>Reference information for the <code>Datamodule</code> classes and functions.</p>"},{"location":"reference/core/data/datamodules/#eva.data.DataModule","title":"<code>eva.data.DataModule</code>","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>DataModule encapsulates all the steps needed to process data.</p> <p>It will initialize and create the mapping between dataloaders and datasets. During the <code>prepare_data</code>, <code>setup</code> and <code>teardown</code>, the datamodule will call the respective methods from all datasets, given that they are defined.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>DatasetsSchema | None</code> <p>The desired datasets.</p> <code>None</code> <code>dataloaders</code> <code>DataloadersSchema | None</code> <p>The desired dataloaders.</p> <code>None</code> Source code in <code>src/eva/core/data/datamodules/datamodule.py</code> <pre><code>def __init__(\n    self,\n    datasets: schemas.DatasetsSchema | None = None,\n    dataloaders: schemas.DataloadersSchema | None = None,\n) -&gt; None:\n    \"\"\"Initializes the datamodule.\n\n    Args:\n        datasets: The desired datasets.\n        dataloaders: The desired dataloaders.\n    \"\"\"\n    super().__init__()\n\n    self.datasets = datasets or self.default_datasets\n    self.dataloaders = dataloaders or self.default_dataloaders\n</code></pre>"},{"location":"reference/core/data/datamodules/#eva.data.DataModule.default_datasets","title":"<code>default_datasets: schemas.DatasetsSchema</code>  <code>property</code>","text":"<p>Returns the default datasets.</p>"},{"location":"reference/core/data/datamodules/#eva.data.DataModule.default_dataloaders","title":"<code>default_dataloaders: schemas.DataloadersSchema</code>  <code>property</code>","text":"<p>Returns the default dataloader schema.</p>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.call.call_method_if_exists","title":"<code>eva.data.datamodules.call.call_method_if_exists</code>","text":"<p>Calls a desired <code>method</code> from the datasets if exists.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>Iterable[Any]</code> <p>An iterable of objects.</p> required <code>method</code> <code>str</code> <p>The dataset method name to call if exists.</p> required Source code in <code>src/eva/core/data/datamodules/call.py</code> <pre><code>def call_method_if_exists(objects: Iterable[Any], /, method: str) -&gt; None:\n    \"\"\"Calls a desired `method` from the datasets if exists.\n\n    Args:\n        objects: An iterable of objects.\n        method: The dataset method name to call if exists.\n    \"\"\"\n    for _object in _recursive_iter(objects):\n        if hasattr(_object, method):\n            fn = getattr(_object, method)\n            fn()\n</code></pre>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.schemas.DatasetsSchema","title":"<code>eva.data.datamodules.schemas.DatasetsSchema</code>  <code>dataclass</code>","text":"<p>Datasets schema used in DataModule.</p>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.schemas.DatasetsSchema.train","title":"<code>train: TRAIN_DATASET = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Train dataset.</p>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.schemas.DatasetsSchema.val","title":"<code>val: EVAL_DATASET = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Validation dataset.</p>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.schemas.DatasetsSchema.test","title":"<code>test: EVAL_DATASET = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Test dataset.</p>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.schemas.DatasetsSchema.predict","title":"<code>predict: EVAL_DATASET = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Predict dataset.</p>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.schemas.DatasetsSchema.tolist","title":"<code>tolist</code>","text":"<p>Returns the dataclass as a list and optionally filters it given the stage.</p> Source code in <code>src/eva/core/data/datamodules/schemas.py</code> <pre><code>def tolist(self, stage: str | None = None) -&gt; List[EVAL_DATASET]:\n    \"\"\"Returns the dataclass as a list and optionally filters it given the stage.\"\"\"\n    match stage:\n        case \"fit\":\n            return [self.train, self.val]\n        case \"validate\":\n            return [self.val]\n        case \"test\":\n            return [self.test]\n        case \"predict\":\n            return [self.predict]\n        case None:\n            return [self.train, self.val, self.test, self.predict]\n        case _:\n            raise ValueError(f\"Invalid stage `{stage}`.\")\n</code></pre>"},{"location":"reference/core/data/datasets/","title":"Datasets","text":"<p>Reference information for the <code>Dataset</code> base class.</p>"},{"location":"reference/core/data/datasets/#eva.data.Dataset","title":"<code>eva.data.Dataset</code>","text":"<p>             Bases: <code>TorchDataset</code></p> <p>Base dataset class.</p>"},{"location":"reference/core/data/datasets/#eva.data.Dataset.prepare_data","title":"<code>prepare_data</code>","text":"<p>Encapsulates all disk related tasks.</p> <p>This method is preferred for downloading and preparing the data, for example generate manifest files. If implemented, it will be called via :class:<code>eva.core.data.datamodules.DataModule</code>, which ensures that is called only within a single process, making it multi-processes safe.</p> Source code in <code>src/eva/core/data/datasets/base.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Encapsulates all disk related tasks.\n\n    This method is preferred for downloading and preparing the data, for\n    example generate manifest files. If implemented, it will be called via\n    :class:`eva.core.data.datamodules.DataModule`, which ensures that is called\n    only within a single process, making it multi-processes safe.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/data/datasets/#eva.data.Dataset.setup","title":"<code>setup</code>","text":"<p>Setups the dataset.</p> <p>This method is preferred for creating datasets or performing train/val/test splits. If implemented, it will be called via :class:<code>eva.core.data.datamodules.DataModule</code> at the beginning of fit (train + validate), validate, test, or predict and it will be called from every process (i.e. GPU) across all the nodes in DDP.</p> Source code in <code>src/eva/core/data/datasets/base.py</code> <pre><code>def setup(self) -&gt; None:\n    \"\"\"Setups the dataset.\n\n    This method is preferred for creating datasets or performing\n    train/val/test splits. If implemented, it will be called via\n    :class:`eva.core.data.datamodules.DataModule` at the beginning of fit\n    (train + validate), validate, test, or predict and it will be called\n    from every process (i.e. GPU) across all the nodes in DDP.\n    \"\"\"\n    self.configure()\n    self.validate()\n</code></pre>"},{"location":"reference/core/data/datasets/#eva.data.Dataset.configure","title":"<code>configure</code>","text":"<p>Configures the dataset.</p> <p>This method is preferred to configure the dataset; assign values to attributes, perform splits etc. This would be called from the method ::method::<code>setup</code>, before calling the ::method::<code>validate</code>.</p> Source code in <code>src/eva/core/data/datasets/base.py</code> <pre><code>def configure(self):\n    \"\"\"Configures the dataset.\n\n    This method is preferred to configure the dataset; assign values\n    to attributes, perform splits etc. This would be called from the\n    method ::method::`setup`, before calling the ::method::`validate`.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/data/datasets/#eva.data.Dataset.validate","title":"<code>validate</code>","text":"<p>Validates the dataset.</p> <p>This method aims to check the integrity of the dataset and verify that is configured properly. This would be called from the method ::method::<code>setup</code>, after calling the ::method::<code>configure</code>.</p> Source code in <code>src/eva/core/data/datasets/base.py</code> <pre><code>def validate(self):\n    \"\"\"Validates the dataset.\n\n    This method aims to check the integrity of the dataset and verify\n    that is configured properly. This would be called from the method\n    ::method::`setup`, after calling the ::method::`configure`.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/data/datasets/#eva.data.Dataset.teardown","title":"<code>teardown</code>","text":"<p>Cleans up the data artifacts.</p> <p>Used to clean-up when the run is finished. If implemented, it will be called via :class:<code>eva.core.data.datamodules.DataModule</code> at the end of fit (train + validate), validate, test, or predict and it will be called from every process (i.e. GPU) across all the nodes in DDP.</p> Source code in <code>src/eva/core/data/datasets/base.py</code> <pre><code>def teardown(self) -&gt; None:\n    \"\"\"Cleans up the data artifacts.\n\n    Used to clean-up when the run is finished. If implemented, it will\n    be called via :class:`eva.core.data.datamodules.DataModule` at the end\n    of fit (train + validate), validate, test, or predict and it will be\n    called from every process (i.e. GPU) across all the nodes in DDP.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/data/datasets/#embeddings-datasets","title":"Embeddings datasets","text":""},{"location":"reference/core/data/datasets/#eva.core.data.datasets.EmbeddingsClassificationDataset","title":"<code>eva.core.data.datasets.EmbeddingsClassificationDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Embeddings classification dataset.</p> <p>Expects a manifest file listing the paths of .pt files that contain tensor embeddings of shape [embedding_dim] or [1, embedding_dim].</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>manifest_file</code> <code>str</code> <p>The path to the manifest file, which is relative to the <code>root</code> argument.</p> required <code>split</code> <code>str | None</code> <p>The dataset split to use. The <code>split</code> column of the manifest file will be splitted based on this value.</p> <code>None</code> <code>column_mapping</code> <code>Dict[str, str]</code> <p>Defines the map between the variables and the manifest columns. It will overwrite the <code>default_column_mapping</code> with the provided values, so that <code>column_mapping</code> can contain only the values which are altered or missing.</p> <code>default_column_mapping</code> <code>embeddings_transforms</code> <code>Callable | None</code> <p>A function/transform that transforms the embedding.</p> <code>None</code> <code>target_transforms</code> <code>Callable | None</code> <p>A function/transform that transforms the target.</p> <code>None</code> Source code in <code>src/eva/core/data/datasets/classification/embeddings.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    manifest_file: str,\n    split: str | None = None,\n    column_mapping: Dict[str, str] = default_column_mapping,\n    embeddings_transforms: Callable | None = None,\n    target_transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initialize dataset.\n\n    Expects a manifest file listing the paths of .pt files that contain\n    tensor embeddings of shape [embedding_dim] or [1, embedding_dim].\n\n    Args:\n        root: Root directory of the dataset.\n        manifest_file: The path to the manifest file, which is relative to\n            the `root` argument.\n        split: The dataset split to use. The `split` column of the manifest\n            file will be splitted based on this value.\n        column_mapping: Defines the map between the variables and the manifest\n            columns. It will overwrite the `default_column_mapping` with\n            the provided values, so that `column_mapping` can contain only the\n            values which are altered or missing.\n        embeddings_transforms: A function/transform that transforms the embedding.\n        target_transforms: A function/transform that transforms the target.\n    \"\"\"\n    super().__init__()\n\n    self._root = root\n    self._manifest_file = manifest_file\n    self._split = split\n    self._column_mapping = self.default_column_mapping | column_mapping\n    self._embeddings_transforms = embeddings_transforms\n    self._target_transforms = target_transforms\n\n    self._data: pd.DataFrame\n</code></pre>"},{"location":"reference/core/data/datasets/#eva.core.data.datasets.EmbeddingsClassificationDataset.default_column_mapping","title":"<code>default_column_mapping: Dict[str, str] = {'data': 'embeddings', 'target': 'target', 'split': 'split'}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The default column mapping of the variables to the manifest columns.</p>"},{"location":"reference/core/data/datasets/#eva.core.data.datasets.EmbeddingsClassificationDataset.filename","title":"<code>filename</code>","text":"<p>Returns the filename of the <code>index</code>'th data sample.</p> <p>Note that this is the relative file path to the root.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the data-sample to select.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The filename of the <code>index</code>'th data sample.</p> Source code in <code>src/eva/core/data/datasets/classification/embeddings.py</code> <pre><code>def filename(self, index: int) -&gt; str:\n    \"\"\"Returns the filename of the `index`'th data sample.\n\n    Note that this is the relative file path to the root.\n\n    Args:\n        index: The index of the data-sample to select.\n\n    Returns:\n        The filename of the `index`'th data sample.\n    \"\"\"\n    return self._data.at[index, self._column_mapping[\"data\"]]\n</code></pre>"},{"location":"reference/core/metrics/","title":"Metrics","text":"<p>Reference information for the <code>Metrics</code> classes.</p>"},{"location":"reference/core/metrics/average_loss/","title":"Average Loss","text":""},{"location":"reference/core/metrics/average_loss/#eva.metrics.AverageLoss","title":"<code>eva.metrics.AverageLoss</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Average loss metric tracker.</p> Source code in <code>src/eva/core/metrics/average_loss.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes the metric.\"\"\"\n    super().__init__()\n\n    self.add_state(\"value\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n    self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/core/metrics/binary_balanced_accuracy/","title":"Binary Balanced Accuracy","text":""},{"location":"reference/core/metrics/binary_balanced_accuracy/#eva.metrics.BinaryBalancedAccuracy","title":"<code>eva.metrics.BinaryBalancedAccuracy</code>","text":"<p>             Bases: <code>BinaryStatScores</code></p> <p>Computes the balanced accuracy for binary classification.</p>"},{"location":"reference/core/metrics/binary_balanced_accuracy/#eva.metrics.BinaryBalancedAccuracy.compute","title":"<code>compute</code>","text":"<p>Compute accuracy based on inputs passed in to <code>update</code> previously.</p> Source code in <code>src/eva/core/metrics/binary_balanced_accuracy.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute accuracy based on inputs passed in to ``update`` previously.\"\"\"\n    tp, fp, tn, fn = self._final_state()\n    sensitivity = _safe_divide(tp, tp + fn)\n    specificity = _safe_divide(tn, tn + fp)\n    return 0.5 * (sensitivity + specificity)\n</code></pre>"},{"location":"reference/core/metrics/core/","title":"Core","text":""},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule","title":"<code>eva.metrics.MetricModule</code>","text":"<p>             Bases: <code>Module</code></p> <p>The metrics module.</p> <p>Allows to store and keep track of <code>train</code>, <code>val</code> and <code>test</code> metrics.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>MetricCollection | None</code> <p>The training metric collection.</p> required <code>val</code> <code>MetricCollection | None</code> <p>The validation metric collection.</p> required <code>test</code> <code>MetricCollection | None</code> <p>The test metric collection.</p> required Source code in <code>src/eva/core/metrics/structs/module.py</code> <pre><code>def __init__(\n    self,\n    train: collection.MetricCollection | None,\n    val: collection.MetricCollection | None,\n    test: collection.MetricCollection | None,\n) -&gt; None:\n    \"\"\"Initializes the metrics for the Trainer.\n\n    Args:\n        train: The training metric collection.\n        val: The validation metric collection.\n        test: The test metric collection.\n    \"\"\"\n    super().__init__()\n\n    self._train = train or self.default_metric_collection\n    self._val = val or self.default_metric_collection\n    self._test = test or self.default_metric_collection\n</code></pre>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule.default_metric_collection","title":"<code>default_metric_collection: collection.MetricCollection</code>  <code>property</code>","text":"<p>Returns the default metric collection.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule.training_metrics","title":"<code>training_metrics: collection.MetricCollection</code>  <code>property</code>","text":"<p>Returns the metrics of the train dataset.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule.validation_metrics","title":"<code>validation_metrics: collection.MetricCollection</code>  <code>property</code>","text":"<p>Returns the metrics of the validation dataset.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule.test_metrics","title":"<code>test_metrics: collection.MetricCollection</code>  <code>property</code>","text":"<p>Returns the metrics of the test dataset.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule.from_metrics","title":"<code>from_metrics</code>  <code>classmethod</code>","text":"<p>Initializes a metric module from a list of metrics.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>MetricModuleType | None</code> <p>Metrics for the training stage.</p> required <code>val</code> <code>MetricModuleType | None</code> <p>Metrics for the validation stage.</p> required <code>test</code> <code>MetricModuleType | None</code> <p>Metrics for the test stage.</p> required <code>separator</code> <code>str</code> <p>The separator between the group name of the metric and the metric itself.</p> <code>'/'</code> Source code in <code>src/eva/core/metrics/structs/module.py</code> <pre><code>@classmethod\ndef from_metrics(\n    cls,\n    train: MetricModuleType | None,\n    val: MetricModuleType | None,\n    test: MetricModuleType | None,\n    *,\n    separator: str = \"/\",\n) -&gt; MetricModule:\n    \"\"\"Initializes a metric module from a list of metrics.\n\n    Args:\n        train: Metrics for the training stage.\n        val: Metrics for the validation stage.\n        test: Metrics for the test stage.\n        separator: The separator between the group name of the metric\n            and the metric itself.\n    \"\"\"\n    return cls(\n        train=_create_collection_from_metrics(train, prefix=\"train\" + separator),\n        val=_create_collection_from_metrics(val, prefix=\"val\" + separator),\n        test=_create_collection_from_metrics(test, prefix=\"test\" + separator),\n    )\n</code></pre>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule.from_schema","title":"<code>from_schema</code>  <code>classmethod</code>","text":"<p>Initializes a metric module from the metrics schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>MetricsSchema</code> <p>The dataclass metric schema.</p> required <code>separator</code> <code>str</code> <p>The separator between the group name of the metric and the metric itself.</p> <code>'/'</code> Source code in <code>src/eva/core/metrics/structs/module.py</code> <pre><code>@classmethod\ndef from_schema(\n    cls,\n    schema: schemas.MetricsSchema,\n    *,\n    separator: str = \"/\",\n) -&gt; MetricModule:\n    \"\"\"Initializes a metric module from the metrics schema.\n\n    Args:\n        schema: The dataclass metric schema.\n        separator: The separator between the group name of the metric\n            and the metric itself.\n    \"\"\"\n    return cls.from_metrics(\n        train=schema.training_metrics,\n        val=schema.evaluation_metrics,\n        test=schema.evaluation_metrics,\n        separator=separator,\n    )\n</code></pre>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricsSchema","title":"<code>eva.metrics.MetricsSchema</code>  <code>dataclass</code>","text":"<p>Metrics schema.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricsSchema.common","title":"<code>common: MetricModuleType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Holds the common train and evaluation metrics.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricsSchema.train","title":"<code>train: MetricModuleType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The exclusive training metrics.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricsSchema.evaluation","title":"<code>evaluation: MetricModuleType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The exclusive evaluation metrics.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricsSchema.training_metrics","title":"<code>training_metrics: MetricModuleType | None</code>  <code>property</code>","text":"<p>Returns the training metics.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricsSchema.evaluation_metrics","title":"<code>evaluation_metrics: MetricModuleType | None</code>  <code>property</code>","text":"<p>Returns the evaluation metics.</p>"},{"location":"reference/core/metrics/defaults/","title":"Defaults","text":""},{"location":"reference/core/metrics/defaults/#eva.metrics.BinaryClassificationMetrics","title":"<code>eva.metrics.BinaryClassificationMetrics</code>","text":"<p>             Bases: <code>MetricCollection</code></p> <p>Default metrics for binary classification tasks.</p> <p>The metrics instantiated here are:</p> <ul> <li>BinaryAUROC</li> <li>BinaryAccuracy</li> <li>BinaryBalancedAccuracy</li> <li>BinaryF1Score</li> <li>BinaryPrecision</li> <li>BinaryRecall</li> </ul> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Threshold for transforming probability to binary (0,1) predictions</p> <code>0.5</code> <code>ignore_index</code> <code>int | None</code> <p>Specifies a target value that is ignored and does not contribute to the metric calculation.</p> <code>None</code> <code>prefix</code> <code>str | None</code> <p>A string to append in front of the keys of the output dict.</p> <code>None</code> <code>postfix</code> <code>str | None</code> <p>A string to append after the keys of the output dict.</p> <code>None</code> Source code in <code>src/eva/core/metrics/defaults/classification/binary.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    ignore_index: int | None = None,\n    prefix: str | None = None,\n    postfix: str | None = None,\n) -&gt; None:\n    \"\"\"Initializes the binary classification metrics.\n\n    The metrics instantiated here are:\n\n    - BinaryAUROC\n    - BinaryAccuracy\n    - BinaryBalancedAccuracy\n    - BinaryF1Score\n    - BinaryPrecision\n    - BinaryRecall\n\n    Args:\n        threshold: Threshold for transforming probability to binary (0,1) predictions\n        ignore_index: Specifies a target value that is ignored and does not\n            contribute to the metric calculation.\n        prefix: A string to append in front of the keys of the output dict.\n        postfix: A string to append after the keys of the output dict.\n    \"\"\"\n    super().__init__(\n        metrics=[\n            classification.BinaryAUROC(\n                ignore_index=ignore_index,\n            ),\n            classification.BinaryAccuracy(\n                threshold=threshold,\n                ignore_index=ignore_index,\n            ),\n            binary_balanced_accuracy.BinaryBalancedAccuracy(\n                threshold=threshold,\n                ignore_index=ignore_index,\n            ),\n            classification.BinaryF1Score(\n                threshold=threshold,\n                ignore_index=ignore_index,\n            ),\n            classification.BinaryPrecision(\n                threshold=threshold,\n                ignore_index=ignore_index,\n            ),\n            classification.BinaryRecall(\n                threshold=threshold,\n                ignore_index=ignore_index,\n            ),\n        ],\n        prefix=prefix,\n        postfix=postfix,\n        compute_groups=[\n            [\n                \"BinaryAccuracy\",\n                \"BinaryBalancedAccuracy\",\n                \"BinaryF1Score\",\n                \"BinaryPrecision\",\n                \"BinaryRecall\",\n            ],\n            [\n                \"BinaryAUROC\",\n            ],\n        ],\n    )\n</code></pre>"},{"location":"reference/core/metrics/defaults/#eva.metrics.MulticlassClassificationMetrics","title":"<code>eva.metrics.MulticlassClassificationMetrics</code>","text":"<p>             Bases: <code>MetricCollection</code></p> <p>Default metrics for multi-class classification tasks.</p> <p>The metrics instantiated here are:</p> <ul> <li>MulticlassAccuracy</li> <li>MulticlassPrecision</li> <li>MulticlassRecall</li> <li>MulticlassF1Score</li> <li>MulticlassAUROC</li> </ul> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Integer specifying the number of classes.</p> required <code>average</code> <code>Literal['macro', 'weighted', 'none']</code> <p>Defines the reduction that is applied over labels.</p> <code>'macro'</code> <code>ignore_index</code> <code>int | None</code> <p>Specifies a target value that is ignored and does not contribute to the metric calculation.</p> <code>None</code> <code>prefix</code> <code>str | None</code> <p>A string to append in front of the keys of the output dict.</p> <code>None</code> <code>postfix</code> <code>str | None</code> <p>A string to append after the keys of the output dict.</p> <code>None</code> Source code in <code>src/eva/core/metrics/defaults/classification/multiclass.py</code> <pre><code>def __init__(\n    self,\n    num_classes: int,\n    average: Literal[\"macro\", \"weighted\", \"none\"] = \"macro\",\n    ignore_index: int | None = None,\n    prefix: str | None = None,\n    postfix: str | None = None,\n) -&gt; None:\n    \"\"\"Initializes the multi-class classification metrics.\n\n    The metrics instantiated here are:\n\n    - MulticlassAccuracy\n    - MulticlassPrecision\n    - MulticlassRecall\n    - MulticlassF1Score\n    - MulticlassAUROC\n\n    Args:\n        num_classes: Integer specifying the number of classes.\n        average: Defines the reduction that is applied over labels.\n        ignore_index: Specifies a target value that is ignored and does not\n            contribute to the metric calculation.\n        prefix: A string to append in front of the keys of the output dict.\n        postfix: A string to append after the keys of the output dict.\n    \"\"\"\n    super().__init__(\n        metrics=[\n            classification.MulticlassAUROC(\n                num_classes=num_classes,\n                average=average,\n                ignore_index=ignore_index,\n            ),\n            classification.MulticlassAccuracy(\n                num_classes=num_classes,\n                average=average,\n                ignore_index=ignore_index,\n            ),\n            classification.MulticlassF1Score(\n                num_classes=num_classes,\n                average=average,\n                ignore_index=ignore_index,\n            ),\n            classification.MulticlassPrecision(\n                num_classes=num_classes,\n                average=average,\n                ignore_index=ignore_index,\n            ),\n            classification.MulticlassRecall(\n                num_classes=num_classes,\n                average=average,\n                ignore_index=ignore_index,\n            ),\n        ],\n        prefix=prefix,\n        postfix=postfix,\n        compute_groups=[\n            [\n                \"MulticlassAccuracy\",\n                \"MulticlassF1Score\",\n                \"MulticlassPrecision\",\n                \"MulticlassRecall\",\n            ],\n            [\n                \"MulticlassAUROC\",\n            ],\n        ],\n    )\n</code></pre>"},{"location":"reference/core/models/modules/","title":"Modules","text":"<p>Reference information for the model <code>Modules</code> API.</p>"},{"location":"reference/core/models/modules/#eva.models.modules.ModelModule","title":"<code>eva.models.modules.ModelModule</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>The base model module.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>MetricsSchema | None</code> <p>The metric groups to track.</p> <code>None</code> <code>postprocess</code> <code>BatchPostProcess | None</code> <p>A list of helper functions to apply after the loss and before the metrics calculation to the model predictions and targets.</p> <code>None</code> Source code in <code>src/eva/core/models/modules/module.py</code> <pre><code>def __init__(\n    self,\n    metrics: metrics_lib.MetricsSchema | None = None,\n    postprocess: batch_postprocess.BatchPostProcess | None = None,\n) -&gt; None:\n    \"\"\"Initializes the basic module.\n\n    Args:\n        metrics: The metric groups to track.\n        postprocess: A list of helper functions to apply after the\n            loss and before the metrics calculation to the model\n            predictions and targets.\n    \"\"\"\n    super().__init__()\n\n    self._metrics = metrics or self.default_metrics\n    self._postprocess = postprocess or self.default_postprocess\n\n    self.metrics = metrics_lib.MetricModule.from_schema(self._metrics)\n</code></pre>"},{"location":"reference/core/models/modules/#eva.models.modules.ModelModule.default_metrics","title":"<code>default_metrics: metrics_lib.MetricsSchema</code>  <code>property</code>","text":"<p>The default metrics.</p>"},{"location":"reference/core/models/modules/#eva.models.modules.ModelModule.default_postprocess","title":"<code>default_postprocess: batch_postprocess.BatchPostProcess</code>  <code>property</code>","text":"<p>The default post-processes.</p>"},{"location":"reference/core/models/modules/#eva.models.modules.HeadModule","title":"<code>eva.models.modules.HeadModule</code>","text":"<p>             Bases: <code>ModelModule</code></p> <p>Neural Net Head Module for training on features.</p> <p>It can be used for supervised (mini-batch) stochastic gradient descent downstream tasks such as classification, regression and segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>head</code> <code>MODEL_TYPE</code> <p>The neural network that would be trained on the features.</p> required <code>criterion</code> <code>Callable[..., Tensor]</code> <p>The loss function to use.</p> required <code>backbone</code> <code>MODEL_TYPE | None</code> <p>The feature extractor. If <code>None</code>, it will be expected that the input batch returns the features directly.</p> <code>None</code> <code>optimizer</code> <code>OptimizerCallable</code> <p>The optimizer to use.</p> <code>Adam</code> <code>lr_scheduler</code> <code>LRSchedulerCallable</code> <p>The learning rate scheduler to use.</p> <code>ConstantLR</code> <code>metrics</code> <code>MetricsSchema | None</code> <p>The metric groups to track.</p> <code>None</code> <code>postprocess</code> <code>BatchPostProcess | None</code> <p>A list of helper functions to apply after the loss and before the metrics calculation to the model predictions and targets.</p> <code>None</code> Source code in <code>src/eva/core/models/modules/head.py</code> <pre><code>def __init__(\n    self,\n    head: MODEL_TYPE,\n    criterion: Callable[..., torch.Tensor],\n    backbone: MODEL_TYPE | None = None,\n    optimizer: OptimizerCallable = optim.Adam,\n    lr_scheduler: LRSchedulerCallable = lr_scheduler.ConstantLR,\n    metrics: metrics_lib.MetricsSchema | None = None,\n    postprocess: batch_postprocess.BatchPostProcess | None = None,\n) -&gt; None:\n    \"\"\"Initializes the neural net head module.\n\n    Args:\n        head: The neural network that would be trained on the features.\n        criterion: The loss function to use.\n        backbone: The feature extractor. If `None`, it will be expected\n            that the input batch returns the features directly.\n        optimizer: The optimizer to use.\n        lr_scheduler: The learning rate scheduler to use.\n        metrics: The metric groups to track.\n        postprocess: A list of helper functions to apply after the\n            loss and before the metrics calculation to the model\n            predictions and targets.\n    \"\"\"\n    super().__init__(metrics=metrics, postprocess=postprocess)\n\n    self.head = head\n    self.criterion = criterion\n    self.backbone = backbone\n    self.optimizer = optimizer\n    self.lr_scheduler = lr_scheduler\n</code></pre>"},{"location":"reference/core/models/modules/#eva.models.modules.InferenceModule","title":"<code>eva.models.modules.InferenceModule</code>","text":"<p>             Bases: <code>ModelModule</code></p> <p>An lightweight model module to perform inference.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>MODEL_TYPE</code> <p>The network to be used for inference.</p> required Source code in <code>src/eva/core/models/modules/inference.py</code> <pre><code>def __init__(self, backbone: MODEL_TYPE) -&gt; None:\n    \"\"\"Initializes the module.\n\n    Args:\n        backbone: The network to be used for inference.\n    \"\"\"\n    super().__init__(metrics=None)\n\n    self.backbone = backbone\n</code></pre>"},{"location":"reference/core/models/networks/","title":"Networks","text":"<p>Reference information for the model <code>Networks</code> API.</p>"},{"location":"reference/core/models/networks/#eva.models.networks.MLP","title":"<code>eva.models.networks.MLP</code>","text":"<p>             Bases: <code>Module</code></p> <p>A Multi-layer Perceptron (MLP) network.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>The number of input features.</p> required <code>output_size</code> <code>int</code> <p>The number of output features.</p> required <code>hidden_layer_sizes</code> <code>tuple[int, ...] | None</code> <p>A list specifying the number of units in each hidden layer.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout probability for hidden layers.</p> <code>0.0</code> <code>hidden_activation_fn</code> <code>Type[Module] | None</code> <p>Activation function to use for hidden layers. Default is ReLU.</p> <code>ReLU</code> <code>output_activation_fn</code> <code>Type[Module] | None</code> <p>Activation function to use for the output layer. Default is None.</p> <code>None</code> Source code in <code>src/eva/core/models/networks/mlp.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    output_size: int,\n    hidden_layer_sizes: tuple[int, ...] | None = None,\n    hidden_activation_fn: Type[torch.nn.Module] | None = nn.ReLU,\n    output_activation_fn: Type[torch.nn.Module] | None = None,\n    dropout: float = 0.0,\n) -&gt; None:\n    \"\"\"Initializes the MLP.\n\n    Args:\n        input_size: The number of input features.\n        output_size: The number of output features.\n        hidden_layer_sizes: A list specifying the number of units in each hidden layer.\n        dropout: Dropout probability for hidden layers.\n        hidden_activation_fn: Activation function to use for hidden layers. Default is ReLU.\n        output_activation_fn: Activation function to use for the output layer. Default is None.\n    \"\"\"\n    super().__init__()\n\n    self.input_size = input_size\n    self.output_size = output_size\n    self.hidden_layer_sizes = hidden_layer_sizes if hidden_layer_sizes is not None else ()\n    self.hidden_activation_fn = hidden_activation_fn\n    self.output_activation_fn = output_activation_fn\n    self.dropout = dropout\n\n    self._network = self._build_network()\n</code></pre>"},{"location":"reference/core/models/networks/#eva.models.networks.MLP.forward","title":"<code>forward</code>","text":"<p>Defines the forward pass of the MLP.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output of the network.</p> Source code in <code>src/eva/core/models/networks/mlp.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Defines the forward pass of the MLP.\n\n    Args:\n        x: The input tensor.\n\n    Returns:\n        The output of the network.\n    \"\"\"\n    return self._network(x)\n</code></pre>"},{"location":"reference/core/models/networks/#wrappers","title":"Wrappers","text":""},{"location":"reference/core/models/networks/#eva.models.networks.wrappers.BaseModel","title":"<code>eva.models.networks.wrappers.BaseModel</code>","text":"<p>             Bases: <code>Module</code></p> <p>Base class for model wrappers.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> Source code in <code>src/eva/core/models/networks/wrappers/base.py</code> <pre><code>def __init__(self, tensor_transforms: Callable | None = None) -&gt; None:\n    \"\"\"Initializes the model.\n\n    Args:\n        tensor_transforms: The transforms to apply to the output\n            tensor produced by the model.\n    \"\"\"\n    super().__init__()\n\n    self._output_transforms = tensor_transforms\n</code></pre>"},{"location":"reference/core/models/networks/#eva.models.networks.wrappers.BaseModel.load_model","title":"<code>load_model</code>  <code>abstractmethod</code>","text":"<p>Loads the model.</p> Source code in <code>src/eva/core/models/networks/wrappers/base.py</code> <pre><code>@abc.abstractmethod\ndef load_model(self) -&gt; Callable[..., torch.Tensor]:\n    \"\"\"Loads the model.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/core/models/networks/#eva.models.networks.wrappers.BaseModel.model_forward","title":"<code>model_forward</code>  <code>abstractmethod</code>","text":"<p>Implements the forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor to the model.</p> required Source code in <code>src/eva/core/models/networks/wrappers/base.py</code> <pre><code>@abc.abstractmethod\ndef model_forward(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Implements the forward pass of the model.\n\n    Args:\n        tensor: The input tensor to the model.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/core/models/networks/#eva.models.networks.wrappers.ModelFromFunction","title":"<code>eva.models.networks.wrappers.ModelFromFunction</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Wrapper class for models which are initialized from functions.</p> <p>This is helpful for initializing models in a <code>.yaml</code> configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Callable[..., Module]</code> <p>The path to the callable object (class or function).</p> required <code>arguments</code> <code>Dict[str, Any] | None</code> <p>The extra callable function / class arguments.</p> <code>None</code> <code>checkpoint_path</code> <code>str | None</code> <p>The path to the checkpoint to load the model weights from. This is currently only supported for torch model checkpoints. For other formats, the checkpoint loading should be handled within the provided callable object in . <code>None</code> <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> Source code in <code>src/eva/core/models/networks/wrappers/from_function.py</code> <pre><code>def __init__(\n    self,\n    path: Callable[..., nn.Module],\n    arguments: Dict[str, Any] | None = None,\n    checkpoint_path: str | None = None,\n    tensor_transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initializes and constructs the model.\n\n    Args:\n        path: The path to the callable object (class or function).\n        arguments: The extra callable function / class arguments.\n        checkpoint_path: The path to the checkpoint to load the model\n            weights from. This is currently only supported for torch\n            model checkpoints. For other formats, the checkpoint loading\n            should be handled within the provided callable object in &lt;path&gt;.\n        tensor_transforms: The transforms to apply to the output tensor\n            produced by the model.\n    \"\"\"\n    super().__init__()\n\n    self._path = path\n    self._arguments = arguments\n    self._checkpoint_path = checkpoint_path\n    self._tensor_transforms = tensor_transforms\n\n    self._model = self.load_model()\n</code></pre>"},{"location":"reference/core/models/networks/#eva.models.networks.wrappers.HuggingFaceModel","title":"<code>eva.models.networks.wrappers.HuggingFaceModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Wrapper class for loading HuggingFace <code>transformers</code> models.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The model name or path to load the model from. This can be a local path or a model name from the <code>HuggingFace</code> model hub.</p> required <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> Source code in <code>src/eva/core/models/networks/wrappers/huggingface.py</code> <pre><code>def __init__(self, model_name_or_path: str, tensor_transforms: Callable | None = None) -&gt; None:\n    \"\"\"Initializes the model.\n\n    Args:\n        model_name_or_path: The model name or path to load the model from.\n            This can be a local path or a model name from the `HuggingFace`\n            model hub.\n        tensor_transforms: The transforms to apply to the output tensor\n            produced by the model.\n    \"\"\"\n    super().__init__(tensor_transforms=tensor_transforms)\n\n    self._model_name_or_path = model_name_or_path\n    self._model = self.load_model()\n</code></pre>"},{"location":"reference/core/models/networks/#eva.models.networks.wrappers.ONNXModel","title":"<code>eva.models.networks.wrappers.ONNXModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Wrapper class for loading ONNX models.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the .onnx model file.</p> required <code>device</code> <code>Literal['cpu', 'cuda'] | None</code> <p>The device to run the model on. This can be either \"cpu\" or \"cuda\".</p> <code>'cpu'</code> <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> Source code in <code>src/eva/core/models/networks/wrappers/onnx.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    device: Literal[\"cpu\", \"cuda\"] | None = \"cpu\",\n    tensor_transforms: Callable | None = None,\n):\n    \"\"\"Initializes the model.\n\n    Args:\n        path: The path to the .onnx model file.\n        device: The device to run the model on. This can be either \"cpu\" or \"cuda\".\n        tensor_transforms: The transforms to apply to the output tensor produced by the model.\n    \"\"\"\n    super().__init__(tensor_transforms=tensor_transforms)\n\n    self._path = path\n    self._device = device\n    self._model = self.load_model()\n</code></pre>"},{"location":"reference/core/trainers/functional/","title":"Functional","text":"<p>Reference information for the trainers <code>Functional</code> API.</p>"},{"location":"reference/core/trainers/functional/#eva.core.trainers.functional.run_evaluation_session","title":"<code>eva.core.trainers.functional.run_evaluation_session</code>","text":"<p>Runs a downstream evaluation session out-of-place.</p> <p>It performs an evaluation run (fit and evaluate) on the model multiple times. Note that as the input <code>base_trainer</code> and <code>base_model</code> would be cloned, the input object would not be modified.</p> <p>Parameters:</p> Name Type Description Default <code>base_trainer</code> <code>Trainer</code> <p>The base trainer module to use.</p> required <code>base_model</code> <code>ModelModule</code> <p>The base model module to use.</p> required <code>datamodule</code> <code>DataModule</code> <p>The data module.</p> required <code>n_runs</code> <code>int</code> <p>The amount of runs (fit and evaluate) to perform.</p> <code>1</code> Source code in <code>src/eva/core/trainers/functional.py</code> <pre><code>def run_evaluation_session(\n    base_trainer: eva_trainer.Trainer,\n    base_model: modules.ModelModule,\n    datamodule: datamodules.DataModule,\n    *,\n    n_runs: int = 1,\n) -&gt; None:\n    \"\"\"Runs a downstream evaluation session out-of-place.\n\n    It performs an evaluation run (fit and evaluate) on the model\n    multiple times. Note that as the input `base_trainer` and\n    `base_model` would be cloned, the input object would not\n    be modified.\n\n    Args:\n        base_trainer: The base trainer module to use.\n        base_model: The base model module to use.\n        datamodule: The data module.\n        n_runs: The amount of runs (fit and evaluate) to perform.\n    \"\"\"\n    recorder = _recorder.SessionRecorder(output_dir=base_trainer.default_log_dir)\n    for run_index in range(n_runs):\n        validation_scores, test_scores = run_evaluation(\n            base_trainer, base_model, datamodule, run_id=f\"run_{run_index}\"\n        )\n        recorder.update(validation_scores, test_scores)\n    recorder.save()\n</code></pre>"},{"location":"reference/core/trainers/functional/#eva.core.trainers.functional.run_evaluation","title":"<code>eva.core.trainers.functional.run_evaluation</code>","text":"<p>Fits and evaluates a model out-of-place.</p> <p>Parameters:</p> Name Type Description Default <code>base_trainer</code> <code>Trainer</code> <p>The base trainer to use but not modify.</p> required <code>base_model</code> <code>ModelModule</code> <p>The model module to use but not modify.</p> required <code>datamodule</code> <code>DataModule</code> <p>The data module.</p> required <code>run_id</code> <code>str | None</code> <p>The run id to be appended to the output log directory. If <code>None</code>, it will use the log directory of the trainer as is.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[_EVALUATE_OUTPUT, _EVALUATE_OUTPUT | None]</code> <p>A tuple of with the validation and the test metrics (if exists).</p> Source code in <code>src/eva/core/trainers/functional.py</code> <pre><code>def run_evaluation(\n    base_trainer: eva_trainer.Trainer,\n    base_model: modules.ModelModule,\n    datamodule: datamodules.DataModule,\n    *,\n    run_id: str | None = None,\n) -&gt; Tuple[_EVALUATE_OUTPUT, _EVALUATE_OUTPUT | None]:\n    \"\"\"Fits and evaluates a model out-of-place.\n\n    Args:\n        base_trainer: The base trainer to use but not modify.\n        base_model: The model module to use but not modify.\n        datamodule: The data module.\n        run_id: The run id to be appended to the output log directory.\n            If `None`, it will use the log directory of the trainer as is.\n\n    Returns:\n        A tuple of with the validation and the test metrics (if exists).\n    \"\"\"\n    trainer, model = _utils.clone(base_trainer, base_model)\n    trainer.setup_log_dirs(run_id or \"\")\n    return fit_and_validate(trainer, model, datamodule)\n</code></pre>"},{"location":"reference/core/trainers/functional/#eva.core.trainers.functional.fit_and_validate","title":"<code>eva.core.trainers.functional.fit_and_validate</code>","text":"<p>Fits and evaluates a model in-place.</p> <p>If the test set is set in the datamodule, it will evaluate the model on the test set as well.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer module to use and update in-place.</p> required <code>model</code> <code>ModelModule</code> <p>The model module to use and update in-place.</p> required <code>datamodule</code> <code>DataModule</code> <p>The data module.</p> required <p>Returns:</p> Type Description <code>Tuple[_EVALUATE_OUTPUT, _EVALUATE_OUTPUT | None]</code> <p>A tuple of with the validation and the test metrics (if exists).</p> Source code in <code>src/eva/core/trainers/functional.py</code> <pre><code>def fit_and_validate(\n    trainer: eva_trainer.Trainer,\n    model: modules.ModelModule,\n    datamodule: datamodules.DataModule,\n) -&gt; Tuple[_EVALUATE_OUTPUT, _EVALUATE_OUTPUT | None]:\n    \"\"\"Fits and evaluates a model in-place.\n\n    If the test set is set in the datamodule, it will evaluate the model\n    on the test set as well.\n\n    Args:\n        trainer: The trainer module to use and update in-place.\n        model: The model module to use and update in-place.\n        datamodule: The data module.\n\n    Returns:\n        A tuple of with the validation and the test metrics (if exists).\n    \"\"\"\n    trainer.fit(model, datamodule=datamodule)\n    validation_scores = trainer.validate(datamodule=datamodule)\n    test_scores = None if datamodule.datasets.test is None else trainer.test(datamodule=datamodule)\n    return validation_scores, test_scores\n</code></pre>"},{"location":"reference/core/trainers/functional/#eva.core.trainers.functional.infer_model","title":"<code>eva.core.trainers.functional.infer_model</code>","text":"<p>Performs model inference out-of-place.</p> <p>Note that the input <code>base_model</code> and <code>base_trainer</code> would not be modified.</p> <p>Parameters:</p> Name Type Description Default <code>base_trainer</code> <code>Trainer</code> <p>The base trainer to use but not modify.</p> required <code>base_model</code> <code>ModelModule</code> <p>The model module to use but not modify.</p> required <code>datamodule</code> <code>DataModule</code> <p>The data module.</p> required <code>return_predictions</code> <code>bool</code> <p>Whether to return the model predictions.</p> <code>False</code> Source code in <code>src/eva/core/trainers/functional.py</code> <pre><code>def infer_model(\n    base_trainer: eva_trainer.Trainer,\n    base_model: modules.ModelModule,\n    datamodule: datamodules.DataModule,\n    *,\n    return_predictions: bool = False,\n) -&gt; None:\n    \"\"\"Performs model inference out-of-place.\n\n    Note that the input `base_model` and `base_trainer` would\n    not be modified.\n\n    Args:\n        base_trainer: The base trainer to use but not modify.\n        base_model: The model module to use but not modify.\n        datamodule: The data module.\n        return_predictions: Whether to return the model predictions.\n    \"\"\"\n    trainer, model = _utils.clone(base_trainer, base_model)\n    return trainer.predict(\n        model=model,\n        datamodule=datamodule,\n        return_predictions=return_predictions,\n    )\n</code></pre>"},{"location":"reference/core/trainers/trainer/","title":"Trainers","text":"<p>Reference information for the <code>Trainers</code> API.</p>"},{"location":"reference/core/trainers/trainer/#eva.core.trainers.Trainer","title":"<code>eva.core.trainers.Trainer</code>","text":"<p>             Bases: <code>Trainer</code></p> <p>Core trainer class.</p> <p>This is an extended version of lightning's core trainer class.</p> <p>For the input arguments, refer to ::class::<code>lightning.pytorch.Trainer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Any</code> <p>Positional arguments of ::class::<code>lightning.pytorch.Trainer</code>.</p> <code>()</code> <code>default_root_dir</code> <code>str</code> <p>The default root directory to store the output logs. Unlike in ::class::<code>lightning.pytorch.Trainer</code>, this path would be the prioritized destination point.</p> <code>'logs'</code> <code>n_runs</code> <code>int</code> <p>The amount of runs (fit and evaluate) to perform in an evaluation session.</p> <code>1</code> <code>kwargs</code> <code>Any</code> <p>Kew-word arguments of ::class::<code>lightning.pytorch.Trainer</code>.</p> <code>{}</code> Source code in <code>src/eva/core/trainers/trainer.py</code> <pre><code>@argparse._defaults_from_env_vars\ndef __init__(\n    self,\n    *args: Any,\n    default_root_dir: str = \"logs\",\n    n_runs: int = 1,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initializes the trainer.\n\n    For the input arguments, refer to ::class::`lightning.pytorch.Trainer`.\n\n    Args:\n        args: Positional arguments of ::class::`lightning.pytorch.Trainer`.\n        default_root_dir: The default root directory to store the output logs.\n            Unlike in ::class::`lightning.pytorch.Trainer`, this path would be the\n            prioritized destination point.\n        n_runs: The amount of runs (fit and evaluate) to perform in an evaluation session.\n        kwargs: Kew-word arguments of ::class::`lightning.pytorch.Trainer`.\n    \"\"\"\n    super().__init__(*args, default_root_dir=default_root_dir, **kwargs)\n\n    self._n_runs = n_runs\n\n    self._session_id: str = _logging.generate_session_id()\n    self._log_dir: str = self.default_log_dir\n\n    self.setup_log_dirs()\n</code></pre>"},{"location":"reference/core/trainers/trainer/#eva.core.trainers.Trainer.default_log_dir","title":"<code>default_log_dir: str</code>  <code>property</code>","text":"<p>Returns the default log directory.</p>"},{"location":"reference/core/trainers/trainer/#eva.core.trainers.Trainer.setup_log_dirs","title":"<code>setup_log_dirs</code>","text":"<p>Setups the logging directory of the trainer and experimental loggers in-place.</p> <p>Parameters:</p> Name Type Description Default <code>subdirectory</code> <code>str</code> <p>Whether to append a subdirectory to the output log.</p> <code>''</code> Source code in <code>src/eva/core/trainers/trainer.py</code> <pre><code>def setup_log_dirs(self, subdirectory: str = \"\") -&gt; None:\n    \"\"\"Setups the logging directory of the trainer and experimental loggers in-place.\n\n    Args:\n        subdirectory: Whether to append a subdirectory to the output log.\n    \"\"\"\n    self._log_dir = os.path.join(self.default_root_dir, self._session_id, subdirectory)\n    os.fspath(self._log_dir)\n\n    for logger in self.loggers:\n        if isinstance(logger, (pl_loggers.CSVLogger, pl_loggers.TensorBoardLogger)):\n            logger._root_dir = self.default_root_dir\n            logger._name = self._session_id\n            logger._version = subdirectory\n</code></pre>"},{"location":"reference/core/trainers/trainer/#eva.core.trainers.Trainer.run_evaluation_session","title":"<code>run_evaluation_session</code>","text":"<p>Runs a evaluation session out-of-place.</p> <p>It performs an evaluation run (fit and evaluate) the model <code>self._n_run</code> times. Note that the input <code>base_model</code> would not be modified, so the weights of the input model will remain as they are.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelModule</code> <p>The base model module to evaluate.</p> required <code>datamodule</code> <code>DataModule</code> <p>The data module.</p> required Source code in <code>src/eva/core/trainers/trainer.py</code> <pre><code>def run_evaluation_session(\n    self,\n    model: modules.ModelModule,\n    datamodule: datamodules.DataModule,\n) -&gt; None:\n    \"\"\"Runs a evaluation session out-of-place.\n\n    It performs an evaluation run (fit and evaluate) the model\n    `self._n_run` times. Note that the input `base_model` would\n    not be modified, so the weights of the input model will remain\n    as they are.\n\n    Args:\n        model: The base model module to evaluate.\n        datamodule: The data module.\n    \"\"\"\n    functional.run_evaluation_session(\n        base_trainer=self,\n        base_model=model,\n        datamodule=datamodule,\n        n_runs=self._n_runs,\n    )\n</code></pre>"},{"location":"reference/core/utils/multiprocessing/","title":"Multiprocessing","text":"<p>Reference information for the utils <code>Multiprocessing</code> API.</p>"},{"location":"reference/core/utils/multiprocessing/#eva.core.utils.multiprocessing.Process","title":"<code>eva.core.utils.multiprocessing.Process</code>","text":"<p>             Bases: <code>Process</code></p> <p>Multiprocessing wrapper with logic to propagate exceptions to the parent process.</p> <p>Source: https://stackoverflow.com/a/33599967/4992248</p> Source code in <code>src/eva/core/utils/multiprocessing.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the process.\"\"\"\n    multiprocessing.Process.__init__(self, *args, **kwargs)\n\n    self._parent_conn, self._child_conn = multiprocessing.Pipe()\n    self._exception = None\n</code></pre>"},{"location":"reference/core/utils/multiprocessing/#eva.core.utils.multiprocessing.Process.exception","title":"<code>exception</code>  <code>property</code>","text":"<p>Property that contains exception information from the process.</p>"},{"location":"reference/core/utils/multiprocessing/#eva.core.utils.multiprocessing.Process.run","title":"<code>run</code>","text":"<p>Run the process.</p> Source code in <code>src/eva/core/utils/multiprocessing.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Run the process.\"\"\"\n    try:\n        multiprocessing.Process.run(self)\n        self._child_conn.send(None)\n    except Exception as e:\n        tb = traceback.format_exc()\n        self._child_conn.send((e, tb))\n</code></pre>"},{"location":"reference/core/utils/multiprocessing/#eva.core.utils.multiprocessing.Process.check_exceptions","title":"<code>check_exceptions</code>","text":"<p>Check for exception propagate it to the parent process.</p> Source code in <code>src/eva/core/utils/multiprocessing.py</code> <pre><code>def check_exceptions(self) -&gt; None:\n    \"\"\"Check for exception propagate it to the parent process.\"\"\"\n    if not self.is_alive():\n        if self.exception:\n            error, traceback = self.exception\n            sys.stderr.write(traceback + \"\\n\")\n            raise error\n</code></pre>"},{"location":"reference/core/utils/workers/","title":"Workers","text":"<p>Reference information for the utils <code>Workers</code> API.</p>"},{"location":"reference/core/utils/workers/#eva.core.utils.workers.main_worker_only","title":"<code>eva.core.utils.workers.main_worker_only</code>","text":"<p>Function decorator which will execute it only on main / worker process.</p> Source code in <code>src/eva/core/utils/workers.py</code> <pre><code>def main_worker_only(func: Callable) -&gt; Any:\n    \"\"\"Function decorator which will execute it only on main / worker process.\"\"\"\n\n    def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Wrapper function for the decorated method.\"\"\"\n        if is_main_worker():\n            return func(*args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"reference/core/utils/workers/#eva.core.utils.workers.is_main_worker","title":"<code>eva.core.utils.workers.is_main_worker</code>","text":"<p>Returns whether the main process / worker is currently used.</p> Source code in <code>src/eva/core/utils/workers.py</code> <pre><code>def is_main_worker() -&gt; bool:\n    \"\"\"Returns whether the main process / worker is currently used.\"\"\"\n    process = multiprocessing.current_process()\n    return process.name == \"MainProcess\"\n</code></pre>"},{"location":"reference/vision/","title":"Vision","text":"<p>Reference information for the <code>Vision</code> API.</p> <p>If you have not already installed the <code>Vision</code>-package, install it with: <pre><code>pip install 'kaiko-eva[vision]'\n</code></pre></p>"},{"location":"reference/vision/utils/","title":"Utils","text":""},{"location":"reference/vision/utils/#eva.vision.utils.io.image","title":"<code>eva.vision.utils.io.image</code>","text":"<p>Image I/O related functions.</p>"},{"location":"reference/vision/utils/#eva.vision.utils.io.image.read_image","title":"<code>read_image</code>","text":"<p>Reads and loads the image from a file path as a RGB.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the image file.</p> required <p>Returns:</p> Type Description <code>NDArray[uint8]</code> <p>The RGB image as a numpy array.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>IOError</code> <p>If the image could not be loaded.</p> Source code in <code>src/eva/vision/utils/io/image.py</code> <pre><code>def read_image(path: str) -&gt; npt.NDArray[np.uint8]:\n    \"\"\"Reads and loads the image from a file path as a RGB.\n\n    Args:\n        path: The path of the image file.\n\n    Returns:\n        The RGB image as a numpy array.\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        IOError: If the image could not be loaded.\n    \"\"\"\n    return read_image_as_array(path, cv2.IMREAD_COLOR)\n</code></pre>"},{"location":"reference/vision/utils/#eva.vision.utils.io.image.read_image_as_array","title":"<code>read_image_as_array</code>","text":"<p>Reads and loads an image file as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the image file.</p> required <code>flags</code> <code>int</code> <p>Specifies the way in which the image should be read.</p> <code>IMREAD_UNCHANGED</code> <p>Returns:</p> Type Description <code>NDArray[uint8]</code> <p>The image as a numpy array.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>IOError</code> <p>If the image could not be loaded.</p> Source code in <code>src/eva/vision/utils/io/image.py</code> <pre><code>def read_image_as_array(path: str, flags: int = cv2.IMREAD_UNCHANGED) -&gt; npt.NDArray[np.uint8]:\n    \"\"\"Reads and loads an image file as a numpy array.\n\n    Args:\n        path: The path to the image file.\n        flags: Specifies the way in which the image should be read.\n\n    Returns:\n        The image as a numpy array.\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        IOError: If the image could not be loaded.\n    \"\"\"\n    _utils.check_file(path)\n    image = cv2.imread(path, flags=flags)\n    if image is None:\n        raise IOError(\n            f\"Input '{path}' could not be loaded. \"\n            \"Please verify that the path is a valid image file.\"\n        )\n\n    if image.ndim == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    if image.ndim == 2 and flags == cv2.IMREAD_COLOR:\n        image = image[:, :, np.newaxis]\n\n    return np.asarray(image).astype(np.uint8)\n</code></pre>"},{"location":"reference/vision/utils/#eva.vision.utils.io.nifti","title":"<code>eva.vision.utils.io.nifti</code>","text":"<p>NIfTI I/O related functions.</p>"},{"location":"reference/vision/utils/#eva.vision.utils.io.nifti.read_nifti_slice","title":"<code>read_nifti_slice</code>","text":"<p>Reads and loads a NIfTI image from a file path as <code>uint8</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the NIfTI file.</p> required <code>slice_index</code> <code>int</code> <p>The image slice index to return. If <code>None</code>, it will return the full 3D image.</p> required <p>Returns:</p> Type Description <code>NDArray[Any]</code> <p>The image as a numpy array.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>ValueError</code> <p>If the input channel is invalid for the image.</p> Source code in <code>src/eva/vision/utils/io/nifti.py</code> <pre><code>def read_nifti_slice(path: str, slice_index: int) -&gt; npt.NDArray[Any]:\n    \"\"\"Reads and loads a NIfTI image from a file path as `uint8`.\n\n    Args:\n        path: The path to the NIfTI file.\n        slice_index: The image slice index to return. If `None`, it will\n            return the full 3D image.\n\n    Returns:\n        The image as a numpy array.\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        ValueError: If the input channel is invalid for the image.\n    \"\"\"\n    _utils.check_file(path)\n    image_data = nib.load(path)  # type: ignore\n    dtype = image_data.get_data_dtype()  # type: ignore\n    image_slice = image_data.slicer[:, :, slice_index : slice_index + 1]  # type: ignore\n    image_array = image_slice.get_fdata()\n    return image_array.astype(dtype)\n</code></pre>"},{"location":"reference/vision/utils/#eva.vision.utils.io.nifti.fetch_total_nifti_slices","title":"<code>fetch_total_nifti_slices</code>","text":"<p>Fetches the total slides of a NIfTI image file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the NIfTI file.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of the total available slides.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>ValueError</code> <p>If the input channel is invalid for the image.</p> Source code in <code>src/eva/vision/utils/io/nifti.py</code> <pre><code>def fetch_total_nifti_slices(path: str) -&gt; int:\n    \"\"\"Fetches the total slides of a NIfTI image file.\n\n    Args:\n        path: The path to the NIfTI file.\n\n    Returns:\n        The number of the total available slides.\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        ValueError: If the input channel is invalid for the image.\n    \"\"\"\n    _utils.check_file(path)\n    image = nib.load(path)  # type: ignore\n    image_shape = image.header.get_data_shape()  # type: ignore\n    return image_shape[-1]\n</code></pre>"},{"location":"reference/vision/data/","title":"Vision Data","text":"<p>Reference information for the <code>Vision Data</code> API.</p>"},{"location":"reference/vision/data/datasets/","title":"Datasets","text":""},{"location":"reference/vision/data/datasets/#visiondataset","title":"VisionDataset","text":""},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.VisionDataset","title":"<code>eva.vision.data.datasets.VisionDataset</code>","text":"<p>             Bases: <code>Dataset</code>, <code>ABC</code>, <code>Generic[DataSample]</code></p> <p>Base dataset class for vision tasks.</p>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.VisionDataset.filename","title":"<code>filename</code>  <code>abstractmethod</code>","text":"<p>Returns the filename of the <code>index</code>'th data sample.</p> <p>Note that this is the relative file path to the root.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the data-sample to select.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The filename of the <code>index</code>'th data sample.</p> Source code in <code>src/eva/vision/data/datasets/vision.py</code> <pre><code>@abc.abstractmethod\ndef filename(self, index: int) -&gt; str:\n    \"\"\"Returns the filename of the `index`'th data sample.\n\n    Note that this is the relative file path to the root.\n\n    Args:\n        index: The index of the data-sample to select.\n\n    Returns:\n        The filename of the `index`'th data sample.\n    \"\"\"\n</code></pre>"},{"location":"reference/vision/data/datasets/#classification-datasets","title":"Classification datasets","text":""},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.BACH","title":"<code>eva.vision.data.datasets.BACH</code>","text":"<p>             Bases: <code>ImageClassification</code></p> <p>Dataset class for BACH images and corresponding targets.</p> <p>The dataset is split into train and validation by taking into account the patient IDs to avoid any data leakage.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Path to the root directory of the dataset. The dataset will be downloaded and extracted here, if it does not already exist.</p> required <code>split</code> <code>Literal['train', 'val'] | None</code> <p>Dataset split to use. If <code>None</code>, the entire dataset is used.</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download the data for the specified split. Note that the download will be executed only by additionally calling the :meth:<code>prepare_data</code> method and if the data does not yet exist on disk.</p> <code>False</code> <code>image_transforms</code> <code>Callable | None</code> <p>A function/transform that takes in an image and returns a transformed version.</p> <code>None</code> <code>target_transforms</code> <code>Callable | None</code> <p>A function/transform that takes in the target and transforms it.</p> <code>None</code> Source code in <code>src/eva/vision/data/datasets/classification/bach.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    split: Literal[\"train\", \"val\"] | None = None,\n    download: bool = False,\n    image_transforms: Callable | None = None,\n    target_transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    The dataset is split into train and validation by taking into account\n    the patient IDs to avoid any data leakage.\n\n    Args:\n        root: Path to the root directory of the dataset. The dataset will\n            be downloaded and extracted here, if it does not already exist.\n        split: Dataset split to use. If `None`, the entire dataset is used.\n        download: Whether to download the data for the specified split.\n            Note that the download will be executed only by additionally\n            calling the :meth:`prepare_data` method and if the data does\n            not yet exist on disk.\n        image_transforms: A function/transform that takes in an image\n            and returns a transformed version.\n        target_transforms: A function/transform that takes in the target\n            and transforms it.\n    \"\"\"\n    super().__init__(\n        image_transforms=image_transforms,\n        target_transforms=target_transforms,\n    )\n\n    self._root = root\n    self._split = split\n    self._download = download\n\n    self._samples: List[Tuple[str, int]] = []\n    self._indices: List[int] = []\n</code></pre>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.BACH.dataset_path","title":"<code>dataset_path: str</code>  <code>property</code>","text":"<p>Returns the path of the image data of the dataset.</p>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.PatchCamelyon","title":"<code>eva.vision.data.datasets.PatchCamelyon</code>","text":"<p>             Bases: <code>ImageClassification</code></p> <p>Dataset class for PatchCamelyon images and corresponding targets.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>The path to the dataset root. This path should contain the uncompressed h5 files and the metadata.</p> required <code>split</code> <code>Literal['train', 'val', 'test']</code> <p>The dataset split for training, validation, or testing.</p> required <code>download</code> <code>bool</code> <p>Whether to download the data for the specified split. Note that the download will be executed only by additionally calling the :meth:<code>prepare_data</code> method.</p> <code>False</code> <code>image_transforms</code> <code>Callable | None</code> <p>A function/transform that takes in an image and returns a transformed version.</p> <code>None</code> <code>target_transforms</code> <code>Callable | None</code> <p>A function/transform that takes in the target and transforms it.</p> <code>None</code> Source code in <code>src/eva/vision/data/datasets/classification/patch_camelyon.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    split: Literal[\"train\", \"val\", \"test\"],\n    download: bool = False,\n    image_transforms: Callable | None = None,\n    target_transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initializes the dataset.\n\n    Args:\n        root: The path to the dataset root. This path should contain\n            the uncompressed h5 files and the metadata.\n        split: The dataset split for training, validation, or testing.\n        download: Whether to download the data for the specified split.\n            Note that the download will be executed only by additionally\n            calling the :meth:`prepare_data` method.\n        image_transforms: A function/transform that takes in an image\n            and returns a transformed version.\n        target_transforms: A function/transform that takes in the target\n            and transforms it.\n    \"\"\"\n    super().__init__(\n        image_transforms=image_transforms,\n        target_transforms=target_transforms,\n    )\n\n    self._root = root\n    self._split = split\n    self._download = download\n</code></pre>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.TotalSegmentatorClassification","title":"<code>eva.vision.data.datasets.TotalSegmentatorClassification</code>","text":"<p>             Bases: <code>ImageClassification</code></p> <p>TotalSegmentator multi-label classification dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Path to the root directory of the dataset. The dataset will be downloaded and extracted here, if it does not already exist.</p> required <code>split</code> <code>Literal['train', 'val'] | None</code> <p>Dataset split to use. If None, the entire dataset is used.</p> required <code>version</code> <code>Literal['small', 'full']</code> <p>The version of the dataset to initialize.</p> <code>'small'</code> <code>download</code> <code>bool</code> <p>Whether to download the data for the specified split. Note that the download will be executed only by additionally calling the :meth:<code>prepare_data</code> method and if the data does not exist yet on disk.</p> <code>False</code> <code>image_transforms</code> <code>Callable | None</code> <p>A function/transform that takes in an image and returns a transformed version.</p> <code>None</code> <code>target_transforms</code> <code>Callable | None</code> <p>A function/transform that takes in the target and transforms it.</p> <code>None</code> Source code in <code>src/eva/vision/data/datasets/classification/total_segmentator.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    split: Literal[\"train\", \"val\"] | None,\n    version: Literal[\"small\", \"full\"] = \"small\",\n    download: bool = False,\n    image_transforms: Callable | None = None,\n    target_transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initialize dataset.\n\n    Args:\n        root: Path to the root directory of the dataset. The dataset will\n            be downloaded and extracted here, if it does not already exist.\n        split: Dataset split to use. If None, the entire dataset is used.\n        version: The version of the dataset to initialize.\n        download: Whether to download the data for the specified split.\n            Note that the download will be executed only by additionally\n            calling the :meth:`prepare_data` method and if the data does not\n            exist yet on disk.\n        image_transforms: A function/transform that takes in an image\n            and returns a transformed version.\n        target_transforms: A function/transform that takes in the target\n            and transforms it.\n    \"\"\"\n    super().__init__(\n        image_transforms=image_transforms,\n        target_transforms=target_transforms,\n    )\n\n    self._root = root\n    self._split = split\n    self._version = version\n    self._download = download\n\n    self._samples_dirs: List[str] = []\n    self._indices: List[int] = []\n</code></pre>"},{"location":"reference/vision/data/datasets/#segmentation-datasets","title":"Segmentation datasets","text":""},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.ImageSegmentation","title":"<code>eva.vision.data.datasets.ImageSegmentation</code>","text":"<p>             Bases: <code>VisionDataset[Tuple[ndarray, ndarray]]</code>, <code>ABC</code></p> <p>Image segmentation abstract dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_transforms</code> <code>Callable | None</code> <p>A function/transform that takes in an image and returns a transformed version.</p> <code>None</code> <code>target_transforms</code> <code>Callable | None</code> <p>A function/transform that takes in the target and transforms it.</p> <code>None</code> <code>image_target_transforms</code> <code>Callable | None</code> <p>A function/transforms that takes in an image and a label and returns the transformed versions of both. This transform happens after the <code>image_transforms</code> and <code>target_transforms</code>.</p> <code>None</code> Source code in <code>src/eva/vision/data/datasets/segmentation/base.py</code> <pre><code>def __init__(\n    self,\n    image_transforms: Callable | None = None,\n    target_transforms: Callable | None = None,\n    image_target_transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initializes the image segmentation base class.\n\n    Args:\n        image_transforms: A function/transform that takes in an image\n            and returns a transformed version.\n        target_transforms: A function/transform that takes in the target\n            and transforms it.\n        image_target_transforms: A function/transforms that takes in an\n            image and a label and returns the transformed versions of both.\n            This transform happens after the `image_transforms` and\n            `target_transforms`.\n    \"\"\"\n    super().__init__()\n\n    self._image_transforms = image_transforms\n    self._target_transforms = target_transforms\n    self._image_target_transforms = image_target_transforms\n</code></pre>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.ImageSegmentation.classes","title":"<code>classes: List[str] | None</code>  <code>property</code>","text":"<p>Returns the list with names of the dataset names.</p>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.ImageSegmentation.class_to_idx","title":"<code>class_to_idx: Dict[str, int] | None</code>  <code>property</code>","text":"<p>Returns a mapping of the class name to its target index.</p>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.ImageSegmentation.load_metadata","title":"<code>load_metadata</code>","text":"<p>Returns the dataset metadata.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | None</code> <p>The index of the data sample to return the metadata of. If <code>None</code>, it will return the metadata of the current dataset.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any] | List[Dict[str, Any]] | None</code> <p>The sample metadata.</p> Source code in <code>src/eva/vision/data/datasets/segmentation/base.py</code> <pre><code>def load_metadata(self, index: int | None) -&gt; Dict[str, Any] | List[Dict[str, Any]] | None:\n    \"\"\"Returns the dataset metadata.\n\n    Args:\n        index: The index of the data sample to return the metadata of.\n            If `None`, it will return the metadata of the current dataset.\n\n    Returns:\n        The sample metadata.\n    \"\"\"\n</code></pre>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.ImageSegmentation.load_image","title":"<code>load_image</code>  <code>abstractmethod</code>","text":"<p>Loads and returns the <code>index</code>'th image sample.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the data sample to load.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The image as a numpy array.</p> Source code in <code>src/eva/vision/data/datasets/segmentation/base.py</code> <pre><code>@abc.abstractmethod\ndef load_image(self, index: int) -&gt; np.ndarray:\n    \"\"\"Loads and returns the `index`'th image sample.\n\n    Args:\n        index: The index of the data sample to load.\n\n    Returns:\n        The image as a numpy array.\n    \"\"\"\n</code></pre>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.ImageSegmentation.load_mask","title":"<code>load_mask</code>  <code>abstractmethod</code>","text":"<p>Returns the <code>index</code>'th target mask sample.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the data sample target mask to load.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The sample mask as a stack of binary mask arrays (label, height, width).</p> Source code in <code>src/eva/vision/data/datasets/segmentation/base.py</code> <pre><code>@abc.abstractmethod\ndef load_mask(self, index: int) -&gt; np.ndarray:\n    \"\"\"Returns the `index`'th target mask sample.\n\n    Args:\n        index: The index of the data sample target mask to load.\n\n    Returns:\n        The sample mask as a stack of binary mask arrays (label, height, width).\n    \"\"\"\n</code></pre>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.TotalSegmentator2D","title":"<code>eva.vision.data.datasets.TotalSegmentator2D</code>","text":"<p>             Bases: <code>ImageSegmentation</code></p> <p>TotalSegmentator 2D segmentation dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Path to the root directory of the dataset. The dataset will be downloaded and extracted here, if it does not already exist.</p> required <code>split</code> <code>Literal['train', 'val'] | None</code> <p>Dataset split to use. If <code>None</code>, the entire dataset is used.</p> required <code>version</code> <code>Literal['small', 'full']</code> <p>The version of the dataset to initialize.</p> <code>'small'</code> <code>download</code> <code>bool</code> <p>Whether to download the data for the specified split. Note that the download will be executed only by additionally calling the :meth:<code>prepare_data</code> method and if the data does not exist yet on disk.</p> <code>False</code> <code>image_transforms</code> <code>Callable | None</code> <p>A function/transform that takes in an image and returns a transformed version.</p> <code>None</code> <code>target_transforms</code> <code>Callable | None</code> <p>A function/transform that takes in the target and transforms it.</p> <code>None</code> <code>image_target_transforms</code> <code>Callable | None</code> <p>A function/transforms that takes in an image and a label and returns the transformed versions of both. This transform happens after the <code>image_transforms</code> and <code>target_transforms</code>.</p> <code>None</code> Source code in <code>src/eva/vision/data/datasets/segmentation/total_segmentator.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    split: Literal[\"train\", \"val\"] | None,\n    version: Literal[\"small\", \"full\"] = \"small\",\n    download: bool = False,\n    image_transforms: Callable | None = None,\n    target_transforms: Callable | None = None,\n    image_target_transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initialize dataset.\n\n    Args:\n        root: Path to the root directory of the dataset. The dataset will\n            be downloaded and extracted here, if it does not already exist.\n        split: Dataset split to use. If `None`, the entire dataset is used.\n        version: The version of the dataset to initialize.\n        download: Whether to download the data for the specified split.\n            Note that the download will be executed only by additionally\n            calling the :meth:`prepare_data` method and if the data does not\n            exist yet on disk.\n        image_transforms: A function/transform that takes in an image\n            and returns a transformed version.\n        target_transforms: A function/transform that takes in the target\n            and transforms it.\n        image_target_transforms: A function/transforms that takes in an\n            image and a label and returns the transformed versions of both.\n            This transform happens after the `image_transforms` and\n            `target_transforms`.\n    \"\"\"\n    super().__init__(\n        image_transforms=image_transforms,\n        target_transforms=target_transforms,\n        image_target_transforms=image_target_transforms,\n    )\n\n    self._root = root\n    self._split = split\n    self._version = version\n    self._download = download\n\n    self._samples_dirs: List[str] = []\n    self._indices: List[int] = []\n</code></pre>"},{"location":"reference/vision/data/transforms/","title":"Transforms","text":""},{"location":"reference/vision/data/transforms/#eva.core.data.transforms.dtype.ArrayToTensor","title":"<code>eva.core.data.transforms.dtype.ArrayToTensor</code>","text":"<p>Converts a numpy array to a torch tensor.</p>"},{"location":"reference/vision/data/transforms/#eva.core.data.transforms.dtype.ArrayToFloatTensor","title":"<code>eva.core.data.transforms.dtype.ArrayToFloatTensor</code>","text":"<p>             Bases: <code>ArrayToTensor</code></p> <p>Converts a numpy array to a torch tensor and casts it to float.</p>"},{"location":"reference/vision/data/transforms/#eva.vision.data.transforms.ResizeAndCrop","title":"<code>eva.vision.data.transforms.ResizeAndCrop</code>","text":"<p>             Bases: <code>Compose</code></p> <p>Resizes, crops and normalizes an input image while preserving its aspect ratio.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int | Sequence[int]</code> <p>Desired output size of the crop. If size is an <code>int</code> instead of sequence like (h, w), a square crop (size, size) is made.</p> <code>224</code> <code>mean</code> <code>Sequence[float]</code> <p>Sequence of means for each image channel.</p> <code>(0.5, 0.5, 0.5)</code> <code>std</code> <code>Sequence[float]</code> <p>Sequence of standard deviations for each image channel.</p> <code>(0.5, 0.5, 0.5)</code> Source code in <code>src/eva/vision/data/transforms/common/resize_and_crop.py</code> <pre><code>def __init__(\n    self,\n    size: int | Sequence[int] = 224,\n    mean: Sequence[float] = (0.5, 0.5, 0.5),\n    std: Sequence[float] = (0.5, 0.5, 0.5),\n) -&gt; None:\n    \"\"\"Initializes the transform object.\n\n    Args:\n        size: Desired output size of the crop. If size is an `int` instead\n            of sequence like (h, w), a square crop (size, size) is made.\n        mean: Sequence of means for each image channel.\n        std: Sequence of standard deviations for each image channel.\n    \"\"\"\n    self._size = size\n    self._mean = mean\n    self._std = std\n\n    super().__init__(transforms=self._build_transforms())\n</code></pre>"},{"location":"reference/vision/models/networks/","title":"Networks","text":""},{"location":"reference/vision/models/networks/#eva.vision.models.networks.ABMIL","title":"<code>eva.vision.models.networks.ABMIL</code>","text":"<p>             Bases: <code>Module</code></p> <p>ABMIL network for multiple instance learning classification tasks.</p> <p>Takes an array of patch level embeddings per slide as input. This implementation supports batched inputs of shape (<code>batch_size</code>, <code>n_instances</code>, <code>input_size</code>). For slides with less than <code>n_instances</code> patches, you can apply padding and provide a mask tensor to the forward pass.</p> <p>The original implementation from [1] was used as a reference: https://github.com/AMLab-Amsterdam/AttentionDeepMIL/blob/master/model.py</p> Notes <ul> <li>use_bias: The paper didn't use bias in their formalism, but their published example code inadvertently does.</li> <li>To prevent dot product similarities near-equal due to concentration of measure as a consequence of large input embedding dimensionality (&gt;128), we added the option to project the input embeddings to a lower dimensionality</li> </ul> <p>[1] Maximilian Ilse, Jakub M. Tomczak, Max Welling, \"Attention-based Deep Multiple     Instance Learning\", 2018     https://arxiv.org/abs/1802.04712</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>input embedding dimension</p> required <code>output_size</code> <code>int</code> <p>number of classes</p> required <code>projected_input_size</code> <code>int | None</code> <p>size of the projected input. if <code>None</code>, no projection is performed.</p> required <code>hidden_size_attention</code> <code>int</code> <p>hidden dimension in attention network</p> <code>128</code> <code>hidden_sizes_mlp</code> <code>tuple</code> <p>dimensions for hidden layers in last mlp</p> <code>(128, 64)</code> <code>use_bias</code> <code>bool</code> <p>whether to use bias in the attention network</p> <code>True</code> <code>dropout_input_embeddings</code> <code>float</code> <p>dropout rate for the input embeddings</p> <code>0.0</code> <code>dropout_attention</code> <code>float</code> <p>dropout rate for the attention network and classifier</p> <code>0.0</code> <code>dropout_mlp</code> <code>float</code> <p>dropout rate for the final MLP network</p> <code>0.0</code> <code>pad_value</code> <code>int | float | None</code> <p>Value indicating padding in the input tensor. If specified, entries with this value in the will be masked. If set to <code>None</code>, no masking is applied.</p> <code>float('-inf')</code> Source code in <code>src/eva/vision/models/networks/abmil.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    output_size: int,\n    projected_input_size: int | None,\n    hidden_size_attention: int = 128,\n    hidden_sizes_mlp: tuple = (128, 64),\n    use_bias: bool = True,\n    dropout_input_embeddings: float = 0.0,\n    dropout_attention: float = 0.0,\n    dropout_mlp: float = 0.0,\n    pad_value: int | float | None = float(\"-inf\"),\n) -&gt; None:\n    \"\"\"Initializes the ABMIL network.\n\n    Args:\n        input_size: input embedding dimension\n        output_size: number of classes\n        projected_input_size: size of the projected input. if `None`, no projection is\n            performed.\n        hidden_size_attention: hidden dimension in attention network\n        hidden_sizes_mlp: dimensions for hidden layers in last mlp\n        use_bias: whether to use bias in the attention network\n        dropout_input_embeddings: dropout rate for the input embeddings\n        dropout_attention: dropout rate for the attention network and classifier\n        dropout_mlp: dropout rate for the final MLP network\n        pad_value: Value indicating padding in the input tensor. If specified, entries with\n            this value in the will be masked. If set to `None`, no masking is applied.\n    \"\"\"\n    super().__init__()\n\n    self._pad_value = pad_value\n\n    if projected_input_size:\n        self.projector = nn.Sequential(\n            nn.Linear(input_size, projected_input_size, bias=True),\n            nn.Dropout(p=dropout_input_embeddings),\n        )\n        input_size = projected_input_size\n    else:\n        self.projector = nn.Dropout(p=dropout_input_embeddings)\n\n    self.gated_attention = GatedAttention(\n        input_dim=input_size,\n        hidden_dim=hidden_size_attention,\n        dropout=dropout_attention,\n        n_classes=1,\n        use_bias=use_bias,\n    )\n\n    self.classifier = MLP(\n        input_size=input_size,\n        output_size=output_size,\n        hidden_layer_sizes=hidden_sizes_mlp,\n        dropout=dropout_mlp,\n        hidden_activation_fn=nn.ReLU,\n    )\n</code></pre>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.ABMIL.forward","title":"<code>forward</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Tensor with expected shape of (batch_size, n_instances, input_size).</p> required Source code in <code>src/eva/vision/models/networks/abmil.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Args:\n        input_tensor: Tensor with expected shape of (batch_size, n_instances, input_size).\n    \"\"\"\n    input_tensor, mask = self._mask_values(input_tensor, self._pad_value)\n\n    # (batch_size, n_instances, input_size) -&gt; (batch_size, n_instances, projected_input_size)\n    input_tensor = self.projector(input_tensor)\n\n    attention_logits = self.gated_attention(input_tensor)  # (batch_size, n_instances, 1)\n    if mask is not None:\n        # fill masked values with -inf, which will yield 0s after softmax\n        attention_logits = attention_logits.masked_fill(mask, float(\"-inf\"))\n\n    attention_weights = nn.functional.softmax(attention_logits, dim=1)\n    # (batch_size, n_instances, 1)\n\n    attention_result = torch.matmul(torch.transpose(attention_weights, 1, 2), input_tensor)\n    # (batch_size, 1, hidden_size_attention)\n\n    attention_result = torch.squeeze(attention_result, 1)  # (batch_size, hidden_size_attention)\n\n    return self.classifier(attention_result)  # (batch_size, output_size)\n</code></pre>"},{"location":"reference/vision/utils/io/","title":"IO","text":""},{"location":"reference/vision/utils/io/#eva.vision.utils.io.image","title":"<code>eva.vision.utils.io.image</code>","text":"<p>Image I/O related functions.</p>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.image.read_image","title":"<code>read_image</code>","text":"<p>Reads and loads the image from a file path as a RGB.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the image file.</p> required <p>Returns:</p> Type Description <code>NDArray[uint8]</code> <p>The RGB image as a numpy array.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>IOError</code> <p>If the image could not be loaded.</p> Source code in <code>src/eva/vision/utils/io/image.py</code> <pre><code>def read_image(path: str) -&gt; npt.NDArray[np.uint8]:\n    \"\"\"Reads and loads the image from a file path as a RGB.\n\n    Args:\n        path: The path of the image file.\n\n    Returns:\n        The RGB image as a numpy array.\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        IOError: If the image could not be loaded.\n    \"\"\"\n    return read_image_as_array(path, cv2.IMREAD_COLOR)\n</code></pre>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.image.read_image_as_array","title":"<code>read_image_as_array</code>","text":"<p>Reads and loads an image file as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the image file.</p> required <code>flags</code> <code>int</code> <p>Specifies the way in which the image should be read.</p> <code>IMREAD_UNCHANGED</code> <p>Returns:</p> Type Description <code>NDArray[uint8]</code> <p>The image as a numpy array.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>IOError</code> <p>If the image could not be loaded.</p> Source code in <code>src/eva/vision/utils/io/image.py</code> <pre><code>def read_image_as_array(path: str, flags: int = cv2.IMREAD_UNCHANGED) -&gt; npt.NDArray[np.uint8]:\n    \"\"\"Reads and loads an image file as a numpy array.\n\n    Args:\n        path: The path to the image file.\n        flags: Specifies the way in which the image should be read.\n\n    Returns:\n        The image as a numpy array.\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        IOError: If the image could not be loaded.\n    \"\"\"\n    _utils.check_file(path)\n    image = cv2.imread(path, flags=flags)\n    if image is None:\n        raise IOError(\n            f\"Input '{path}' could not be loaded. \"\n            \"Please verify that the path is a valid image file.\"\n        )\n\n    if image.ndim == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    if image.ndim == 2 and flags == cv2.IMREAD_COLOR:\n        image = image[:, :, np.newaxis]\n\n    return np.asarray(image).astype(np.uint8)\n</code></pre>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.nifti","title":"<code>eva.vision.utils.io.nifti</code>","text":"<p>NIfTI I/O related functions.</p>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.nifti.read_nifti_slice","title":"<code>read_nifti_slice</code>","text":"<p>Reads and loads a NIfTI image from a file path as <code>uint8</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the NIfTI file.</p> required <code>slice_index</code> <code>int</code> <p>The image slice index to return. If <code>None</code>, it will return the full 3D image.</p> required <p>Returns:</p> Type Description <code>NDArray[Any]</code> <p>The image as a numpy array.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>ValueError</code> <p>If the input channel is invalid for the image.</p> Source code in <code>src/eva/vision/utils/io/nifti.py</code> <pre><code>def read_nifti_slice(path: str, slice_index: int) -&gt; npt.NDArray[Any]:\n    \"\"\"Reads and loads a NIfTI image from a file path as `uint8`.\n\n    Args:\n        path: The path to the NIfTI file.\n        slice_index: The image slice index to return. If `None`, it will\n            return the full 3D image.\n\n    Returns:\n        The image as a numpy array.\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        ValueError: If the input channel is invalid for the image.\n    \"\"\"\n    _utils.check_file(path)\n    image_data = nib.load(path)  # type: ignore\n    dtype = image_data.get_data_dtype()  # type: ignore\n    image_slice = image_data.slicer[:, :, slice_index : slice_index + 1]  # type: ignore\n    image_array = image_slice.get_fdata()\n    return image_array.astype(dtype)\n</code></pre>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.nifti.fetch_total_nifti_slices","title":"<code>fetch_total_nifti_slices</code>","text":"<p>Fetches the total slides of a NIfTI image file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the NIfTI file.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of the total available slides.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>ValueError</code> <p>If the input channel is invalid for the image.</p> Source code in <code>src/eva/vision/utils/io/nifti.py</code> <pre><code>def fetch_total_nifti_slices(path: str) -&gt; int:\n    \"\"\"Fetches the total slides of a NIfTI image file.\n\n    Args:\n        path: The path to the NIfTI file.\n\n    Returns:\n        The number of the total available slides.\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        ValueError: If the input channel is invalid for the image.\n    \"\"\"\n    _utils.check_file(path)\n    image = nib.load(path)  # type: ignore\n    image_shape = image.header.get_data_shape()  # type: ignore\n    return image_shape[-1]\n</code></pre>"},{"location":"user-guide/","title":"User Guide","text":"<p>Here you can find everything you need to install, understand and interact with eva.</p>"},{"location":"user-guide/#getting-started","title":"Getting started","text":"<p>Install eva on your machine and learn how to use eva.</p>"},{"location":"user-guide/#tutorials","title":"Tutorials","text":"<p>To familiarize yourself with eva, try out some of our tutorials.</p> <ul> <li>Go through Offline vs. online evaluations to run eva workflows.</li> <li>Train and evaluate a ResNet model from scratch.</li> </ul>"},{"location":"user-guide/#advanced-user-guide","title":"Advanced user guide","text":"<p>Get to know eva in more depth by studying our advanced user guides.</p> <ul> <li>See how to replicate our evaluations of public FM-checkpoints.</li> <li>Understand how to use eva's Model Wrapper API to load models from different formats and sources.</li> </ul>"},{"location":"user-guide/advanced/model_wrappers/","title":"Model Wrappers","text":"<p>This document shows how to use eva's Model Wrapper API (<code>eva.models.networks.wrappers</code>) to load different model formats from a series of sources such as PyTorch Hub, HuggingFace Model Hub and ONNX. </p>"},{"location":"user-guide/advanced/model_wrappers/#loading-pytorch-models","title":"Loading PyTorch models","text":"<p>The eva framework is built on top of PyTorch Lightning and thus naturally supports loading PyTorch models. You just need to specify the class path of your model in the backbone section of the <code>.yaml</code> config file.</p> <pre><code>backbone:\n  class_path: path.to.your.ModelClass\n  init_args:\n    arg_1: ...\n    arg_2: ...\n</code></pre> <p>Note that your <code>ModelClass</code> should subclass <code>torch.nn.Module</code> and implement the <code>forward()</code> method to return embedding tensors of shape <code>[embedding_dim]</code>.</p>"},{"location":"user-guide/advanced/model_wrappers/#pytorch-hub","title":"PyTorch Hub","text":"<p>To load models from PyTorch Hub or other torch model providers, the easiest way is to use the <code>ModelFromFunction</code> wrapper class:</p> <pre><code>backbone:\n  class_path: eva.models.networks.wrappers.ModelFromFunction\n  init_args:\n    path: torch.hub.load\n    arguments:\n      repo_or_dir: facebookresearch/dino:main\n      model: dino_vits16\n      pretrained: false\n    checkpoint_path: path/to/your/checkpoint.torch\n</code></pre> <p>Note that if a <code>checkpoint_path</code> is provided, <code>ModelFromFunction</code> will automatically initialize the specified model using the provided weights from that checkpoint file.</p>"},{"location":"user-guide/advanced/model_wrappers/#timm","title":"timm","text":"<p>Similar to the above example, we can easily load models using the common vision library <code>timm</code>: <pre><code>backbone:\n  class_path: eva.models.networks.wrappers.ModelFromFunction\n  init_args:\n    path: timm.create_model\n    arguments:\n      model_name: resnet18\n      pretrained: true\n</code></pre></p>"},{"location":"user-guide/advanced/model_wrappers/#loading-models-from-huggingface-hub","title":"Loading models from HuggingFace Hub","text":"<p>For loading models from HuggingFace Hub, eva provides a custom wrapper class <code>HuggingFaceModel</code> which can be used as follows:</p> <pre><code>backbone:\n  class_path: eva.models.networks.wrappers.HuggingFaceModel\n  init_args:\n    model_name_or_path: owkin/phikon\n    tensor_transforms: \n      class_path: eva.models.networks.transforms.ExtractCLSFeatures\n</code></pre> <p>In the above example, the forward pass implemented by the <code>owkin/phikon</code> model returns an output tensor containing the hidden states of all input tokens. In order to extract the state corresponding to the CLS token only, we can specify a transformation via the <code>tensor_transforms</code> argument which will be applied to the model output.</p>"},{"location":"user-guide/advanced/model_wrappers/#loading-onnx-models","title":"Loading ONNX models","text":"<p><code>.onnx</code> model checkpoints can be loaded using the <code>ONNXModel</code> wrapper class as follows:</p> <pre><code>class_path: eva.models.networks.wrappers.ONNXModel\ninit_args:\n  path: path/to/model.onnx\n  device: cuda\n</code></pre>"},{"location":"user-guide/advanced/model_wrappers/#implementing-custom-model-wrappers","title":"Implementing custom model wrappers","text":"<p>You can also implement your own model wrapper classes, in case your model format is not supported by the wrapper classes that eva already provides. To do so, you need to subclass <code>eva.models.networks.wrappers.BaseModel</code> and implement the following abstract methods: </p> <ul> <li><code>load_model</code>: Returns an instantiated model object &amp; loads pre-trained model weights from a checkpoint if available. </li> <li><code>model_forward</code>: Implements the forward pass of the model and returns the output as a <code>torch.Tensor</code> of shape <code>[embedding_dim]</code></li> </ul> <p>You can take the implementations of <code>ModelFromFunction</code>, <code>HuggingFaceModel</code> and <code>ONNXModel</code> wrappers as a reference.</p>"},{"location":"user-guide/advanced/replicate_evaluations/","title":"Replicate evaluations","text":"<p>To produce the evaluation results presented here, you can run eva with the settings below.</p> <p>Make sure to replace <code>&lt;task&gt;</code> in the commands below with <code>bach</code>, <code>crc</code>, <code>mhist</code> or <code>patch_camelyon</code>.</p> <p>Note that to run the commands below you will need to first download the data. BACH, CRC and PatchCamelyon provide automatic download by setting the argument <code>download: true</code> in their respective config-files. In the case of MHIST you will need to download the data manually by following the instructions provided here.</p>"},{"location":"user-guide/advanced/replicate_evaluations/#dino-vit-s16-random-weights","title":"DINO ViT-S16 (random weights)","text":"<p>Evaluating the backbone with randomly initialized weights serves as a baseline to compare the pretrained FMs to an FM that produces embeddings without any prior learning on image tasks. To evaluate, run:</p> <pre><code>PRETRAINED=false \\\nEMBEDDINGS_ROOT=\"./data/embeddings/dino_vits16_random\" \\\neva predict_fit --config configs/vision/dino_vit/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#dino-vit-s16-imagenet","title":"DINO ViT-S16 (ImageNet)","text":"<p>The next baseline model, uses a pretrained ViT-S16 backbone with ImageNet weights. To evaluate, run:</p> <pre><code>EMBEDDINGS_ROOT=\"./data/embeddings/dino_vits16_imagenet\" \\\neva predict_fit --config configs/vision/dino_vit/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#dino-vit-b8-imagenet","title":"DINO ViT-B8 (ImageNet)","text":"<p>To evaluate performance on the larger ViT-B8 backbone pretrained on ImageNet, run: <pre><code>EMBEDDINGS_ROOT=\"./data/embeddings/dino_vitb8_imagenet\" \\\nDINO_BACKBONE=dino_vitb8 \\\nIN_FEATURES=768 \\\neva predict_fit --config configs/vision/dino_vit/offline/&lt;task&gt;.yaml\n</code></pre></p>"},{"location":"user-guide/advanced/replicate_evaluations/#lunit-dino-vit-s16-tcga","title":"Lunit - DINO ViT-S16 (TCGA)","text":"<p>Lunit, released the weights for a DINO ViT-S16 backbone, pretrained on TCGA data on GitHub. To evaluate, run:</p> <pre><code>PRETRAINED=false \\\nEMBEDDINGS_ROOT=\"./data/embeddings/dino_vits16_lunit\" \\\nCHECKPOINT_PATH=\"https://github.com/lunit-io/benchmark-ssl-pathology/releases/download/pretrained-weights/dino_vit_small_patch16_ep200.torch\" \\\nNORMALIZE_MEAN=[0.70322989,0.53606487,0.66096631] \\\nNORMALIZE_STD=[0.21716536,0.26081574,0.20723464] \\\neva predict_fit --config configs/vision/dino_vit/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#owkin-ibot-vit-b16-tcga","title":"Owkin - iBOT ViT-B16 (TCGA)","text":"<p>Owkin released the weights for \"Phikon\", an FM trained with iBOT on TCGA data, via HuggingFace. To evaluate, run:</p> <pre><code>EMBEDDINGS_ROOT=\"./data/embeddings/dino_vitb16_owkin\" \\\neva predict_fit --config configs/vision/owkin/phikon/offline/&lt;task&gt;.yaml\n</code></pre> <p>Note: since eva provides the config files to evaluate tasks with the Phikon FM in  \"configs/vision/owkin/phikon/offline\", it is not necessary to set the environment variables needed for the runs above.</p>"},{"location":"user-guide/advanced/replicate_evaluations/#kaikoai-dino-vit-s16-tcga","title":"kaiko.ai - DINO ViT-S16 (TCGA)","text":"<p>To evaluate kaiko.ai's FM with DINO ViT-S16 backbone, pretrained on TCGA data  on GitHub, run:</p> <pre><code>PRETRAINED=false \\\nEMBEDDINGS_ROOT=\"./data/embeddings/dino_vits16_kaiko\" \\\nCHECKPOINT_PATH=[TBD*] \\\nNORMALIZE_MEAN=[0.5,0.5,0.5] \\\nNORMALIZE_STD=[0.5,0.5,0.5] \\\neva predict_fit --config configs/vision/dino_vit/offline/&lt;task&gt;.yaml\n</code></pre> <p>* path to public checkpoint will be added when available.</p>"},{"location":"user-guide/advanced/replicate_evaluations/#kaikoai-dino-vit-s8-tcga","title":"kaiko.ai - DINO ViT-S8 (TCGA)","text":"<p>To evaluate kaiko.ai's FM with DINO ViT-S8 backbone, pretrained on TCGA data  on GitHub, run:</p> <pre><code>PRETRAINED=false \\\nEMBEDDINGS_ROOT=\"./data/embeddings/dino_vits8_kaiko\" \\\nDINO_BACKBONE=dino_vits8 \\\nCHECKPOINT_PATH=[TBD*] \\\nNORMALIZE_MEAN=[0.5,0.5,0.5] \\\nNORMALIZE_STD=[0.5,0.5,0.5] \\\neva predict_fit --config configs/vision/dino_vit/offline/&lt;task&gt;.yaml\n</code></pre> <p>* path to public checkpoint will be added when available.</p>"},{"location":"user-guide/advanced/replicate_evaluations/#kaikoai-dino-vit-b16-tcga","title":"kaiko.ai - DINO ViT-B16 (TCGA)","text":"<p>To evaluate kaiko.ai's FM with the larger DINO ViT-B16 backbone, pretrained on TCGA data, run:</p> <pre><code>PRETRAINED=false \\\nEMBEDDINGS_ROOT=\"./data/embeddings/dino_vitb16_kaiko\" \\\nDINO_BACKBONE=dino_vitb16 \\\nCHECKPOINT_PATH=[TBD*] \\\nIN_FEATURES=768 \\\nNORMALIZE_MEAN=[0.5,0.5,0.5] \\\nNORMALIZE_STD=[0.5,0.5,0.5] \\\neva predict_fit --config configs/vision/dino_vit/offline/&lt;task&gt;.yaml\n</code></pre> <p>* path to public checkpoint will be added when available.</p>"},{"location":"user-guide/advanced/replicate_evaluations/#kaikoai-dino-vit-b8-tcga","title":"kaiko.ai - DINO ViT-B8 (TCGA)","text":"<p>To evaluate kaiko.ai's FM with the larger DINO ViT-B8 backbone, pretrained on TCGA data, run:</p> <pre><code>PRETRAINED=false \\\nEMBEDDINGS_ROOT=\"./data/embeddings/dino_vitb8_kaiko\" \\\nDINO_BACKBONE=dino_vitb8 \\\nCHECKPOINT_PATH=[TBD*] \\\nIN_FEATURES=768 \\\nNORMALIZE_MEAN=[0.5,0.5,0.5] \\\nNORMALIZE_STD=[0.5,0.5,0.5] \\\neva predict_fit --config configs/vision/dino_vit/offline/&lt;task&gt;.yaml\n</code></pre> <p>* path to public checkpoint will be added when available.</p>"},{"location":"user-guide/advanced/replicate_evaluations/#kaikoai-dinov2-vit-l14-tcga","title":"kaiko.ai - DINOv2 ViT-L14 (TCGA)","text":"<p>To evaluate kaiko.ai's FM with the larger DINOv2 ViT-L14 backbone, pretrained on TCGA data, run:</p> <pre><code>PRETRAINED=false \\\nEMBEDDINGS_ROOT=\"./data/embeddings/dinov2_vitl14_kaiko\" \\\nREPO_OR_DIR=facebookresearch/dinov2:main \\\nDINO_BACKBONE=dinov2_vitl14_reg \\\nFORCE_RELOAD=true \\\nCHECKPOINT_PATH=[TBD*] \\\nIN_FEATURES=1024 \\\nNORMALIZE_MEAN=[0.5,0.5,0.5] \\\nNORMALIZE_STD=[0.5,0.5,0.5] \\\neva predict_fit --config configs/vision/dino_vit/offline/&lt;task&gt;.yaml\n</code></pre> <p>* path to public checkpoint will be added when available.</p>"},{"location":"user-guide/getting-started/how_to_use/","title":"How to use eva","text":"<p>Before starting to use eva, it's important to get familiar with the different workflows, subcommands and configurations.</p>"},{"location":"user-guide/getting-started/how_to_use/#eva-subcommands","title":"eva subcommands","text":"<p>To run an evaluation, we call: <pre><code>eva &lt;subcommand&gt; --config &lt;path-to-config-file&gt;\n</code></pre></p> <p>The eva interface supports the subcommands: <code>predict</code>, <code>fit</code> and <code>predict_fit</code>.</p> <ul> <li><code>fit</code>: is used to train a decoder for a specific task and subsequently evaluate the performance. This can be done online or offline *</li> <li><code>predict</code>: is used to compute embeddings for input images with a provided FM-checkpoint. This is the first step of the offline workflow</li> <li><code>predict_fit</code>: runs <code>predict</code> and <code>fit</code> sequentially. Like the <code>fit</code>-online run, it runs a complete evaluation with images as input.</li> </ul>"},{"location":"user-guide/getting-started/how_to_use/#online-vs-offline-workflows","title":"* online vs. offline workflows","text":"<p>We distinguish between the online and offline workflow:</p> <ul> <li>online: This mode uses raw images as input and generates the embeddings using a frozen FM backbone on the fly to train a downstream head network.</li> <li>offline: In this mode, embeddings are pre-computed and stored locally in a first step, and loaded in a 2nd step from disk to train the downstream head network.</li> </ul> <p>The online workflow can be used to quickly run a complete evaluation without saving and tracking embeddings. The offline workflow runs faster (only one FM-backbone forward pass) and is ideal to experiment with different decoders on the same FM-backbone.</p>"},{"location":"user-guide/getting-started/how_to_use/#run-configurations","title":"Run configurations","text":""},{"location":"user-guide/getting-started/how_to_use/#config-files","title":"Config files","text":"<p>The setup for an eva run is provided in a <code>.yaml</code> config file which is defined with the <code>--config</code> flag.</p> <p>A config file specifies the setup for the trainer (including callback for the model backbone), the model (setup of the trainable decoder) and data module. </p> <p>To get a better understanding, inspect some of the provided config files (which you will download if you run the tutorials).</p>"},{"location":"user-guide/getting-started/how_to_use/#environment-variables","title":"Environment variables","text":"<p>To customize runs, without the need of creating custom config-files, you can overwrite the config-parameters listed below by setting them as environment variables.</p> Type Description <code>OUTPUT_ROOT</code> str The directory to store logging outputs and evaluation results <code>EMBEDDINGS_ROOT</code> str The directory to store the computed embeddings <code>CHECKPOINT_PATH</code> str Path to the FM-checkpoint to be evaluated <code>IN_FEATURES</code> int The input feature dimension (embedding) <code>NUM_CLASSES</code> int Number of classes for classification tasks <code>N_RUNS</code> int Number of <code>fit</code> runs to perform in a session, defaults to 5 <code>MAX_STEPS</code> int Maximum number of training steps (if early stopping is not triggered) <code>BATCH_SIZE</code> int Batch size for a training step <code>PREDICT_BATCH_SIZE</code> int Batch size for a predict step <code>LR_VALUE</code> float Learning rate for training the decoder <code>MONITOR_METRIC</code> str The metric to monitor for early stopping and final model checkpoint loading <code>MONITOR_METRIC_MODE</code> str \"min\" or \"max\", depending on the <code>MONITOR_METRIC</code> used <code>REPO_OR_DIR</code> str GitHub repo with format containing model implementation, e.g. \"facebookresearch/dino:main\" <code>DINO_BACKBONE</code> str Backbone model architecture if a facebookresearch/dino FM is evaluated <code>FORCE_RELOAD</code> bool Whether to force a fresh download of the github repo unconditionally <code>PRETRAINED</code> bool Whether to load FM-backbone weights from a pretrained model"},{"location":"user-guide/getting-started/installation/","title":"Installation","text":"<p>Note: this section applies in the current form only to Kaiko-internal user testing and will be revised for the public package when publishing eva</p> <ul> <li> <p>Create and activate a virtual environment with Python 3.10+</p> </li> <li> <p>Install eva and the eva-vision package with:</p> </li> </ul> <pre><code>pip install \"kaiko-eva[vision] @ git+https://github.com/kaiko-ai/eva.git\"\n</code></pre> <ul> <li>To be able to use the existing configs, download them into directory where you installed eva. You can get them from our blob storage with:</li> </ul> <pre><code>azcopy copy https://kaiko.blob.core.windows.net/long-term-experimental/eva/configs . --recursive=true\n</code></pre> <p>(Alternatively you can also download them from the eva GitHub repo)</p>"},{"location":"user-guide/getting-started/installation/#run-eva","title":"Run eva","text":"<p>Now you are all setup and you can start running eva with: <pre><code>eva &lt;subcommand&gt; --config &lt;path-to-config-file&gt;\n</code></pre> To learn how the subcommands and configs work, we recommend you familiarize yourself with How to use eva and then proceed to running eva with the Tutorials.</p>"},{"location":"user-guide/tutorials/evaluate_resnet/","title":"Train and evaluate a ResNet","text":"<p>If you read How to use eva and followed the Tutorials to this point, you might ask yourself why you would not always use the offline workflow to run a complete evaluation. An offline-run stores the computed embeddings and runs faster than the online-workflow which computes a backbone-forward pass in every epoch.</p> <p>One use case for the online-workflow is the evaluation of a supervised ML model that does not rely on an backbone/head architecture. To demonstrate this, let's train a ResNet 18 from Pytoch Image Models (timm).</p> <p>To do this we need to create a new config-file:</p> <ul> <li>Create a new folder: <code>configs/vision/resnet18</code></li> <li>Create a copy of <code>configs/vision/dino_vit/online/bach.yaml</code> and move it to the new folder.</li> </ul> <p>Now let's adapt the new <code>bach.yaml</code>-config to the new model:</p> <ul> <li>remove the <code>backbone</code>-key from the config. If no backbone is specified, the backbone will be skipped during inference.</li> <li>adapt the model-head configuration as follows:</li> </ul> <p><pre><code>     head:\n      class_path: eva.models.ModelFromFunction\n      init_args:\n        path: timm.create_model\n        arguments:\n          model_name: resnet18\n          num_classes: &amp;NUM_CLASSES 4\n          drop_rate: 0.0\n          pretrained: false\n</code></pre> To reduce training time, let's overwrite some of the default parameters. Run the training &amp; evaluation with: <pre><code>OUTPUT_ROOT=logs/resnet/bach \\\nMAX_STEPS=50 \\\nLR_VALUE=0.01 \\\neva fit --config configs/vision/resnet18/bach.yaml\n</code></pre> Once the run is complete, take a look at the results in <code>logs/resnet/bach/&lt;session-id&gt;/results.json</code> and check out the tensorboard with <code>tensorboard --logdir logs/resnet/bach</code>. How does the performance compare to the results observed in the previous tutorials?</p>"},{"location":"user-guide/tutorials/offline_vs_online/","title":"Offline vs. online evaluations","text":"<p>In this tutorial we run eva with the three subcommands <code>predict</code>, <code>fit</code> and <code>predict_fit</code>, and take a look at the difference between offline and online workflows.</p>"},{"location":"user-guide/tutorials/offline_vs_online/#before-you-start","title":"Before you start","text":"<p>For this tutorial we use the BACH classification task which is available on Zenodo and is distributed under Attribution-NonCommercial-ShareAlike 4.0 International license.</p> <p>If you have not yet downloaded the BACH data to your machine, open <code>configs/vision/dino_vit/offline/bach.yaml</code> and enable automatic download by setting: <code>download: true</code>.</p>"},{"location":"user-guide/tutorials/offline_vs_online/#offline-evaluations","title":"Offline evaluations","text":""},{"location":"user-guide/tutorials/offline_vs_online/#1-compute-the-embeddings","title":"1. Compute the embeddings","text":"<p>First, let's use the <code>predict</code>-command to download the data and compute embeddings. In this example we use a randomly initialized <code>dino_vits16</code> as backbone.</p> <p>Open a terminal in the folder where you installed eva and run:</p> <pre><code>PRETRAINED=false \\\nEMBEDDINGS_ROOT=./data/embeddings/dino_vits16_random \\\neva predict --config configs/vision/dino_vit/offline/bach.yaml\n</code></pre> <p>Executing this command will:</p> <ul> <li>Download and extract the BACH dataset to <code>./data/bach</code> (if it has not already been downloaded to this location). This will take a few minutes.</li> <li>Compute the embeddings for all input images with the specified FM-backbone and store them in the <code>EMBEDDINGS_ROOT</code> along with a <code>manifest.csv</code> file.</li> </ul> <p>Once the session is complete, verify that:</p> <ul> <li>The raw images have been downloaded to <code>./data/bach/ICIAR2018_BACH_Challenge</code></li> <li>The embeddings have been computed and are stored in <code>./data/embeddings/dino_vits16_random/bach</code></li> <li>The <code>manifest.csv</code> file that maps the filename to the embedding, target and split has been created in <code>./data/embeddings/dino_vits16/bach</code>.</li> </ul>"},{"location":"user-guide/tutorials/offline_vs_online/#2-evaluate-the-fm","title":"2. Evaluate the FM","text":"<p>Now we can use the <code>fit</code>-command to evaluate the FM on the precomputed embeddings.</p> <p>To ensure a quick run for the purpose of this exercise, we overwrite some of the default parameters. Run eva to fit the decoder classifier with:</p> <pre><code>N_RUNS=2 \\\nMAX_STEPS=20 \\\nLR_VALUE=0.1 \\\neva fit --config configs/vision/dino_vit/offline/bach.yaml\n</code></pre> <p>Executing this command will:</p> <ul> <li>Fit a downstream head (single layer MLP) on the BACH-train split, using the computed embeddings and provided labels as input.</li> <li>Evaluate the trained model on the validation split and store the results.</li> </ul> <p>Once the session is complete:</p> <ul> <li>Check the evaluation results in <code>logs/dino_vits16/offline/bach/&lt;session-id&gt;/results.json</code>. (The <code>&lt;session-id&gt;</code> consists of a timestamp and a hash that is based on the run configuration.)</li> <li>Take a look at the training curves with the Tensorboard. Open a new terminal, activate the environment and run: <pre><code>tensorboard --logdir logs/dino_vits16/offline/bach\n</code></pre></li> </ul>"},{"location":"user-guide/tutorials/offline_vs_online/#3-run-a-complete-offline-workflow","title":"3. Run a complete offline-workflow","text":"<p>With the <code>predict_fit</code>-command, the two steps above can be executed with one command. Let's do this, but this time let's use an FM pretrained from ImageNet.</p> <p>Go back to the terminal and execute: <pre><code>N_RUNS=1 \\\nMAX_STEPS=20 \\\nLR_VALUE=0.1 \\\nPRETRAINED=true \\\nEMBEDDINGS_ROOT=./data/embeddings/dino_vits16_pretrained \\\neva predict_fit --config configs/vision/dino_vit/offline/bach.yaml\n</code></pre></p> <p>Once the session is complete, inspect the evaluation results as you did in Step 2. Compare the performance metrics and training curves. Can you observe better performance with the ImageNet pretrained encoder?</p>"},{"location":"user-guide/tutorials/offline_vs_online/#online-evaluations","title":"Online evaluations","text":"<p>Alternatively to the offline workflow from Step 3, a complete evaluation can also be computed online. In this case we don't save and track embeddings and instead fit the ML model (encoder with frozen layers + trainable decoder) directly on the given task.</p> <p>As in Step 3 above, we again use a <code>dino_vits16</code> pretrained from ImageNet. </p> <p>Run a complete online workflow with the following command: <pre><code>N_RUNS=1 \\\nMAX_STEPS=20 \\\nLR_VALUE=0.1 \\\nPRETRAINED=true \\\neva fit --config configs/vision/dino_vit/online/bach.yaml\n</code></pre></p> <p>Executing this command will:</p> <ul> <li>Fit a complete model - the frozen FM-backbone and downstream head - on the BACH-train split. (The download step will be skipped if you executed Step 1 or 3 before.)</li> <li>Evaluate the trained model on the val split and report the results</li> </ul> <p>Once the run is complete:</p> <ul> <li>Check the evaluation results in <code>logs/dino_vits16/offline/bach/&lt;session-id&gt;/results.json</code> and compare them to the results of Step 3. Do they match?</li> <li>You might have noticed that the online run took considerably longer than the offline run. Do you understand why that is?</li> </ul>"}]}