trainer:
  class_path: eva.Trainer
  init_args:
    n_runs: &N_RUNS ${oc.env:N_RUNS, 1}
    default_root_dir: &OUTPUT_ROOT ${oc.env:OUTPUT_ROOT, logs/total_segmentator_2d/${oc.env:TIMM_MODEL_NAME, vit_small_patch16_224}}
    max_steps: &MAX_STEPS ${oc.env:MAX_STEPS, 12500}
    callbacks:
      # - class_path: eva.vision.callbacks.SemanticSegmentationLogger
      #   init_args:
      #     log_every_n_epochs: 1
      - class_path: eva.callbacks.SegmentationEmbeddingsWriter
        init_args:
          output_dir: &DATASET_EMBEDDINGS_ROOT ${oc.env:EMBEDDINGS_ROOT, ./data/embeddings}/${oc.env:DINO_BACKBONE, dino_vits16}/total_segmentator_2d
          dataloader_idx_map:
            0: train
            1: val
          backbone:
            class_path: eva.vision.models.networks.encoders.TimmEncoder
            init_args:
              model_name: ${oc.env:TIMM_MODEL_NAME, vit_small_patch16_224}
              pretrained: true
              out_indices: 1
              model_arguments:
                dynamic_img_size: true
    logger:
      - class_path: lightning.pytorch.loggers.TensorBoardLogger
        init_args:
          save_dir: *OUTPUT_ROOT
          name: ""
model:
  class_path: eva.vision.models.modules.SemanticSegmentationModule
  init_args:
    decoder:
      class_path: eva.vision.models.networks.decoders.segmentation.ConvDecoder
      init_args:
        layers:
          class_path: torch.nn.Conv2d
          init_args:
            in_channels: ${oc.env:IN_FEATURES, 384}
            out_channels:  &NUM_CLASSES 118
            kernel_size: [1, 1]
    criterion: torch.nn.CrossEntropyLoss
    lr_multiplier_encoder: 0.0
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 0.0001
        weight_decay: 0.05
    lr_scheduler:
      class_path: torch.optim.lr_scheduler.PolynomialLR
      init_args:
        total_iters: *MAX_STEPS
        power: 0.9
    postprocess:
      targets_transforms:
        - class_path: torchvision.transforms.v2.ToDtype
          init_args:
            dtype: torch.int64
    metrics:
      common:
        - class_path: eva.metrics.AverageLoss
        - class_path: torchmetrics.Dice
          init_args:
            num_classes: *NUM_CLASSES
data:
  class_path: eva.DataModule
  init_args:
    datasets:
      train:
        class_path: eva.vision.datasets.EmbeddingsSegmentationDataset
        init_args: &DATASET_ARGS
          root: *DATASET_EMBEDDINGS_ROOT
          manifest_file: manifest.csv
          split: train
      val:
        class_path: eva.vision.datasets.EmbeddingsSegmentationDataset
        init_args:
          <<: *DATASET_ARGS
          split: val
      predict:
        - class_path: eva.vision.datasets.TotalSegmentator2D
          init_args: &PREDICT_DATASET_ARGS
            root: ${oc.env:DATA_ROOT, ./data}/total_segmentator
            split: train
            download: false
            # Set `download: true` to download the dataset from https://zenodo.org/records/10047292
            # The TotalSegmentator dataset is distributed under the following license: 
            # "Creative Commons Attribution 4.0 International"
            # (see: https://creativecommons.org/licenses/by/4.0/deed.en)
            transforms:
              class_path: eva.vision.data.transforms.common.ResizeAndCrop
        - class_path: eva.vision.datasets.TotalSegmentator2D
          init_args:
            <<: *PREDICT_DATASET_ARGS
            split: val
    dataloaders:
      train:
        batch_size: &BATCH_SIZE ${oc.env:BATCH_SIZE, 256}
        shuffle: true
      val:
        batch_size: *BATCH_SIZE
      predict:
        batch_size: &PREDICT_BATCH_SIZE ${oc.env:PREDICT_BATCH_SIZE, 64}
