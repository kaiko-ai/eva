{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#_1","title":"Introduction","text":"<p>Oncology FM Evaluation Framework by kaiko.ai</p> <p>eva currently supports performance evaluation for vision Foundation Models (\"FMs\") and supervised machine learning models on WSI (patch- and slide-level) as well as radiology image classification tasks.</p> <p>With eva we provide the open-source community with an easy-to-use framework that follows industry best practices to deliver a robust, reproducible and fair evaluation benchmark across FMs of different sizes and architectures.</p> <p>Support for additional modalities and tasks will be added soon.</p>"},{"location":"#use-cases","title":"Use cases","text":""},{"location":"#1-evaluate-your-own-fms-on-public-benchmark-datasets","title":"1. Evaluate your own FMs on public benchmark datasets","text":"<p>With a specified FM as input, you can run eva on several publicly available datasets &amp; tasks. One evaluation run will download (if supported) and preprocess the relevant data, compute embeddings, fit and evaluate a downstream head and report the mean and standard deviation of the relevant performance metrics.</p> <p>Supported datasets &amp; tasks include:</p> <p>WSI patch-level pathology datasets</p> <ul> <li>Patch Camelyon: binary breast cancer classification</li> <li>BACH: multiclass breast cancer classification</li> <li>CRC: multiclass colorectal cancer classification</li> <li>MHIST: binary colorectal polyp cancer classification</li> <li>MoNuSAC: multi-organ nuclei segmentation</li> <li>CoNSeP: segmentation colorectal nuclei and phenotypes</li> </ul> <p>WSI slide-level pathology datasets</p> <ul> <li>Camelyon16: binary breast cancer classification</li> <li>PANDA: multiclass prostate cancer classification</li> </ul> <p>Radiology datasets</p> <ul> <li>TotalSegmentator: radiology/CT-scan for segmentation of anatomical structures</li> <li>LiTS: radiology/CT-scan for segmentation of liver and tumor</li> </ul> <p>To evaluate FMs, eva provides support for different model-formats, including models trained with PyTorch, models available on HuggingFace and ONNX-models. For other formats custom wrappers can be implemented.</p>"},{"location":"#2-evaluate-ml-models-on-your-own-dataset-task","title":"2. Evaluate ML models on your own dataset &amp; task","text":"<p>If you have your own labeled dataset, all that is needed is to implement a dataset class tailored to your source data. Start from one of our out-of-the box provided dataset classes, adapt it to your data and run eva to see how different FMs perform on your task.</p>"},{"location":"#evaluation-results","title":"Evaluation results","text":"<p>Check out our Leaderboards to inspect evaluation results of publicly available FMs.</p>"},{"location":"#license","title":"License","text":"<p>eva is distributed under the terms of the Apache-2.0 license.</p>"},{"location":"#next-steps","title":"Next steps","text":"<p>Check out the User Guide to get started with eva</p> <p></p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or  advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic  address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a  professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at eva@kaiko.ai. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"},{"location":"CONTRIBUTING/","title":"Contributing to eva","text":"<p>eva is open source and community contributions are welcome!</p>"},{"location":"CONTRIBUTING/#contribution-process","title":"Contribution Process","text":""},{"location":"CONTRIBUTING/#github-issues","title":"GitHub Issues","text":"<p>The eva contribution process generally starts with filing a GitHub issue.</p> <p>eva defines four categories of issues: feature requests, bug reports, documentation fixes, and installation issues. In general, we recommend waiting for feedback from a eva maintainer or community member before proceeding to implement a feature or patch.</p>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>After you have agreed upon an implementation strategy for your feature or patch with an eva maintainer, the next step is to introduce your changes as a pull request against the eva repository.</p> <p>Steps to make a pull request:</p> <ul> <li>Fork https://github.com/kaiko-ai/eva</li> <li>Implement your feature as a branch off of the <code>main</code> branch</li> <li>Create a pull request into the <code>main</code> branch of https://github.com/kaiko-ai/eva</li> </ul> <p>Once your pull request has been merged, your changes will be automatically included in the next eva release!</p>"},{"location":"DEVELOPER_GUIDE/","title":"Developer Guide","text":""},{"location":"DEVELOPER_GUIDE/#setting-up-a-dev-environment","title":"Setting up a DEV environment","text":"<p>We use PDM as a package and dependency manager. You can set up a local Python environment for development as follows:  1. Install package and dependency manager PDM following the instructions here. 2. Install system dependencies     - For MacOS: <code>brew install Cmake</code>     - For Linux (Debian): <code>sudo apt-get install build-essential cmake</code> 3. Run <code>PDM_PYTHON=$(pyenv which python) &amp;&amp; pdm install -G all -G dev</code> to install the Python dependencies. This will create a virtual environment in <code>eva/.venv</code>. If you don't use <code>pyenv</code> to manage your python installations, you can replace <code>$(pyenv which python)</code> with the path to your python executable. Note that the python version used should match <code>PYTHON_VERSIONS</code> in <code>noxfile.py</code>, as this is the version is used for running the unit tests.</p>"},{"location":"DEVELOPER_GUIDE/#adding-new-dependencies","title":"Adding new dependencies","text":"<p>Add a new dependency to the <code>core</code> submodule: <code>pdm add &lt;package_name&gt;</code></p> <p>Add a new dependency to the <code>vision</code> submodule: <code>pdm add -G vision -G all &lt;package_name&gt;</code></p> <p>For more information about managing dependencies please look here.</p>"},{"location":"DEVELOPER_GUIDE/#update-dependencies","title":"Update dependencies","text":"<p>To update all dependencies in the lock file: <code>pdm update</code></p> <p>To update the dependencies in a specific group <code>pdm update -G &lt;group_name&gt;</code></p> <p>To update a specific dependency in a specified group <code>pdm update -G &lt;group_name&gt; &lt;package_name&gt;</code></p>"},{"location":"DEVELOPER_GUIDE/#regenerate-the-lock-file","title":"Regenerate the lock file","text":"<p>If you want to regenerate the lock file from scratch: <code>pdm lock -G all -G vision -G lint -G typecheck -G test -G dev -G docs</code></p>"},{"location":"DEVELOPER_GUIDE/#continuous-integration-ci","title":"Continuous Integration (CI)","text":"<p>For testing automation, we use <code>nox</code>.</p> <p>Installation: - with brew: <code>brew install nox</code> - with pip: <code>pip install --user --upgrade nox</code> (this way, you might need to run nox commands with <code>python -m nox</code> or specify an alias)</p> <p>Commands: - <code>nox</code> to run all the automation tests.  - <code>nox -s fmt</code> to run the code formatting tests. - <code>nox -s lint</code> to run the code lining tests. - <code>nox -s check</code> to run the type-annotation tests. - <code>nox -s test</code> to run the unit tests.   - <code>nox -s test -- tests/eva/metrics/test_average_loss.py</code> to run specific tests</p>"},{"location":"STYLE_GUIDE/","title":"eva Style Guide","text":"<p>This document contains our style guides used in <code>eva</code>.</p> <p>Our priority is consistency, so that developers can quickly ingest and understand the entire codebase without being distracted by style idiosyncrasies.</p>"},{"location":"STYLE_GUIDE/#general-coding-principles","title":"General coding principles","text":"<p>Q: How to keep code readable and maintainable? - Don't Repeat Yourself (DRY) - Use the lowest possible visibility for a variable or method (i.e. make private if possible) -- see Information Hiding / Encapsulation</p> <p>Q: How big should a function be? - Single Level of Abstraction Principle (SLAP) - High Cohesion and Low Coupling</p> <pre><code>TL;DR: functions should usually be quite small, and _do one thing_\n</code></pre>"},{"location":"STYLE_GUIDE/#python-style-guide","title":"Python Style Guide","text":"<p>In general we follow the following regulations: PEP8, the  Google Python Style Guide and we expect type hints/annotations.</p>"},{"location":"STYLE_GUIDE/#docstrings","title":"Docstrings","text":"<p>Our docstring style is derived from Google Python style.</p> <pre><code>def example_function(variable: int, optional: str | None = None) -&gt; str:\n    \"\"\"An example docstring that explains what this functions do.\n\n    Docs sections can be referenced via :ref:`custom text here &lt;anchor-link&gt;`.\n\n    Classes can be referenced via :class:`eva.data.datamodules.DataModule`.\n\n    Functions can be referenced via :func:`eva.data.datamodules.call.call_method_if_exists`.\n\n    Example:\n\n        &gt;&gt;&gt; from torch import nn\n        &gt;&gt;&gt; import eva\n        &gt;&gt;&gt; eva.models.modules.HeadModule(\n        &gt;&gt;&gt;     head=nn.Linear(10, 2),\n        &gt;&gt;&gt;     criterion=nn.CrossEntropyLoss(),\n        &gt;&gt;&gt; )\n\n    Args:\n        variable: A required argument.\n        optional: An optional argument.\n\n    Returns:\n        A description of the output string.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"STYLE_GUIDE/#module-docstrings","title":"Module docstrings","text":"<p>PEP-8 and PEP-257 indicate docstrings should have very specific syntax:</p> <pre><code>\"\"\"One line docstring that shouldn't wrap onto next line.\"\"\"\n</code></pre> <pre><code>\"\"\"First line of multiline docstring that shouldn't wrap.\n\nSubsequent line or paragraphs.\n\"\"\"\n</code></pre>"},{"location":"STYLE_GUIDE/#constants-docstrings","title":"Constants docstrings","text":"<p>Public constants should usually have docstrings. Optional on private constants. Docstrings on constants go underneath</p> <pre><code>SOME_CONSTANT = 3\n\"\"\"Either a single-line docstring or multiline as per above.\"\"\"\n</code></pre>"},{"location":"STYLE_GUIDE/#function-docstrings","title":"Function docstrings","text":"<p>All public functions should have docstrings following the pattern shown below.</p> <p>Each section can be omitted if there are no inputs, outputs, or no notable exceptions raised, respectively.</p> <pre><code>def fake_datamodule(\n    n_samples: int, random: bool = True\n) -&gt; eva.data.datamodules.DataModule:\n    \"\"\"Generates a fake DataModule.\n\n    It builds a :class:`eva.data.datamodules.DataModule` by generating\n    a fake dataset with generated data while fixing the seed. It can\n    be useful for debugging purposes.\n\n    Args:\n        n_samples: The number of samples of the generated datasets.\n        random: Whether to generated randomly.\n\n    Returns:\n        A :class:`eva.data.datamodules.DataModule` with generated random data.\n\n    Raises:\n        ValueError: If `n_samples` is `0`.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"STYLE_GUIDE/#class-docstrings","title":"Class docstrings","text":"<p>All public classes should have class docstrings following the pattern shown below.</p> <pre><code>class DataModule(pl.LightningDataModule):\n    \"\"\"DataModule encapsulates all the steps needed to process data.\n\n    It will initialize and create the mapping between dataloaders and\n    datasets. During the `prepare_data`, `setup` and `teardown`, the\n    datamodule will call the respectively methods from all the datasets,\n    given that they are defined.\n    \"\"\"\n\n    def __init__(\n        self,\n        datasets: schemas.DatasetsSchema | None = None,\n        dataloaders: schemas.DataloadersSchema | None = None,\n    ) -&gt; None:\n        \"\"\"Initializes the datamodule.\n\n        Args:\n            datasets: The desired datasets. Defaults to `None`.\n            dataloaders: The desired dataloaders. Defaults to `None`.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"leaderboards/","title":"Leaderboards","text":"<p>We evaluated the following FMs on the 6 supported WSI-classification tasks. We report Balanced Accuracy for binary &amp; multiclass tasks and generalized Dice score (no background) for segmentation tasks. The score shows the average performance over 5 runs. Note the leaderboard orders from best to worst according to the average performance across all tasks, excluding BACH (not comparable due to much larger patch size).</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>The runs use the default setup described in the section below.</p> <p>eva trains the decoder on the \"train\" split and uses the \"validation\" split for monitoring, early stopping and checkpoint selection. Evaluation results are reported on the \"test\" split if available and otherwise on the \"validation\" split.</p> <p>For details on the FM-backbones and instructions to replicate the results, check out Replicate evaluations. For information on the tasks, check out Datasets. For Camelyon16 runtime optimization we use only 1000 foreground patches per slide which impacts the performance on this benchmark accross all models. </p>"},{"location":"leaderboards/#evaluation-protocol","title":"Evaluation protocol","text":"<p>eva uses a fixed protocol customized to each category of tasks. The setup has proven to be performant and robust independent of task and model size &amp; architecture and generally prioritizes fairness and comparability over state-of-the-art performance.</p> <p>We selected this approach to prioritize reliable, robust and fair FM-evaluation while being in line with common literature.</p> WSI patch-level classification tasks WSI slide-level classification tasks WSI patch-level segmentation tasks Backbone frozen frozen frozen Head single layer MLP ABMIL Mult-stage convolutional Dropout 0.0 0.0 0.0 Hidden activation function n/a ReLU n/a Output activation function none none none Number of steps 12,500 12,500 (1) 2,000 Base batch size 256 32 64 Base learning rate 0.0003 0.001 0.002 Early stopping 5% * [Max epochs] 10% * [Max epochs] (2) 10% * [Max epochs] (2) Optimizer AdamW AdamW AdamW Momentum 0.9 n/a n/a Weight Decay 0.0 n/a n/a betas n/a [0.9, 0.999] [0.9, 0.999] LR Schedule Cosine without warmup Cosine without warmup PolynomialLR Loss Cross entropy Cross entropy Dice number of patches per slide 1 dataset specific (3) dataset specific (3) <p>(1) Upper cap at a maximum of 100 epochs.</p> <p>(2) Lower cap at a minimum of 8 epochs.</p> <p>(3) Number of patches per slide depends on task and slide size. E.g. for <code>PANDASmall</code> and <code>Camelyon16Small</code> we use a max of 200 and 1000 random patches per slide respectively.</p>"},{"location":"datasets/","title":"Datasets","text":"<p>eva provides native support for several public datasets. When possible, the corresponding dataset classes facilitate automatic download to disk, if not possible, this documentation provides download instructions.</p>"},{"location":"datasets/#vision-datasets-overview","title":"Vision Datasets Overview","text":""},{"location":"datasets/#whole-slide-wsi-and-microscopy-image-datasets","title":"Whole Slide (WSI) and microscopy image datasets","text":""},{"location":"datasets/#patch-level","title":"Patch-level","text":"Dataset #Patches Patch Size Magnification (\u03bcm/px) Task Tissue Type BACH 400 2048x1536 20x (0.5) Classification (4 classes) Breast BRACS 4539 variable 40x (0.25) Classification (7 classes) Breast BreakHis 1471 700x460 40x (0.25) Classification (4 classes) Breast CRC 107,180 224x224 20x (0.5) Classification (9 classes) Colorectal GleasonArvaniti 22,752 750x750 40x (0.23) Classification (4 classes) Prostate PatchCamelyon 327,680 96x96 10x (1.0) * Classification (2 classes) Breast MHIST 3,152 224x224 5x (2.0) * Classification (2 classes) Colorectal Polyp UniToPatho 8669 1812 x 1812 20x (0.4415) Classification (6 classes) Colorectal Polyp MoNuSAC 294 113x81 - 1398x1956 40x (0.25) Segmentation (4 classes) Multi-Organ Cell Type (Breast, Kidney, Lung and Prostate) CoNSeP 41 1000x1000 40x (0.25) * Segmentation (8 classes) Colorectal Nuclear <p>* Downsampled from 40x (0.25 \u03bcm/px) to increase the field of view.</p>"},{"location":"datasets/#slide-level","title":"Slide-level","text":"Dataset #Slides Slide Size Magnification (\u03bcm/px) Task Cancer Type Camelyon16 400 ~100-250k x ~100-250k x 3 40x (0.25) Classification (2 classes) Breast PANDA 9,555 ~20k x 20k x 3 20x (0.5) Classification (6 classes) Prostate PANDASmall 1,909 ~20k x 20k x 3 20x (0.5) Classification (6 classes) Prostate"},{"location":"datasets/#radiology-datasets","title":"Radiology datasets","text":"Dataset #Images Image Size Task Download provided TotalSegmentator 1228 ~300 x ~300 x ~350 * Semantic Segmentation (117 classes) Yes LiTS 131 (58638) ~300 x ~300 x ~350 * Semantic Segmentation (2 classes) No <p>* 3D images of varying sizes</p>"},{"location":"datasets/bach/","title":"BACH","text":"<p>The BACH dataset consists of microscopy and WSI images, of which we use only the microscopy images. These are 408 labeled images from 4 classes (\"Normal\", \"Benign\", \"Invasive\", \"InSitu\"). This dataset was used for the \"BACH Grand Challenge on Breast Cancer Histology images\".</p>"},{"location":"datasets/bach/#raw-data","title":"Raw data","text":""},{"location":"datasets/bach/#key-stats","title":"Key stats","text":"Modality Vision (microscopy images) Task Multiclass classification (4 classes) Cancer type Breast Data size total: 10.4GB / data in use: 7.37 GB (18.9 MB per image) Image dimension 1536 x 2048 x 3 Magnification (\u03bcm/px) 20x (0.42) Files format <code>.tif</code> images Number of images 408 (102 from each class) Splits in use one labeled split"},{"location":"datasets/bach/#organization","title":"Organization","text":"<p>The data <code>ICIAR2018_BACH_Challenge.zip</code> from zenodo is organized as follows:</p> <pre><code>ICAR2018_BACH_Challenge\n\u251c\u2500\u2500 Photos                    # All labeled patches used by eva\n\u2502   \u251c\u2500\u2500 Normal\n\u2502   \u2502   \u251c\u2500\u2500 n032.tif\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 Benign\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 Invasive\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 InSitu\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 WSI                       # WSIs, not in use\n\u2502   \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/bach/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The <code>BACH</code> dataset class supports downloading the data during runtime by setting the init argument <code>download=True</code>.</p> <p>[!NOTE] In the provided <code>BACH</code>-config files the download argument is set to <code>false</code>. To enable automatic download you will need to open the config and set <code>download: true</code>.</p> <p>The splits are created from the indices specified in the BACH dataset class. These indices were picked to prevent data  leakage due to images belonging to the same patient. Because the small dataset in combination with the patient ID constraint  does not allow to split the data three-ways with sufficient amount of data in each split, we only create a train and val  split and leave it to the user to submit predictions on the official test split to the BACH Challenge Leaderboard.</p> Splits Train Validation #Samples 268 (67%) 132 (33%)"},{"location":"datasets/bach/#relevant-links","title":"Relevant links","text":"<ul> <li>BACH dataset on zenodo</li> <li>BACH Challenge website</li> <li>BACH Challenge Leaderboard</li> <li>Patient ID information (Link provided on BACH challenge website)</li> <li>Reference API Vision dataset classes</li> </ul>"},{"location":"datasets/bach/#license","title":"License","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International</p>"},{"location":"datasets/bcss/","title":"BCSS","text":"<p>The BCSS (Breast Cancer Semantic Segmentation) consists of extracts from 151 WSI images from TCGA, containing over 20,000 segmentation annotations covering 21 different tissue types.</p>"},{"location":"datasets/bcss/#raw-data","title":"Raw data","text":""},{"location":"datasets/bcss/#key-stats","title":"Key stats","text":"Modality Vision (WSI extracts) Task Segmentation - 22 classes (tissue types) Data size total: ~5GB Image dimension ~1000-3000 x ~1000-3000 x 3 Magnification (\u03bcm/px) 40x (0.25) Files format <code>.png</code> images / <code>.mat</code> segmentation masks Number of images 151 Splits in use Train, Val and Test"},{"location":"datasets/bcss/#organization","title":"Organization","text":"<p>The data is organized as follows:</p> <pre><code>bcss\n\u251c\u2500\u2500 rgbs_colorNormalized       # wsi images\n\u2502   \u251c\u2500\u2500 TCGA-*.png\n\u251c\u2500\u2500 masks                      # segmentation masks\n\u2502   \u251c\u2500\u2500 TCGA-*.png             # same filenames as images \n</code></pre>"},{"location":"datasets/bcss/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The <code>BCSS</code> dataset class doesn't download the data during runtime and must be downloaded manually from links provided here.</p> <p>Although the original images have a resolution of 0.25 microns per pixel (mpp), we extract patches at 0.5 mpp for evaluation. This is because using the original resolution with common foundation model patch sizes (e.g. 224x224 pixels) would result in regions that are too small, leading to less expressive segmentation masks and unnecessarily complicating the task.</p>"},{"location":"datasets/bcss/#splits","title":"Splits","text":"<p>As a test set, we use the images from the medical institues OL, LL, E2, EW, GM, and S3, as proposed by the authors. For the validation split, we use images from the institutes BH, C8, A8, A1 and E9, which results in the following dataset sizes:</p> Splits Train Validation Test #Samples 76 (50.3%) 30 (19.9%) 45 (29.8%)"},{"location":"datasets/bcss/#relevant-links","title":"Relevant links","text":"<ul> <li>Dataset Repo</li> <li>Breast Cancer Segmentation Grand Challenge</li> <li>Google Drive Download Link for 0.25 mpp version</li> </ul>"},{"location":"datasets/bcss/#license","title":"License","text":"<p>The BCSS dataset is held under the CC0 1.0 UNIVERSAL license.</p>"},{"location":"datasets/bracs/","title":"BRACS","text":"<p>The BReAst Carcinoma Subtyping (BRACS) is a new dataset of hematoxylin and eosin (H&amp;E) histopathological images of breast carcinoma.</p> <p>Images (WSI) of hematoxylin and eosin (H&amp;E) stained breast tissues were generated by using an Aperio AT2 scanner at 0.25 \u00b5m/pixel for 40\u00d7 resolution. Some Regions of Interest (RoIs) are associated with a subset of WSIs. See example figure below. Both WSIs and RoIs were annotated according to the seven classes mentioned above (N, PB, UDH, FEA, ADH, DCIS, IC), by three expert pathologists of the Complex Structure Pathological Anatomy and Cytopathology of National Cancer Institute \u2013 IRCCS Fondazione Pascale, Naples, Italy.</p> <p>While the BRACS contains 547 WSIs collected by 189 patients, the <code>BRACS_ROI</code> subset which we use in this benchmarks contains 4539 extracted ROIs / patches.</p>"},{"location":"datasets/bracs/#raw-data","title":"Raw data","text":""},{"location":"datasets/bracs/#key-stats","title":"Key stats","text":"Modality Vision (WSI patches) Task Multiclass classification (7 classes) Cancer type Breast Data size 52 GB Image dimension variable Magnification (\u03bcm/px) 40x (0.25) Files format <code>png</code> Number of images 4539"},{"location":"datasets/bracs/#splits","title":"Splits","text":"<p>The data source provides train/validation/test splits</p> Splits Train Validation Test #Samples 3657 (80.57%) 312 (6.87%) 570 (12.56%)"},{"location":"datasets/bracs/#organization","title":"Organization","text":"<p>The BRACS data is organized as follows:</p> <pre><code>BRACS_RoI\n\u251c\u2500\u2500 train\n\u2502   \u251c\u2500\u2500 0_N                    # 1 folder per class \n\u2502   \u251c\u2500\u2500 1_PB\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 val\n\u2502   \u251c\u2500\u2500 0_N\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 0_N\n\u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/bracs/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The <code>BRACS</code> dataset class doesn't download the data during runtime and must be downloaded manually from the official source.</p>"},{"location":"datasets/bracs/#relevant-links","title":"Relevant links","text":"<ul> <li>Official Website</li> <li>Paper</li> </ul>"},{"location":"datasets/bracs/#license","title":"License","text":"<p>CC BY-NC 4.0</p>"},{"location":"datasets/breakhis/","title":"BreakHis","text":"<p>The Breast Cancer Histopathological Image Classification (BreakHis) is  composed of 9,109 microscopic images of breast tumor tissue collected from 82 patients using different magnifying factors (40X, 100X, 200X, and 400X). For this benchmark we only use the 40X samples which results in a subset of 1,995 images. This database has been built in collaboration with the P&amp;D Laboratory, Pathological Anatomy and Cytopathology, Parana, Brazil.</p> <p>The dataset is divided into two main groups: benign tumors and malignant tumors. The original dataset contains four histological distinct types of benign breast tumors: adenosis (A), fibroadenoma (F), phyllodes tumor (PT), and tubular adenona (TA); and four malignant tumors (breast cancer): carcinoma (DC), lobular carcinoma (LC), mucinous carcinoma (MC) and papillary carcinoma (PC).</p> <p>Given that patient counts for some classes are very low (e.g. 3 for PT), we only use classes with at least 7 patients for this benchmark: TA, MC, F &amp; DC.</p>"},{"location":"datasets/breakhis/#raw-data","title":"Raw data","text":""},{"location":"datasets/breakhis/#key-stats","title":"Key stats","text":"Modality Vision (WSI patches) Task Multiclass classification (4 classes) Cancer type Breast Data size 4 GB Image dimension 700 x 460 Magnification (\u03bcm/px) 40x (0.25) Files format <code>png</code> Number of images 1471"},{"location":"datasets/breakhis/#splits","title":"Splits","text":"<p>The data source provides train/validation splits. There is no overlap of patients between the splits, and a stratified distribution of the classes is approximated (extact stratification is not possible due to the patient separation constraint).</p> Splits Train Validation #Samples 1132 (76.95%) 339 (23.04%)"},{"location":"datasets/breakhis/#organization","title":"Organization","text":"<p>The BreakHis data is organized as follows:</p> <pre><code>BreaKHis_v1\n\u251c\u2500\u2500 histology_slides\n\u2502   \u251c\u2500\u2500 breast\n|   \u2502   \u251c\u2500\u2500 benign\n|   \u2502   |   \u251c\u2500\u2500 SOB\n|   \u2502   |   |   \u251c\u2500\u2500 adenosis\n|   \u2502   |   |   \u251c\u2500\u2500 fibroadenoma\n|   \u2502   |   |   \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/breakhis/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The <code>BreakHis</code> dataset class supports downloading the data during runtime through setting the environment variable <code>DOWNLOAD_DATA=true</code>.</p>"},{"location":"datasets/breakhis/#relevant-links","title":"Relevant links","text":"<ul> <li>Official Source</li> </ul>"},{"location":"datasets/breakhis/#license","title":"License","text":"<p>CC BY 4.0</p>"},{"location":"datasets/camelyon16/","title":"Camelyon16","text":"<p>The Camelyon16 dataset consists of 400 WSIs of lymph nodes for breast cancer metastasis classification. The dataset is a combination of two independent datasets, collected from two separate medical centers in the Netherlands (Radboud University Medical Center and University Medical Center Utrecht). The dataset contains the slides from which PatchCamelyon-patches were extracted.</p> <p>The dataset is divided in a train set (270 slides) and test set (130 slides), both containing images from both centers. Note that one test set slide was a duplicate has been removed (see here).</p> <p>The task was part of Grand Challenge in 2016 and has later been replaced by Camelyon17.</p> <p>Source: https://camelyon16.grand-challenge.org</p>"},{"location":"datasets/camelyon16/#raw-data","title":"Raw data","text":""},{"location":"datasets/camelyon16/#key-stats","title":"Key stats","text":"Modality Vision (WSI) Task Binary classification Cancer type Breast Data size ~700 GB Image dimension ~100-250k x ~100-250k x 3 Magnification (\u03bcm/px) 40x (0.25) - Level 0 Files format <code>.tif</code> Number of images 399 (270 train, 129 test)"},{"location":"datasets/camelyon16/#organization","title":"Organization","text":"<p>The data <code>CAMELYON16</code> (download links here) is organized as follows:</p> <pre><code>CAMELYON16\n\u251c\u2500\u2500 training\n\u2502   \u251c\u2500\u2500 normal\n|   \u2502   \u251c\u2500\u2500 normal_001.tif\n|   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 tumor\n|   \u2502   \u251c\u2500\u2500 tumor_001.tif\n|   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 lesion_annotations.zip\n\u251c\u2500\u2500 testing\n\u2502   \u251c\u2500\u2500 images\n|   \u2502   \u251c\u2500\u2500 test_001.tif\n|   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 evaluation     # masks not in use\n\u2502   \u251c\u2500\u2500 reference.csv  # targets\n\u2502   \u2514\u2500\u2500 lesion_annotations.zip\n</code></pre>"},{"location":"datasets/camelyon16/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The <code>Camelyon16</code> dataset class doesn't download the data during runtime and must be downloaded manually from links provided here.</p> <p>The dataset is split into train / test. Additionally, we split the train set into train/val using the same splits as PatchCamelyon (see metadata CSV files on Zenodo).</p> Splits Train Validation Test #Samples 216 (54.1%) 54 (13.5%) 129 (32.3%)"},{"location":"datasets/camelyon16/#relevant-links","title":"Relevant links","text":"<ul> <li>Grand Challenge dataset description</li> <li>Download links</li> <li>GitHub with dataset description by DIDSR</li> </ul>"},{"location":"datasets/camelyon16/#references","title":"References","text":"<p>1 : A General-Purpose Self-Supervised Model for Computational Pathology</p>"},{"location":"datasets/consep/","title":"CoNSeP","text":"<p>CoNSep (Colorectal Nuclear Segmentation and Phenotypes) consists of 41 1000x1000 tiles extracted from 16 WSIs of unique patients. Labels are segmentation masks which indicate if a pixel belongs to one of 7 categories of cell nuclei. In total 24,319 unique nuclei are present.</p>"},{"location":"datasets/consep/#raw-data","title":"Raw data","text":""},{"location":"datasets/consep/#key-stats","title":"Key stats","text":"Modality Vision (WSI patches) Task Segmentation - 4 classes * Data size total: ~800MB Image dimension 1000 x 1000 x 3 Magnification (\u03bcm/px) 40x (0.25) Files format <code>.png</code> images / <code>.mat</code> segmentation masks Number of images 41 Splits in use Train and Test <p>* The original dataset has 7 classes:</p> <ol> <li>Miscellaneous</li> <li>Inflammatory</li> <li>Normal epithelium</li> <li>Malignant/dysplastic epithelium</li> <li>Fibroblast</li> <li>Muscle</li> <li>Endothelial</li> </ol> <p>As in the original paper [1], we combine classes 3 (normal epithelial) &amp; 4 (malignant/dysplastic epithelial) into the epithelial class and 5 (fibroblast), 6 (muscle) &amp; 7 (endothelial) into the spindle-shaped class.</p>"},{"location":"datasets/consep/#organization","title":"Organization","text":"<p>The data is organized as follows:</p> <pre><code>consep\n\u251c\u2500\u2500 Train\n\u2502   \u251c\u2500\u2500 Images                 # raw training input images\n\u2502   \u2502   \u251c\u2500\u2500 train_1.png\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 Labels                 # train segmentation labels        \n\u2502   \u2502   \u251c\u2500\u2500 train_1.mat\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 Overlay                # train images with bounding boxes, not in use\n\u251c\u2500\u2500 Test\n\u2502   \u251c\u2500\u2500 Images                 # raw test input images\n\u2502   \u2502   \u251c\u2500\u2500 test_1.png\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 Labels                 # test segmentation labels        \n\u2502   \u2502   \u251c\u2500\u2500 test_1.mat\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 Overlay                # test images with bounding boxes, not in use\n\u2514\u2500\u2500 README.txt                 # data description\n</code></pre>"},{"location":"datasets/consep/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>Note that the CoNSeP dataset is currently not available for download. As soon as it becomes availble we will add support &amp; instructions (monitor this issue for updates)</p>"},{"location":"datasets/consep/#splits","title":"Splits","text":"<p>We work with the splits provided by the data source. Since no \"validation\" split is provided, we use the \"test\" split as validation split.</p> Splits Train Validation #Samples 27 (66%) 14 (34%)"},{"location":"datasets/consep/#relevant-links","title":"Relevant links","text":"<ul> <li>CoNSeP Dataset description</li> <li>Data download (currently not available)</li> <li>GitHub issue for data availability</li> </ul>"},{"location":"datasets/consep/#license","title":"License","text":"<p>The CoNSeP dataset are held under the Apache 2.0 License</p>"},{"location":"datasets/consep/#references","title":"References","text":"<p>[1] : HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images</p>"},{"location":"datasets/crc/","title":"CRC","text":"<p>The CRC-HE dataset consists of labeled patches (9 classes) from colorectal cancer (CRC) and normal tissue. We use the <code>NCT-CRC-HE-100K</code> dataset for training and validation and the <code>CRC-VAL-HE-7K for testing</code>. </p> <p>The <code>NCT-CRC-HE-100K-NONORM</code> consists of 100,000 images without applied color normalization. The <code>CRC-VAL-HE-7K</code> consists of 7,180 image patches from 50 patients without overlap with <code>NCT-CRC-HE-100K-NONORM</code>.</p> <p>The tissue classes are: Adipose (ADI), background (BACK), debris (DEB), lymphocytes (LYM), mucus (MUC), smooth muscle (MUS), normal colon mucosa (NORM), cancer-associated stroma (STR) and colorectal adenocarcinoma epithelium (TUM)</p>"},{"location":"datasets/crc/#raw-data","title":"Raw data","text":""},{"location":"datasets/crc/#key-stats","title":"Key stats","text":"Modality Vision (WSI patches) Task Multiclass classification (9 classes) Cancer type Colorectal Data size total: 11.7GB (train), 800MB (val) Image dimension 224 x 224 x 3 Magnification (\u03bcm/px) 20x (0.5) Files format <code>.tif</code> images Number of images 107,180 (100k train, 7.2k val) Splits in use NCT-CRC-HE-100K (train), CRC-VAL-HE-7K (val)"},{"location":"datasets/crc/#splits","title":"Splits","text":"<p>We use the splits according to the data sources:</p> <ul> <li>Train split: <code>NCT-CRC-HE-100K</code></li> <li>Validation split: <code>CRC-VAL-HE-7K</code></li> </ul> Splits Train Validation #Samples 100,000 (93.3%) 7,180 (6.7%) <p>A test split is not provided. Because the patient information for the training data is not available, dividing the  training data in a train/val split (and using the given val split as test split) is not possible without risking data leakage. eva therefore reports evaluation results for CRC HE on the validation split.</p>"},{"location":"datasets/crc/#organization","title":"Organization","text":"<p>The data <code>NCT-CRC-HE-100K.zip</code>, <code>NCT-CRC-HE-100K-NONORM.zip</code> and <code>CRC-VAL-HE-7K.zip</code> from zenodo are organized as follows:</p> <pre><code>NCT-CRC-HE-100K                # All images used for training\n\u251c\u2500\u2500 ADI                        # All labeled patches belonging to the 1st class\n\u2502   \u251c\u2500\u2500 ADI-AAAFLCLY.tif\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 BACK                       # All labeled patches belonging to the 2nd class\n\u2502   \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n\nNCT-CRC-HE-100K-NONORM         # All images used for training\n\u251c\u2500\u2500 ADI                        # All labeled patches belonging to the 1st class\n\u2502   \u251c\u2500\u2500 ADI-AAAFLCLY.tif\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 BACK                       # All labeled patches belonging to the 2nd class\n\u2502   \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n\nCRC-VAL-HE-7K                  # All images used for validation\n\u251c\u2500\u2500 ...                        # identical structure as for NCT-CRC-HE-100K-NONORM\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/crc/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The <code>CRC</code> dataset class supports downloading the data during runtime by setting the init argument <code>download=True</code>.</p> <p>[!NOTE] In the provided <code>CRC</code>-config files the download argument is set to <code>false</code>. To enable automatic download you will need to open the config and set <code>download: true</code>.</p>"},{"location":"datasets/crc/#relevant-links","title":"Relevant links","text":"<ul> <li>CRC datasets on zenodo</li> <li>Reference API Vision dataset classes</li> </ul>"},{"location":"datasets/crc/#license","title":"License","text":"<p>CC BY 4.0 LEGAL CODE</p>"},{"location":"datasets/gleason_arvaniti/","title":"Gleason (Arvaniti)","text":"<p>Benchmark dataset for automated Gleason grading of prostate cancer tissue microarrays via deep learning as proposed by Arvaniti et al..</p> <p>Images are classified as benign, Gleason pattern 3, 4 or 5. The dataset contains annotations on a discovery / train cohort of 641 patients and an independent test cohort of 245 patients annotated by two pathologists. For the test cohort, we only use the labels from pathologist Nr. 1 for this benchmark</p>"},{"location":"datasets/gleason_arvaniti/#raw-data","title":"Raw data","text":""},{"location":"datasets/gleason_arvaniti/#key-stats","title":"Key stats","text":"Modality Vision (WSI patches) Task Multiclass classification (4 classes) Cancer type Prostate Data size 4 GB Image dimension 750 x 750 Magnification (\u03bcm/px) 40x (0.23) Files format <code>jpg</code> Number of images 22,752"},{"location":"datasets/gleason_arvaniti/#splits","title":"Splits","text":"<p>The following splits are proposed in the paper:</p> Splits Train Validation Test #Samples 15,303 (67.26%) 2,482 (10.91%) 4,967 (21.83%) <p>Note that the authors chose TMA 76 as validation cohort because it contains the most balanced distribution of Gleason scores. We couldn't achieve stable results when evaluating on the test set, so we only use the train and validation sets for this benchmark.</p>"},{"location":"datasets/gleason_arvaniti/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The <code>GleasonArvaniti</code> dataset class doesn't download the data during runtime and must be downloaded and preprocessed manually:</p> <ol> <li>Download dataset archives from the official source</li> <li>Unpack all .tar.gz archives into the same folder</li> <li>Adjust the folder structure and then run the <code>create_patches.py</code> from https://github.com/eiriniar/gleason_CNN/tree/master</li> </ol> <p>This should result in the folloing folder structure:</p> <pre><code>arvaniti_gleason_patches\n\u251c\u2500\u2500 test_patches_750\n\u2502   \u251c\u2500\u2500 patho_1\n\u2502   \u2502   \u251c\u2500\u2500 ZT80_38_A_1_1\n    \u2502   \u2502   \u251c\u2500\u2500 ZT76_39_A_1_1_patch_12_class_0.jpg\n    \u2502   \u2502   \u251c\u2500\u2500 ZT76_39_A_1_1_patch_23_class_0.jpg\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500 ZT80_38_A_1_2\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 patho_2  # we don't use this\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 train_validation_patches_750\n\u2502   \u251c\u2500\u2500 ZT76_39_A_1_1\n\u2502   \u2502   \u251c\u2500\u2500 ZT76_39_A_1_1_patch_12_class_0.jpg\n\u2502   \u2502   \u251c\u2500\u2500 ZT76_39_A_1_1_patch_23_class_0.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 ZT76_39_A_1_2\n\u2502   \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/gleason_arvaniti/#relevant-links","title":"Relevant links","text":"<ul> <li>Paper</li> <li>GitHub</li> <li>Dataset</li> </ul>"},{"location":"datasets/gleason_arvaniti/#license","title":"License","text":"<p>CC0 1.0 Universal</p>"},{"location":"datasets/lits/","title":"LiTS17 (Liver Tumor Segmentation Challenge 2017)","text":"<p>LiTS17 is a liver tumor segmentation benchmark. The data and segmentations are provided by various clinical sites around the world. The training data set contains 130 CT scans and the test data set 70 CT scans.</p> <p>The segmentation classes are: Background, Liver and Tumor.</p>"},{"location":"datasets/lits/#raw-data","title":"Raw data","text":""},{"location":"datasets/lits/#key-stats","title":"Key stats","text":"Modality Vision (radiology, CT scans) Task Segmentation (3 classes) Data size train: 15GB (53.66 GB uncompressed) Image dimension ~300 x ~300 x ~350 (number of slices) x 1 (grey scale) * Files format <code>.nii</code> (\"NIFTI\") images Number of scans 131 (58638 slices) Splits in use train (70%) / val (15%) / test (15%)"},{"location":"datasets/lits/#splits","title":"Splits","text":"<p>We use the following random split:</p> Splits Train Validation Test #Scans; Slices 91; 38686 (77%) 19; 11192 (11.5%) 21; 8760 (11.5%)"},{"location":"datasets/lits/#organization","title":"Organization","text":"<p>The training data are organized as follows:</p> <pre><code>Training Batch 1               # Train images part 1\n\u251c\u2500\u2500 segmentation-0.nii         # Semantic labels for volume 0\n\u251c\u2500\u2500 segmentation-1.nii         # Semantic labels for volume 1\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 volume-0.nii               # CT-Scan 0\n\u251c\u2500\u2500 volume-1.nii               # CT-Scan 1\n\u2514\u2500\u2500 ...\n\nTraining Batch 2               # Train images part 2\n\u251c\u2500\u2500 segmentation-28.nii        # Semantic labels for volume 28\n\u251c\u2500\u2500 segmentation-29.nii        # Semantic labels for volume 29\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 volume-28.nii              # CT-Scan 28\n\u251c\u2500\u2500 volume-29.nii              # CT-Scan 29\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/lits/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The <code>LiTS</code> dataset can be downloaded from the official LiTS competition page. The training split comes into two <code>.zip</code> files, namely <code>Training_Batch1.zip</code> and <code>Training_Batch2.zip</code>, which should be extracted and merged.</p>"},{"location":"datasets/lits/#relevant-links","title":"Relevant links","text":"<ul> <li>LiTS - Liver Tumor Segmentation Challenge</li> <li>Whitepaper</li> </ul>"},{"location":"datasets/lits/#license","title":"License","text":"<p>CC BY-NC-ND 4.0</p>"},{"location":"datasets/mhist/","title":"MHIST","text":"<p>MHIST is a binary classification task which comprises of 3,152 hematoxylin and eosin (H&amp;E)-stained Formalin Fixed Paraffin-Embedded (FFPE) fixed-size images (224 by 224 pixels) of colorectal polyps from the Department of Pathology and Laboratory Medicine at Dartmouth-Hitchcock Medical Center (DHMC).</p> <p>The tissue classes are: Hyperplastic Polyp (HP), Sessile Serrated Adenoma (SSA). This classification task focuses on the clinically-important binary distinction between HPs and SSAs, a challenging problem with considerable inter-pathologist variability. HPs are typically benign, while sessile serrated adenomas are precancerous lesions that can turn into cancer if left untreated and require sooner follow-up examinations. Histologically, HPs have a superficial serrated architecture and elongated crypts, whereas SSAs are characterized by broad-based crypts, often with complex structure and heavy serration.</p>"},{"location":"datasets/mhist/#raw-data","title":"Raw data","text":""},{"location":"datasets/mhist/#key-stats","title":"Key stats","text":"Modality Vision (WSI patches) Task Binary classification (2 classes) Cancer type Colorectal Polyp Data size 354 MB Image dimension 224 x 224 x 3 Magnification (\u03bcm/px) 5x (2.0) * Files format <code>.png</code> images Number of images 3,152 (2,175 train, 977 test) Splits in use annotations.csv (train / test) <p>* Downsampled from 40x to increase the field of view.</p>"},{"location":"datasets/mhist/#organization","title":"Organization","text":"<p>The contents from <code>images.zip</code> and the file <code>annotations.csv</code> from bmirds are organized as follows:</p> <pre><code>mhist                           # Root folder\n\u251c\u2500\u2500 images                      # All the dataset images\n\u2502   \u251c\u2500\u2500 MHIST_aaa.png\n\u2502   \u251c\u2500\u2500 MHIST_aab.png\n\u2502   \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 annotations.csv             # The dataset annotations file\n</code></pre>"},{"location":"datasets/mhist/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>To download the dataset, please visit the access portal on BMIRDS and follow the instructions. You will then receive an email with all the relative links that you can use to download the data (<code>images.zip</code>, <code>annotations.csv</code>, <code>Dataset Research Use Agreement.pdf</code> and <code>MD5SUMs.txt</code>). </p> <p>Please create a root folder, e.g. <code>mhist</code>, and download all the files there, which unzipping the contents of <code>images.zip</code> to a directory named <code>images</code> inside your root folder (i.e. <code>mhist/images</code>). Afterwards, you can (optionally) delete the <code>images.zip</code> file.</p>"},{"location":"datasets/mhist/#splits","title":"Splits","text":"<p>We work with the splits provided by the data source. Since no \"validation\" split is provided, we use the \"test\" split as validation split.</p> <ul> <li>Train split: <code>annotations.csv</code> :: \"Partition\" == \"train\"</li> <li>Validation split: <code>annotations.csv</code> :: \"Partition\" == \"test\"</li> </ul> Splits Train Validation #Samples 2,175 (69%) 977 (31%)"},{"location":"datasets/mhist/#relevant-links","title":"Relevant links","text":"<ul> <li>Accessing MHIST Dataset (BMIRDS)</li> <li>Paper: A Petri Dish for Histopathology Image Analysis</li> </ul>"},{"location":"datasets/monusac/","title":"MoNuSAC","text":"<p>MoNuSAC (Multi-Organ Nuclei Segmentation And Classification Challenge) consists of H&amp;E stained tissue images of four organs with annotations of multiple cell-types including epithelial cells, lymphocytes, macrophages, and neutrophils with over 46,000 nuclei from 37 hospitals and 71 patients.</p>"},{"location":"datasets/monusac/#raw-data","title":"Raw data","text":""},{"location":"datasets/monusac/#key-stats","title":"Key stats","text":"Modality Vision (WSI patches) Task Segmentation - 5 classes * Data size total: ~600MB Image dimension 113x81 - 1398x1956 Magnification (\u03bcm/px) 40x (0.25) Files format <code>.svs</code> or <code>.tif</code> images / <code>.xml</code> segmentation masks Number of images 294 Splits in use Train and Test <p>* The fith class is \"ambiguous\" and doesn't contain a specific cell type.</p>"},{"location":"datasets/monusac/#organization","title":"Organization","text":"<p>The data is organized as follows:</p> <pre><code>monusac\n\u251c\u2500\u2500 MoNuSAC_images_and_annotations\n\u2502   \u251c\u2500\u2500 TCGA-5P-A9K0-01Z-00-DX1             # patient id\n\u2502   \u2502   \u251c\u2500\u2500 TCGA-5P-A9K0-01Z-00-DX1_1.svs   # tissue image\n\u2502   \u2502   \u251c\u2500\u2500 TCGA-5P-A9K0-01Z-00-DX1_1.tif   # tissue image\n\u2502   \u2502   \u251c\u2500\u2500 TCGA-5P-A9K0-01Z-00-DX1_1.xml   # annotations\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 MoNuSAC Testing Data and Annotations\n\u2502   \u251c\u2500\u2500 TCGA-5P-A9K0-01Z-00-DX1             # patient id\n\u2502   \u2502   \u251c\u2500\u2500 TCGA-5P-A9K0-01Z-00-DX1_1.svs   # tissue image\n\u2502   \u2502   \u251c\u2500\u2500 TCGA-5P-A9K0-01Z-00-DX1_1.tif   # tissue image\n\u2502   \u2502   \u251c\u2500\u2500 TCGA-5P-A9K0-01Z-00-DX1_1.xml   # annotations\n\u2502   \u2502   \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/monusac/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The dataset class <code>MoNuSAC</code> supports downloading the data during runtime by setting the init argument <code>download=True</code>.</p> <p>[!NOTE] In the provided <code>MoNuSAC</code>-config files the download argument is set to <code>false</code>. To enable automatic download you will need to open the config and set <code>download: true</code>.</p>"},{"location":"datasets/monusac/#splits","title":"Splits","text":"<p>We work with the splits provided by the data source. Since no \"validation\" split is provided, we use the \"test\" split as validation split.</p> Splits Train Validation #Samples 209 (71%) 85 (29%)"},{"location":"datasets/monusac/#relevant-links","title":"Relevant links","text":"<ul> <li>MoNuSAC Dataset</li> </ul>"},{"location":"datasets/monusac/#license","title":"License","text":"<p>The challenge data is released under the creative commons license (CC BY-NC-SA 4.0).</p>"},{"location":"datasets/panda/","title":"PANDA (Prostate cANcer graDe Assessment)","text":"<p>The PANDA datasets consists of 10,616 whole-slide images of digitized H&amp;E-stained prostate tissue biopsies originating from two medical centers. After the biopsy, the slides were classified into Gleason patterns (3, 4 or 5) based on the architectural growth patterns of the tumor, which are then converted into an ISUP grade on a 0-5 scale.</p> <p>The Gleason grading system is the most important prognostic marker for prostate cancer and the ISUP grade has a crucial role when deciding how a patient should be treated. However, the system suffers from significant inter-observer variability between pathologists, leading to imperfect and noisy labels.</p> <p>Source: https://www.kaggle.com/competitions/prostate-cancer-grade-assessment</p>"},{"location":"datasets/panda/#raw-data","title":"Raw data","text":""},{"location":"datasets/panda/#key-stats","title":"Key stats","text":"Modality Vision (WSI) Task Multiclass classification (6 classes) Cancer type Prostate Data size 347 GB Image dimension ~20k x 20k x 3 Magnification (\u03bcm/px) 20x (0.5) - Level 0 Files format <code>.tiff</code> Number of images 10,616 (9,555 after removing noisy labels)"},{"location":"datasets/panda/#organization","title":"Organization","text":"<p>The data <code>prostate-cancer-grade-assessment.zip</code> from kaggle is organized as follows:</p> <pre><code>prostate-cancer-grade-assessment\n\u251c\u2500\u2500 train_images\n\u2502   \u251c\u2500\u2500 0005f7aaab2800f6170c399693a96917.tiff\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 train_label_masks (not used in eva)\n\u2502   \u251c\u2500\u2500 0005f7aaab2800f6170c399693a96917_mask.tiff\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 train.csv (contains Gleason &amp; ISUP labels)\n\u251c\u2500\u2500 test.csv\n\u251c\u2500\u2500 sample_submission.csv\n</code></pre>"},{"location":"datasets/panda/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The <code>PANDA</code> dataset class doesn't download the data during runtime and must be downloaded manually from kaggle.</p> <p>As done in other studies<sup>1</sup> we exclude ~10% of the samples with noisy labels according to kaggle's 6th place solution resulting in a total dataset size of 9555 WSIs.</p> <p>We then generate random stratified train / validation and test splits using a 0.7 / 0.15 / 0.15 ratio:</p> Splits Train Validation Test #Samples 6686 (70%) 1430 (15%) 1439 (15%)"},{"location":"datasets/panda/#relevant-links","title":"Relevant links","text":"<ul> <li>Kaggle Challenge</li> <li>Noisy Labels</li> </ul>"},{"location":"datasets/panda/#license","title":"License","text":"<p>CC BY-SA-NC 4.0</p>"},{"location":"datasets/panda/#references","title":"References","text":"<p>1 : A General-Purpose Self-Supervised Model for Computational Pathology</p>"},{"location":"datasets/panda_small/","title":"PANDASmall","text":"<p>A small version of the PANDA dataset for quicker benchmarking. </p> <p>We generate stratified splits, using only 20% of the original dataset:</p> Splits Train Validation Test #Samples 955 (10%) 477 (5%) 477 (5%) <p>See PANDA for the description of the the full dataset.</p>"},{"location":"datasets/patch_camelyon/","title":"PatchCamelyon","text":"<p>The PatchCamelyon benchmark is an image classification dataset with 327,680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue.</p>"},{"location":"datasets/patch_camelyon/#raw-data","title":"Raw data","text":""},{"location":"datasets/patch_camelyon/#key-stats","title":"Key stats","text":"Modality Vision (WSI patches) Task Binary classification Cancer type Breast Data size 8 GB Image dimension 96 x 96 x 3 Magnification (\u03bcm/px) 10x (1.0) * Files format <code>h5</code> Number of images 327,680 (50% of each class) <p>* The slides were acquired and digitized at 2 different medical centers using a 40x objective but under-sampled to 10x to increase the field of view.</p>"},{"location":"datasets/patch_camelyon/#splits","title":"Splits","text":"<p>The data source provides train/validation/test splits</p> Splits Train Validation Test #Samples 262,144 (80%) 32,768 (10%) 32,768 (10%)"},{"location":"datasets/patch_camelyon/#organization","title":"Organization","text":"<p>The PatchCamelyon data from zenodo is organized as follows:</p> <pre><code>\u251c\u2500\u2500 camelyonpatch_level_2_split_train_x.h5.gz               # train images\n\u251c\u2500\u2500 camelyonpatch_level_2_split_train_y.h5.gz               # train labels\n\u251c\u2500\u2500 camelyonpatch_level_2_split_valid_x.h5.gz               # val images\n\u251c\u2500\u2500 camelyonpatch_level_2_split_valid_y.h5.gz               # val labels\n\u251c\u2500\u2500 camelyonpatch_level_2_split_test_x.h5.gz                # test images\n\u251c\u2500\u2500 camelyonpatch_level_2_split_test_y.h5.gz                # test labels\n</code></pre>"},{"location":"datasets/patch_camelyon/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The dataset class <code>PatchCamelyon</code> supports downloading the data during runtime by setting the init argument <code>download=True</code>.</p> <p>[!NOTE] In the provided <code>PatchCamelyon</code>-config files the download argument is set to <code>false</code>. To enable automatic download you will need to open the config and set <code>download: true</code>.</p> <p>Labels are provided by source files, splits are given by file names.</p>"},{"location":"datasets/patch_camelyon/#relevant-links","title":"Relevant links","text":"<ul> <li>PatchCamelyon dataset on zenodo</li> <li>GitHub repository</li> <li>Reference API Vision dataset classes</li> </ul>"},{"location":"datasets/patch_camelyon/#citation","title":"Citation","text":"<pre><code>@misc{b_s_veeling_j_linmans_j_winkens_t_cohen_2018_2546921,\n  author       = {B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling},\n  title        = {Rotation Equivariant CNNs for Digital Pathology},\n  month        = sep,\n  year         = 2018,\n  doi          = {10.1007/978-3-030-00934-2_24},\n  url          = {https://doi.org/10.1007/978-3-030-00934-2_24}\n}\n</code></pre>"},{"location":"datasets/patch_camelyon/#license","title":"License","text":"<p>Creative Commons Zero v1.0 Universal</p>"},{"location":"datasets/total_segmentator/","title":"TotalSegmentator","text":"<p>The TotalSegmentator dataset is a radiology image-segmentation dataset with 1228 3D images and corresponding masks with 117 different anatomical structures. It can be used for segmentation and multilabel classification tasks.</p>"},{"location":"datasets/total_segmentator/#raw-data","title":"Raw data","text":""},{"location":"datasets/total_segmentator/#key-stats","title":"Key stats","text":"Modality Vision (radiology, CT scans) Task Segmentation / multilabel classification (117 classes) Data size total: 23.6GB Image dimension ~300 x ~300 x ~350 (number of slices) x 1 (grey scale) * Files format <code>.nii</code> (\"NIFTI\") images Number of images 1228 Splits in use one labeled split <p>/* image resolution and number of slices per image vary</p>"},{"location":"datasets/total_segmentator/#organization","title":"Organization","text":"<p>The data <code>Totalsegmentator_dataset_v201.zip</code> from zenodo is organized as follows:</p> <pre><code>Totalsegmentator_dataset_v201\n\u251c\u2500\u2500 s0011                               # one image\n\u2502   \u251c\u2500\u2500 ct.nii.gz                       # CT scan\n\u2502   \u251c\u2500\u2500 segmentations                   # directory with segmentation masks\n\u2502   \u2502   \u251c\u2500\u2500 adrenal_gland_left.nii.gz   # segmentation mask 1st anatomical structure\n\u2502   \u2502   \u251c\u2500\u2500 adrenal_gland_right.nii.gz  # segmentation mask 2nd anatomical structure\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/total_segmentator/#download-and-preprocessing","title":"Download and preprocessing","text":"<ul> <li>The dataset class <code>TotalSegmentator</code> supports download the data on runtime with the initialized argument <code>download: bool = True</code>. </li> <li>For the multilabel classification task, every mask with at least one positive pixel it gets the label \"1\", all others get the label \"0\".</li> <li>For the multilabel classification task, the <code>TotalSegmentator</code> class creates a manifest file with one row/slice and the columns: <code>path</code>, <code>slice</code>, <code>split</code> and additional 117 columns for each class.</li> <li>The 3D images are treated as 2D. Every 25th slice is sampled and treated as individual image</li> <li>The splits with the following sizes are created after ordering images by filename:</li> </ul> Splits Train Validation Test #Samples 737 (60%) 246 (20%) 245 (20%)"},{"location":"datasets/total_segmentator/#relevant-links","title":"Relevant links","text":"<ul> <li>TotalSegmentator dataset on zenodo</li> <li>TotalSegmentator small subset (102 images) on zenodo</li> <li>Reference API TotalSegmentator dataset class</li> </ul>"},{"location":"datasets/total_segmentator/#license","title":"License","text":"<p>Creative Commons Attribution 4.0 International</p>"},{"location":"datasets/unitopatho/","title":"UniToPatho","text":"<p>UniToPatho is an annotated dataset of 9536 hematoxylin and eosin stained patches extracted from 292 whole-slide images, meant for training deep neural networks for colorectal polyps classification and adenomas grading. The slides are acquired through a Hamamatsu Nanozoomer S210 scanner at 20x magnification (0.4415 \u03bcm/px). Each slide belongs to a different patient and is annotated by expert pathologists, according to six classes as follows:</p> <ul> <li>NORM - Normal tissue;</li> <li>HP - Hyperplastic Polyp;</li> <li>TA.HG - Tubular Adenoma, High-Grade dysplasia;</li> <li>TA.LG - Tubular Adenoma, Low-Grade dysplasia;</li> <li>TVA.HG - Tubulo-Villous Adenoma, High-Grade dysplasia;</li> <li>TVA.LG - Tubulo-Villous Adenoma, Low-Grade dysplasia.</li> </ul> <p>For this benchmark we used only the <code>800</code> subset which contains 8669 images of resolution 1812x1812 (the <code>7000</code> subset contains much bigger images and would therefore be difficult to handle as patch classification task).</p>"},{"location":"datasets/unitopatho/#raw-data","title":"Raw data","text":""},{"location":"datasets/unitopatho/#key-stats","title":"Key stats","text":"Modality Vision (WSI patches) Task Multiclass classification (6 classes) Cancer type Colorectal Data size 48.37 GB Image dimension 1812 x 1812 Magnification (\u03bcm/px) 20x (0.4415) Magnification after resize (\u03bcm/px) 162x (3.57) Files format <code>png</code> Number of images 8669"},{"location":"datasets/unitopatho/#splits","title":"Splits","text":"<p>The data source provides train/validation splits</p> Splits Train Validation #Samples 6270 (72.33) 2399 (27.67%) <p>The dataset authors only provide two splits, which is why we don't report performance on a third test split.</p>"},{"location":"datasets/unitopatho/#organization","title":"Organization","text":"<p>The UniToPatho data is organized as follows (note that we are using only the <code>800</code> subset):</p> <pre><code>unitopatho\n\u251c\u2500\u2500 800\n    test.csv\n    train.csv\n\u2502   \u251c\u2500\u2500 HP                    # 1 folder per class \n\u2502   \u251c\u2500\u2500 NORM\n\u2502   \u251c\u2500\u2500 TA.HG\n\u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/unitopatho/#download-and-preprocessing","title":"Download and preprocessing","text":"<p>The <code>UniToPatho</code> dataset class doesn't download the data during runtime and must be downloaded manually from the official source.</p>"},{"location":"datasets/unitopatho/#relevant-links","title":"Relevant links","text":"<ul> <li>GitHub Repo</li> </ul>"},{"location":"datasets/unitopatho/#license","title":"License","text":"<p>CC BY 4.0</p>"},{"location":"reference/","title":"Reference API","text":"<p>Here is the Reference API, describing the classes, functions, parameters and attributes of the eva package.</p> <p>To learn how to use eva, however, its best to get started with the User Guide</p>"},{"location":"reference/core/callbacks/","title":"Callbacks","text":""},{"location":"reference/core/callbacks/#writers","title":"Writers","text":""},{"location":"reference/core/callbacks/#eva.core.callbacks.writers.ClassificationEmbeddingsWriter","title":"<code>eva.core.callbacks.writers.ClassificationEmbeddingsWriter</code>","text":"<p>               Bases: <code>EmbeddingsWriter</code></p> <p>Callback for writing generated embeddings to disk for classification tasks.</p> <p>This callback writes the embedding files in a separate process to avoid blocking the main process where the model forward pass is executed.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>The directory where the embeddings will be saved.</p> required <code>backbone</code> <code>Module | None</code> <p>A model to be used as feature extractor. If <code>None</code>, it will be expected that the input batch returns the features directly.</p> <code>None</code> <code>dataloader_idx_map</code> <code>Dict[int, str] | None</code> <p>A dictionary mapping dataloader indices to their respective names (e.g. train, val, test).</p> <code>None</code> <code>metadata_keys</code> <code>List[str] | None</code> <p>An optional list of keys to extract from the batch metadata and store as additional columns in the manifest file.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite if embeddings are already present in the specified output directory. If set to <code>False</code>, an error will be raised if embeddings are already present (recommended).</p> <code>False</code> <code>save_every_n</code> <code>int</code> <p>Interval for number of iterations to save the embeddings to disk. During this interval, the embeddings are accumulated in memory.</p> <code>100</code> Source code in <code>src/eva/core/callbacks/writers/embeddings/base.py</code> <pre><code>def __init__(\n    self,\n    output_dir: str,\n    backbone: nn.Module | None = None,\n    dataloader_idx_map: Dict[int, str] | None = None,\n    metadata_keys: List[str] | None = None,\n    overwrite: bool = False,\n    save_every_n: int = 100,\n) -&gt; None:\n    \"\"\"Initializes a new EmbeddingsWriter instance.\n\n    This callback writes the embedding files in a separate process to avoid blocking the\n    main process where the model forward pass is executed.\n\n    Args:\n        output_dir: The directory where the embeddings will be saved.\n        backbone: A model to be used as feature extractor. If `None`,\n            it will be expected that the input batch returns the features directly.\n        dataloader_idx_map: A dictionary mapping dataloader indices to their respective\n            names (e.g. train, val, test).\n        metadata_keys: An optional list of keys to extract from the batch metadata and store\n            as additional columns in the manifest file.\n        overwrite: Whether to overwrite if embeddings are already present in the specified\n            output directory. If set to `False`, an error will be raised if embeddings are\n            already present (recommended).\n        save_every_n: Interval for number of iterations to save the embeddings to disk.\n            During this interval, the embeddings are accumulated in memory.\n    \"\"\"\n    super().__init__(write_interval=\"batch\")\n\n    self._output_dir = output_dir\n    self._backbone = backbone\n    self._dataloader_idx_map = dataloader_idx_map or {}\n    self._overwrite = overwrite\n    self._save_every_n = save_every_n\n    self._metadata_keys = metadata_keys or []\n\n    self._write_queue: multiprocessing.Queue\n    self._write_process: eva_multiprocessing.Process\n</code></pre>"},{"location":"reference/core/interface/","title":"Interface API","text":"<p>Reference information for the <code>Interface</code> API.</p>"},{"location":"reference/core/interface/#eva.Interface","title":"<code>eva.Interface</code>","text":"<p>A high-level interface for training and validating a machine learning model.</p> <p>This class provides a convenient interface to connect a model, data, and trainer to train and validate a model.</p>"},{"location":"reference/core/interface/#eva.Interface.fit","title":"<code>fit</code>","text":"<p>Perform model training and evaluation out-of-place.</p> <p>This method uses the specified trainer to fit the model using the provided data.</p> <p>Example use cases:</p> <ul> <li>Using a model consisting of a frozen backbone and a head, the backbone will generate   the embeddings on the fly which are then used as input features to train the head on   the downstream task specified by the given dataset.</li> <li>Fitting only the head network using a dataset that loads pre-computed embeddings.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The base trainer to use but not modify.</p> required <code>model</code> <code>ModelModule</code> <p>The model module to use but not modify.</p> required <code>data</code> <code>DataModule</code> <p>The data module.</p> required Source code in <code>src/eva/core/interface/interface.py</code> <pre><code>def fit(\n    self,\n    trainer: eva_trainer.Trainer,\n    model: modules.ModelModule,\n    data: datamodules.DataModule,\n) -&gt; None:\n    \"\"\"Perform model training and evaluation out-of-place.\n\n    This method uses the specified trainer to fit the model using the provided data.\n\n    Example use cases:\n\n    - Using a model consisting of a frozen backbone and a head, the backbone will generate\n      the embeddings on the fly which are then used as input features to train the head on\n      the downstream task specified by the given dataset.\n    - Fitting only the head network using a dataset that loads pre-computed embeddings.\n\n    Args:\n        trainer: The base trainer to use but not modify.\n        model: The model module to use but not modify.\n        data: The data module.\n    \"\"\"\n    trainer.run_evaluation_session(model=model, datamodule=data)\n</code></pre>"},{"location":"reference/core/interface/#eva.Interface.predict","title":"<code>predict</code>","text":"<p>Perform model prediction out-of-place.</p> <p>This method performs inference with a pre-trained foundation model to compute embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The base trainer to use but not modify.</p> required <code>model</code> <code>ModelModule</code> <p>The model module to use but not modify.</p> required <code>data</code> <code>DataModule</code> <p>The data module.</p> required Source code in <code>src/eva/core/interface/interface.py</code> <pre><code>def predict(\n    self,\n    trainer: eva_trainer.Trainer,\n    model: modules.ModelModule,\n    data: datamodules.DataModule,\n) -&gt; None:\n    \"\"\"Perform model prediction out-of-place.\n\n    This method performs inference with a pre-trained foundation model to compute embeddings.\n\n    Args:\n        trainer: The base trainer to use but not modify.\n        model: The model module to use but not modify.\n        data: The data module.\n    \"\"\"\n    eva_trainer.infer_model(\n        base_trainer=trainer,\n        base_model=model,\n        datamodule=data,\n        return_predictions=False,\n    )\n</code></pre>"},{"location":"reference/core/interface/#eva.Interface.predict_fit","title":"<code>predict_fit</code>","text":"<p>Combines the predict and fit commands in one method.</p> <p>This method performs the following two steps: 1. predict: perform inference with a pre-trained foundation model to compute embeddings. 2. fit: training the head network using the embeddings generated in step 1.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The base trainer to use but not modify.</p> required <code>model</code> <code>ModelModule</code> <p>The model module to use but not modify.</p> required <code>data</code> <code>DataModule</code> <p>The data module.</p> required Source code in <code>src/eva/core/interface/interface.py</code> <pre><code>def predict_fit(\n    self,\n    trainer: eva_trainer.Trainer,\n    model: modules.ModelModule,\n    data: datamodules.DataModule,\n) -&gt; None:\n    \"\"\"Combines the predict and fit commands in one method.\n\n    This method performs the following two steps:\n    1. predict: perform inference with a pre-trained foundation model to compute embeddings.\n    2. fit: training the head network using the embeddings generated in step 1.\n\n    Args:\n        trainer: The base trainer to use but not modify.\n        model: The model module to use but not modify.\n        data: The data module.\n    \"\"\"\n    self.predict(trainer=trainer, model=model, data=data)\n    self.fit(trainer=trainer, model=model, data=data)\n</code></pre>"},{"location":"reference/core/data/dataloaders/","title":"Dataloaders","text":"<p>Reference information for the <code>Dataloader</code> classes.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader","title":"<code>eva.data.DataLoader</code>  <code>dataclass</code>","text":"<p>The <code>DataLoader</code> combines a dataset and a sampler.</p> <p>It provides an iterable over the given dataset.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.batch_size","title":"<code>batch_size: int | None = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How many samples per batch to load.</p> <p>Set to <code>None</code> for iterable dataset where dataset produces batches.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.shuffle","title":"<code>shuffle: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to shuffle the data at every epoch.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.sampler","title":"<code>sampler: samplers.Sampler | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Defines the strategy to draw samples from the dataset.</p> <p>Can be any Iterable with <code>__len__</code> implemented. If specified, shuffle must not be specified.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.batch_sampler","title":"<code>batch_sampler: samplers.Sampler | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Like <code>sampler</code>, but returns a batch of indices at a time.</p> <p>Mutually exclusive with <code>batch_size</code>, <code>shuffle</code>, <code>sampler</code> and <code>drop_last</code>.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.num_workers","title":"<code>num_workers: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How many workers to use for loading the data.</p> <p>By default, it will use the number of CPUs available.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.collate_fn","title":"<code>collate_fn: Callable | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The batching process.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.pin_memory","title":"<code>pin_memory: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Will copy Tensors into CUDA pinned memory before returning them.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.drop_last","title":"<code>drop_last: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Drops the last incomplete batch.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.persistent_workers","title":"<code>persistent_workers: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Will keep the worker processes after a dataset has been consumed once.</p>"},{"location":"reference/core/data/dataloaders/#eva.data.DataLoader.prefetch_factor","title":"<code>prefetch_factor: int | None = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches loaded in advance by each worker.</p>"},{"location":"reference/core/data/datamodules/","title":"Datamodules","text":"<p>Reference information for the <code>Datamodule</code> classes and functions.</p>"},{"location":"reference/core/data/datamodules/#eva.data.DataModule","title":"<code>eva.data.DataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>DataModule encapsulates all the steps needed to process data.</p> <p>It will initialize and create the mapping between dataloaders and datasets. During the <code>prepare_data</code>, <code>setup</code> and <code>teardown</code>, the datamodule will call the respective methods from all datasets, given that they are defined.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>DatasetsSchema | None</code> <p>The desired datasets.</p> <code>None</code> <code>dataloaders</code> <code>DataloadersSchema | None</code> <p>The desired dataloaders.</p> <code>None</code> <code>samplers</code> <code>SamplersSchema | None</code> <p>The desired samplers for the dataloaders.</p> <code>None</code> Source code in <code>src/eva/core/data/datamodules/datamodule.py</code> <pre><code>def __init__(\n    self,\n    datasets: schemas.DatasetsSchema | None = None,\n    dataloaders: schemas.DataloadersSchema | None = None,\n    samplers: schemas.SamplersSchema | None = None,\n) -&gt; None:\n    \"\"\"Initializes the datamodule.\n\n    Args:\n        datasets: The desired datasets.\n        dataloaders: The desired dataloaders.\n        samplers: The desired samplers for the dataloaders.\n    \"\"\"\n    super().__init__()\n\n    self.datasets = datasets or self.default_datasets\n    self.dataloaders = dataloaders or self.default_dataloaders\n    self.samplers = samplers or self.default_samplers\n</code></pre>"},{"location":"reference/core/data/datamodules/#eva.data.DataModule.default_datasets","title":"<code>default_datasets: schemas.DatasetsSchema</code>  <code>property</code>","text":"<p>Returns the default datasets.</p>"},{"location":"reference/core/data/datamodules/#eva.data.DataModule.default_dataloaders","title":"<code>default_dataloaders: schemas.DataloadersSchema</code>  <code>property</code>","text":"<p>Returns the default dataloader schema.</p>"},{"location":"reference/core/data/datamodules/#eva.data.DataModule.default_samplers","title":"<code>default_samplers: schemas.SamplersSchema</code>  <code>property</code>","text":"<p>Returns the default samplers schema.</p>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.call.call_method_if_exists","title":"<code>eva.data.datamodules.call.call_method_if_exists</code>","text":"<p>Calls a desired <code>method</code> from the datasets if exists.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>Iterable[Any]</code> <p>An iterable of objects.</p> required <code>method</code> <code>str</code> <p>The dataset method name to call if exists.</p> required Source code in <code>src/eva/core/data/datamodules/call.py</code> <pre><code>def call_method_if_exists(objects: Iterable[Any], /, method: str) -&gt; None:\n    \"\"\"Calls a desired `method` from the datasets if exists.\n\n    Args:\n        objects: An iterable of objects.\n        method: The dataset method name to call if exists.\n    \"\"\"\n    for _object in _recursive_iter(objects):\n        if hasattr(_object, method):\n            fn = getattr(_object, method)\n            fn()\n</code></pre>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.schemas.DatasetsSchema","title":"<code>eva.data.datamodules.schemas.DatasetsSchema</code>  <code>dataclass</code>","text":"<p>Datasets schema used in DataModule.</p>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.schemas.DatasetsSchema.train","title":"<code>train: TRAIN_DATASET = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Train dataset.</p>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.schemas.DatasetsSchema.val","title":"<code>val: EVAL_DATASET = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Validation dataset.</p>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.schemas.DatasetsSchema.test","title":"<code>test: EVAL_DATASET = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Test dataset.</p>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.schemas.DatasetsSchema.predict","title":"<code>predict: EVAL_DATASET = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Predict dataset.</p>"},{"location":"reference/core/data/datamodules/#eva.data.datamodules.schemas.DatasetsSchema.tolist","title":"<code>tolist</code>","text":"<p>Returns the dataclass as a list and optionally filters it given the stage.</p> Source code in <code>src/eva/core/data/datamodules/schemas.py</code> <pre><code>def tolist(self, stage: str | None = None) -&gt; List[EVAL_DATASET]:\n    \"\"\"Returns the dataclass as a list and optionally filters it given the stage.\"\"\"\n    match stage:\n        case \"fit\":\n            return [self.train, self.val]\n        case \"validate\":\n            return [self.val]\n        case \"test\":\n            return [self.test]\n        case \"predict\":\n            return [self.predict]\n        case None:\n            return [self.train, self.val, self.test, self.predict]\n        case _:\n            raise ValueError(f\"Invalid stage `{stage}`.\")\n</code></pre>"},{"location":"reference/core/data/datasets/","title":"Datasets","text":"<p>Reference information for the <code>Dataset</code> base class.</p>"},{"location":"reference/core/data/datasets/#eva.core.data.Dataset","title":"<code>eva.core.data.Dataset</code>","text":"<p>               Bases: <code>TorchDataset</code></p> <p>Base dataset class.</p>"},{"location":"reference/core/data/datasets/#eva.core.data.Dataset.prepare_data","title":"<code>prepare_data</code>","text":"<p>Encapsulates all disk related tasks.</p> <p>This method is preferred for downloading and preparing the data, for example generate manifest files. If implemented, it will be called via :class:<code>eva.core.data.datamodules.DataModule</code>, which ensures that is called only within a single process, making it multi-processes safe.</p> Source code in <code>src/eva/core/data/datasets/base.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Encapsulates all disk related tasks.\n\n    This method is preferred for downloading and preparing the data, for\n    example generate manifest files. If implemented, it will be called via\n    :class:`eva.core.data.datamodules.DataModule`, which ensures that is called\n    only within a single process, making it multi-processes safe.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/data/datasets/#eva.core.data.Dataset.setup","title":"<code>setup</code>","text":"<p>Setups the dataset.</p> <p>This method is preferred for creating datasets or performing train/val/test splits. If implemented, it will be called via :class:<code>eva.core.data.datamodules.DataModule</code> at the beginning of fit (train + validate), validate, test, or predict and it will be called from every process (i.e. GPU) across all the nodes in DDP.</p> Source code in <code>src/eva/core/data/datasets/base.py</code> <pre><code>def setup(self) -&gt; None:\n    \"\"\"Setups the dataset.\n\n    This method is preferred for creating datasets or performing\n    train/val/test splits. If implemented, it will be called via\n    :class:`eva.core.data.datamodules.DataModule` at the beginning of fit\n    (train + validate), validate, test, or predict and it will be called\n    from every process (i.e. GPU) across all the nodes in DDP.\n    \"\"\"\n    self.configure()\n    self.validate()\n</code></pre>"},{"location":"reference/core/data/datasets/#eva.core.data.Dataset.configure","title":"<code>configure</code>","text":"<p>Configures the dataset.</p> <p>This method is preferred to configure the dataset; assign values to attributes, perform splits etc. This would be called from the method ::method::<code>setup</code>, before calling the ::method::<code>validate</code>.</p> Source code in <code>src/eva/core/data/datasets/base.py</code> <pre><code>def configure(self):\n    \"\"\"Configures the dataset.\n\n    This method is preferred to configure the dataset; assign values\n    to attributes, perform splits etc. This would be called from the\n    method ::method::`setup`, before calling the ::method::`validate`.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/data/datasets/#eva.core.data.Dataset.validate","title":"<code>validate</code>","text":"<p>Validates the dataset.</p> <p>This method aims to check the integrity of the dataset and verify that is configured properly. This would be called from the method ::method::<code>setup</code>, after calling the ::method::<code>configure</code>.</p> Source code in <code>src/eva/core/data/datasets/base.py</code> <pre><code>def validate(self):\n    \"\"\"Validates the dataset.\n\n    This method aims to check the integrity of the dataset and verify\n    that is configured properly. This would be called from the method\n    ::method::`setup`, after calling the ::method::`configure`.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/data/datasets/#eva.core.data.Dataset.teardown","title":"<code>teardown</code>","text":"<p>Cleans up the data artifacts.</p> <p>Used to clean-up when the run is finished. If implemented, it will be called via :class:<code>eva.core.data.datamodules.DataModule</code> at the end of fit (train + validate), validate, test, or predict and it will be called from every process (i.e. GPU) across all the nodes in DDP.</p> Source code in <code>src/eva/core/data/datasets/base.py</code> <pre><code>def teardown(self) -&gt; None:\n    \"\"\"Cleans up the data artifacts.\n\n    Used to clean-up when the run is finished. If implemented, it will\n    be called via :class:`eva.core.data.datamodules.DataModule` at the end\n    of fit (train + validate), validate, test, or predict and it will be\n    called from every process (i.e. GPU) across all the nodes in DDP.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/data/datasets/#embeddings-datasets","title":"Embeddings datasets","text":""},{"location":"reference/core/data/datasets/#eva.core.data.datasets.EmbeddingsClassificationDataset","title":"<code>eva.core.data.datasets.EmbeddingsClassificationDataset</code>","text":"<p>               Bases: <code>EmbeddingsDataset[Tensor]</code></p> <p>Embeddings dataset class for classification tasks.</p> <p>Expects a manifest file listing the paths of .pt files that contain tensor embeddings of shape [embedding_dim] or [1, embedding_dim].</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>manifest_file</code> <code>str</code> <p>The path to the manifest file, which is relative to the <code>root</code> argument.</p> required <code>split</code> <code>Literal['train', 'val', 'test'] | None</code> <p>The dataset split to use. The <code>split</code> column of the manifest file will be splitted based on this value.</p> <code>None</code> <code>column_mapping</code> <code>Dict[str, str]</code> <p>Defines the map between the variables and the manifest columns. It will overwrite the <code>default_column_mapping</code> with the provided values, so that <code>column_mapping</code> can contain only the values which are altered or missing.</p> <code>default_column_mapping</code> <code>embeddings_transforms</code> <code>Callable | None</code> <p>A function/transform that transforms the embedding.</p> <code>None</code> <code>target_transforms</code> <code>Callable | None</code> <p>A function/transform that transforms the target.</p> <code>None</code> Source code in <code>src/eva/core/data/datasets/embeddings.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    manifest_file: str,\n    split: Literal[\"train\", \"val\", \"test\"] | None = None,\n    column_mapping: Dict[str, str] = default_column_mapping,\n    embeddings_transforms: Callable | None = None,\n    target_transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initialize dataset.\n\n    Expects a manifest file listing the paths of .pt files that contain\n    tensor embeddings of shape [embedding_dim] or [1, embedding_dim].\n\n    Args:\n        root: Root directory of the dataset.\n        manifest_file: The path to the manifest file, which is relative to\n            the `root` argument.\n        split: The dataset split to use. The `split` column of the manifest\n            file will be splitted based on this value.\n        column_mapping: Defines the map between the variables and the manifest\n            columns. It will overwrite the `default_column_mapping` with\n            the provided values, so that `column_mapping` can contain only the\n            values which are altered or missing.\n        embeddings_transforms: A function/transform that transforms the embedding.\n        target_transforms: A function/transform that transforms the target.\n    \"\"\"\n    super().__init__()\n\n    self._root = root\n    self._manifest_file = manifest_file\n    self._split = split\n    self._column_mapping = default_column_mapping | column_mapping\n    self._embeddings_transforms = embeddings_transforms\n    self._target_transforms = target_transforms\n\n    self._data: pd.DataFrame\n\n    self._set_multiprocessing_start_method()\n</code></pre>"},{"location":"reference/core/data/datasets/#eva.core.data.datasets.MultiEmbeddingsClassificationDataset","title":"<code>eva.core.data.datasets.MultiEmbeddingsClassificationDataset</code>","text":"<p>               Bases: <code>EmbeddingsDataset[Tensor]</code></p> <p>Dataset class for where a sample corresponds to multiple embeddings.</p> <p>Example use case: Slide level dataset where each slide has multiple patch embeddings.</p> <p>Expects a manifest file listing the paths of <code>.pt</code> files containing tensor embeddings.</p> <p>The manifest must have a <code>column_mapping[\"multi_id\"]</code> column that contains the unique identifier group of embeddings. For oncology datasets, this would be usually the slide id. Each row in the manifest file points to a .pt file that can contain one or multiple embeddings (either as a list or stacked tensors). There can also be multiple rows for the same <code>multi_id</code>, in which case the embeddings from the different .pt files corresponding to that same <code>multi_id</code> will be stacked along the first dimension.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>manifest_file</code> <code>str</code> <p>The path to the manifest file, which is relative to the <code>root</code> argument.</p> required <code>split</code> <code>Literal['train', 'val', 'test']</code> <p>The dataset split to use. The <code>split</code> column of the manifest file will be splitted based on this value.</p> required <code>column_mapping</code> <code>Dict[str, str]</code> <p>Defines the map between the variables and the manifest columns. It will overwrite the <code>default_column_mapping</code> with the provided values, so that <code>column_mapping</code> can contain only the values which are altered or missing.</p> <code>default_column_mapping</code> <code>embeddings_transforms</code> <code>Callable | None</code> <p>A function/transform that transforms the embedding.</p> <code>None</code> <code>target_transforms</code> <code>Callable | None</code> <p>A function/transform that transforms the target.</p> <code>None</code> Source code in <code>src/eva/core/data/datasets/classification/multi_embeddings.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    manifest_file: str,\n    split: Literal[\"train\", \"val\", \"test\"],\n    column_mapping: Dict[str, str] = embeddings_base.default_column_mapping,\n    embeddings_transforms: Callable | None = None,\n    target_transforms: Callable | None = None,\n):\n    \"\"\"Initialize dataset.\n\n    Expects a manifest file listing the paths of `.pt` files containing tensor embeddings.\n\n    The manifest must have a `column_mapping[\"multi_id\"]` column that contains the\n    unique identifier group of embeddings. For oncology datasets, this would be usually\n    the slide id. Each row in the manifest file points to a .pt file that can contain\n    one or multiple embeddings (either as a list or stacked tensors). There can also be\n    multiple rows for the same `multi_id`, in which case the embeddings from the different\n    .pt files corresponding to that same `multi_id` will be stacked along the first dimension.\n\n    Args:\n        root: Root directory of the dataset.\n        manifest_file: The path to the manifest file, which is relative to\n            the `root` argument.\n        split: The dataset split to use. The `split` column of the manifest\n            file will be splitted based on this value.\n        column_mapping: Defines the map between the variables and the manifest\n            columns. It will overwrite the `default_column_mapping` with\n            the provided values, so that `column_mapping` can contain only the\n            values which are altered or missing.\n        embeddings_transforms: A function/transform that transforms the embedding.\n        target_transforms: A function/transform that transforms the target.\n    \"\"\"\n    super().__init__(\n        manifest_file=manifest_file,\n        root=root,\n        split=split,\n        column_mapping=column_mapping,\n        embeddings_transforms=embeddings_transforms,\n        target_transforms=target_transforms,\n    )\n\n    self._multi_ids: List[int]\n</code></pre>"},{"location":"reference/core/data/transforms/","title":"Transforms","text":""},{"location":"reference/core/data/transforms/#eva.data.transforms.ArrayToTensor","title":"<code>eva.data.transforms.ArrayToTensor</code>","text":"<p>Converts a numpy array to a torch tensor.</p>"},{"location":"reference/core/data/transforms/#eva.data.transforms.ArrayToFloatTensor","title":"<code>eva.data.transforms.ArrayToFloatTensor</code>","text":"<p>               Bases: <code>ArrayToTensor</code></p> <p>Converts a numpy array to a torch tensor and casts it to float.</p>"},{"location":"reference/core/data/transforms/#eva.data.transforms.Pad2DTensor","title":"<code>eva.data.transforms.Pad2DTensor</code>","text":"<p>Pads a 2D tensor to a fixed dimension accross the first dimension.</p> <p>Parameters:</p> Name Type Description Default <code>pad_size</code> <code>int</code> <p>The size to pad the tensor to. If the tensor is larger than this size, no padding will be applied.</p> required <code>pad_value</code> <code>int | float</code> <p>The value to use for padding.</p> <code>float('-inf')</code> Source code in <code>src/eva/core/data/transforms/padding/pad_2d_tensor.py</code> <pre><code>def __init__(self, pad_size: int, pad_value: int | float = float(\"-inf\")):\n    \"\"\"Initialize the transformation.\n\n    Args:\n        pad_size: The size to pad the tensor to. If the tensor is larger than this size,\n            no padding will be applied.\n        pad_value: The value to use for padding.\n    \"\"\"\n    self._pad_size = pad_size\n    self._pad_value = pad_value\n</code></pre>"},{"location":"reference/core/data/transforms/#eva.data.transforms.SampleFromAxis","title":"<code>eva.data.transforms.SampleFromAxis</code>","text":"<p>Samples n_samples entries from a tensor along a given axis.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to draw.</p> required <code>seed</code> <code>int</code> <p>The seed to use for sampling.</p> <code>42</code> <code>axis</code> <code>int</code> <p>The axis along which to sample.</p> <code>0</code> Source code in <code>src/eva/core/data/transforms/sampling/sample_from_axis.py</code> <pre><code>def __init__(self, n_samples: int, seed: int = 42, axis: int = 0):\n    \"\"\"Initialize the transformation.\n\n    Args:\n        n_samples: The number of samples to draw.\n        seed: The seed to use for sampling.\n        axis: The axis along which to sample.\n    \"\"\"\n    self._seed = seed\n    self._n_samples = n_samples\n    self._axis = axis\n    self._generator = self._get_generator()\n</code></pre>"},{"location":"reference/core/loggers/loggers/","title":"Loggers","text":""},{"location":"reference/core/loggers/loggers/#eva.core.loggers.DummyLogger","title":"<code>eva.core.loggers.DummyLogger</code>","text":"<p>               Bases: <code>DummyLogger</code></p> <p>Dummy logger class.</p> <p>This logger is currently used as a placeholder when saving results to remote storage, as common lightning loggers do not work with azure blob storage:</p> <p>https://github.com/Lightning-AI/pytorch-lightning/issues/18861 https://github.com/Lightning-AI/pytorch-lightning/issues/19736</p> <p>Simply disabling the loggers when pointing to remote storage doesn't work because callbacks such as LearningRateMonitor or ModelCheckpoint require a logger to be present.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str</code> <p>The save directory (this logger does not save anything, but callbacks might use this path to save their outputs).</p> required Source code in <code>src/eva/core/loggers/dummy.py</code> <pre><code>def __init__(self, save_dir: str) -&gt; None:\n    \"\"\"Initializes the logger.\n\n    Args:\n        save_dir: The save directory (this logger does not save anything,\n            but callbacks might use this path to save their outputs).\n    \"\"\"\n    super().__init__()\n    self._save_dir = save_dir\n</code></pre>"},{"location":"reference/core/loggers/loggers/#eva.core.loggers.DummyLogger.save_dir","title":"<code>save_dir: str</code>  <code>property</code>","text":"<p>Returns the save directory.</p>"},{"location":"reference/core/metrics/","title":"Metrics","text":"<p>Reference information for the <code>Metrics</code> classes.</p>"},{"location":"reference/core/metrics/average_loss/","title":"Average Loss","text":""},{"location":"reference/core/metrics/average_loss/#eva.metrics.AverageLoss","title":"<code>eva.metrics.AverageLoss</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Average loss metric tracker.</p> Source code in <code>src/eva/core/metrics/average_loss.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes the metric.\"\"\"\n    super().__init__()\n\n    self.add_state(\"value\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n    self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/core/metrics/binary_balanced_accuracy/","title":"Binary Balanced Accuracy","text":""},{"location":"reference/core/metrics/binary_balanced_accuracy/#eva.metrics.BinaryBalancedAccuracy","title":"<code>eva.metrics.BinaryBalancedAccuracy</code>","text":"<p>               Bases: <code>BinaryStatScores</code></p> <p>Computes the balanced accuracy for binary classification.</p>"},{"location":"reference/core/metrics/binary_balanced_accuracy/#eva.metrics.BinaryBalancedAccuracy.compute","title":"<code>compute</code>","text":"<p>Compute accuracy based on inputs passed in to <code>update</code> previously.</p> Source code in <code>src/eva/core/metrics/binary_balanced_accuracy.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute accuracy based on inputs passed in to ``update`` previously.\"\"\"\n    tp, fp, tn, fn = self._final_state()\n    sensitivity = _safe_divide(tp, tp + fn)\n    specificity = _safe_divide(tn, tn + fp)\n    return 0.5 * (sensitivity + specificity)\n</code></pre>"},{"location":"reference/core/metrics/core/","title":"Core","text":""},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule","title":"<code>eva.metrics.MetricModule</code>","text":"<p>               Bases: <code>Module</code></p> <p>The metrics module.</p> <p>Allows to store and keep track of <code>train</code>, <code>val</code> and <code>test</code> metrics.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>MetricCollection | None</code> <p>The training metric collection.</p> required <code>val</code> <code>MetricCollection | None</code> <p>The validation metric collection.</p> required <code>test</code> <code>MetricCollection | None</code> <p>The test metric collection.</p> required Source code in <code>src/eva/core/metrics/structs/module.py</code> <pre><code>def __init__(\n    self,\n    train: collection.MetricCollection | None,\n    val: collection.MetricCollection | None,\n    test: collection.MetricCollection | None,\n) -&gt; None:\n    \"\"\"Initializes the metrics for the Trainer.\n\n    Args:\n        train: The training metric collection.\n        val: The validation metric collection.\n        test: The test metric collection.\n    \"\"\"\n    super().__init__()\n\n    self._train = train or self.default_metric_collection\n    self._val = val or self.default_metric_collection\n    self._test = test or self.default_metric_collection\n</code></pre>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule.default_metric_collection","title":"<code>default_metric_collection: collection.MetricCollection</code>  <code>property</code>","text":"<p>Returns the default metric collection.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule.training_metrics","title":"<code>training_metrics: collection.MetricCollection</code>  <code>property</code>","text":"<p>Returns the metrics of the train dataset.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule.validation_metrics","title":"<code>validation_metrics: collection.MetricCollection</code>  <code>property</code>","text":"<p>Returns the metrics of the validation dataset.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule.test_metrics","title":"<code>test_metrics: collection.MetricCollection</code>  <code>property</code>","text":"<p>Returns the metrics of the test dataset.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule.from_metrics","title":"<code>from_metrics</code>  <code>classmethod</code>","text":"<p>Initializes a metric module from a list of metrics.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>MetricModuleType | None</code> <p>Metrics for the training stage.</p> required <code>val</code> <code>MetricModuleType | None</code> <p>Metrics for the validation stage.</p> required <code>test</code> <code>MetricModuleType | None</code> <p>Metrics for the test stage.</p> required <code>separator</code> <code>str</code> <p>The separator between the group name of the metric and the metric itself.</p> <code>'/'</code> <code>compute_groups</code> <code>bool | List[List[str]]</code> <p>All metrics in a compute group share the same metric state and are therefore only different in their compute step. To disable this behavior, set to <code>False</code>.</p> <code>True</code> Source code in <code>src/eva/core/metrics/structs/module.py</code> <pre><code>@classmethod\ndef from_metrics(\n    cls,\n    train: MetricModuleType | None,\n    val: MetricModuleType | None,\n    test: MetricModuleType | None,\n    *,\n    separator: str = \"/\",\n    compute_groups: bool | List[List[str]] = True,\n) -&gt; MetricModule:\n    \"\"\"Initializes a metric module from a list of metrics.\n\n    Args:\n        train: Metrics for the training stage.\n        val: Metrics for the validation stage.\n        test: Metrics for the test stage.\n        separator: The separator between the group name of the metric\n            and the metric itself.\n        compute_groups: All metrics in a compute group share the same metric state\n            and are therefore only different in their compute step. To disable this\n            behavior, set to `False`.\n    \"\"\"\n    return cls(\n        train=_create_collection_from_metrics(\n            train, prefix=\"train\" + separator, compute_groups=compute_groups\n        ),\n        val=_create_collection_from_metrics(\n            val, prefix=\"val\" + separator, compute_groups=compute_groups\n        ),\n        test=_create_collection_from_metrics(\n            test, prefix=\"test\" + separator, compute_groups=compute_groups\n        ),\n    )\n</code></pre>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricModule.from_schema","title":"<code>from_schema</code>  <code>classmethod</code>","text":"<p>Initializes a metric module from the metrics schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>MetricsSchema</code> <p>The dataclass metric schema.</p> required <code>separator</code> <code>str</code> <p>The separator between the group name of the metric and the metric itself.</p> <code>'/'</code> <code>compute_groups</code> <code>bool</code> <p>All metrics in a compute group share the same metric state and are therefore only different in their compute step. To disable this behavior, set to <code>False</code>.</p> <code>True</code> Source code in <code>src/eva/core/metrics/structs/module.py</code> <pre><code>@classmethod\ndef from_schema(\n    cls, schema: schemas.MetricsSchema, *, separator: str = \"/\", compute_groups: bool = True\n) -&gt; MetricModule:\n    \"\"\"Initializes a metric module from the metrics schema.\n\n    Args:\n        schema: The dataclass metric schema.\n        separator: The separator between the group name of the metric\n            and the metric itself.\n        compute_groups: All metrics in a compute group share the same metric state\n            and are therefore only different in their compute step. To disable this\n            behavior, set to `False`.\n    \"\"\"\n    return cls.from_metrics(\n        train=schema.training_metrics,\n        val=schema.evaluation_metrics,\n        test=schema.evaluation_metrics,\n        separator=separator,\n        compute_groups=compute_groups,\n    )\n</code></pre>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricsSchema","title":"<code>eva.metrics.MetricsSchema</code>  <code>dataclass</code>","text":"<p>Metrics schema.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricsSchema.common","title":"<code>common: MetricModuleType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Holds the common train and evaluation metrics.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricsSchema.train","title":"<code>train: MetricModuleType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The exclusive training metrics.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricsSchema.evaluation","title":"<code>evaluation: MetricModuleType | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The exclusive evaluation metrics.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricsSchema.training_metrics","title":"<code>training_metrics: MetricModuleType | None</code>  <code>property</code>","text":"<p>Returns the training metics.</p>"},{"location":"reference/core/metrics/core/#eva.metrics.MetricsSchema.evaluation_metrics","title":"<code>evaluation_metrics: MetricModuleType | None</code>  <code>property</code>","text":"<p>Returns the evaluation metics.</p>"},{"location":"reference/core/metrics/defaults/","title":"Defaults","text":""},{"location":"reference/core/metrics/defaults/#eva.metrics.BinaryClassificationMetrics","title":"<code>eva.metrics.BinaryClassificationMetrics</code>","text":"<p>               Bases: <code>MetricCollection</code></p> <p>Default metrics for binary classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Threshold for transforming probability to binary (0,1) predictions</p> <code>0.5</code> <code>ignore_index</code> <code>int | None</code> <p>Specifies a target value that is ignored and does not contribute to the metric calculation.</p> <code>None</code> <code>prefix</code> <code>str | None</code> <p>A string to append in front of the keys of the output dict.</p> <code>None</code> <code>postfix</code> <code>str | None</code> <p>A string to append after the keys of the output dict.</p> <code>None</code> Source code in <code>src/eva/core/metrics/defaults/classification/binary.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    ignore_index: int | None = None,\n    prefix: str | None = None,\n    postfix: str | None = None,\n) -&gt; None:\n    \"\"\"Initializes the binary classification metrics.\n\n    Args:\n        threshold: Threshold for transforming probability to binary (0,1) predictions\n        ignore_index: Specifies a target value that is ignored and does not\n            contribute to the metric calculation.\n        prefix: A string to append in front of the keys of the output dict.\n        postfix: A string to append after the keys of the output dict.\n    \"\"\"\n    super().__init__(\n        metrics=[\n            classification.BinaryAUROC(\n                ignore_index=ignore_index,\n            ),\n            classification.BinaryAccuracy(\n                threshold=threshold,\n                ignore_index=ignore_index,\n            ),\n            binary_balanced_accuracy.BinaryBalancedAccuracy(\n                threshold=threshold,\n                ignore_index=ignore_index,\n            ),\n            classification.BinaryF1Score(\n                threshold=threshold,\n                ignore_index=ignore_index,\n            ),\n            classification.BinaryPrecision(\n                threshold=threshold,\n                ignore_index=ignore_index,\n            ),\n            classification.BinaryRecall(\n                threshold=threshold,\n                ignore_index=ignore_index,\n            ),\n        ],\n        prefix=prefix,\n        postfix=postfix,\n        compute_groups=[\n            [\n                \"BinaryAccuracy\",\n                \"BinaryBalancedAccuracy\",\n                \"BinaryF1Score\",\n                \"BinaryPrecision\",\n                \"BinaryRecall\",\n            ],\n            [\n                \"BinaryAUROC\",\n            ],\n        ],\n    )\n</code></pre>"},{"location":"reference/core/metrics/defaults/#eva.metrics.MulticlassClassificationMetrics","title":"<code>eva.metrics.MulticlassClassificationMetrics</code>","text":"<p>               Bases: <code>MetricCollection</code></p> <p>Default metrics for multi-class classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Integer specifying the number of classes.</p> required <code>average</code> <code>Literal['macro', 'weighted', 'none']</code> <p>Defines the reduction that is applied over labels.</p> <code>'macro'</code> <code>ignore_index</code> <code>int | None</code> <p>Specifies a target value that is ignored and does not contribute to the metric calculation.</p> <code>None</code> <code>prefix</code> <code>str | None</code> <p>A string to append in front of the keys of the output dict.</p> <code>None</code> <code>postfix</code> <code>str | None</code> <p>A string to append after the keys of the output dict.</p> <code>None</code> Source code in <code>src/eva/core/metrics/defaults/classification/multiclass.py</code> <pre><code>def __init__(\n    self,\n    num_classes: int,\n    average: Literal[\"macro\", \"weighted\", \"none\"] = \"macro\",\n    ignore_index: int | None = None,\n    prefix: str | None = None,\n    postfix: str | None = None,\n) -&gt; None:\n    \"\"\"Initializes the multi-class classification metrics.\n\n    Args:\n        num_classes: Integer specifying the number of classes.\n        average: Defines the reduction that is applied over labels.\n        ignore_index: Specifies a target value that is ignored and does not\n            contribute to the metric calculation.\n        prefix: A string to append in front of the keys of the output dict.\n        postfix: A string to append after the keys of the output dict.\n    \"\"\"\n    super().__init__(\n        metrics=[\n            classification.MulticlassAUROC(\n                num_classes=num_classes,\n                average=average,\n                ignore_index=ignore_index,\n            ),\n            classification.MulticlassAccuracy(\n                num_classes=num_classes,\n                average=average,\n                ignore_index=ignore_index,\n            ),\n            classification.MulticlassF1Score(\n                num_classes=num_classes,\n                average=average,\n                ignore_index=ignore_index,\n            ),\n            classification.MulticlassPrecision(\n                num_classes=num_classes,\n                average=average,\n                ignore_index=ignore_index,\n            ),\n            classification.MulticlassRecall(\n                num_classes=num_classes,\n                average=average,\n                ignore_index=ignore_index,\n            ),\n        ],\n        prefix=prefix,\n        postfix=postfix,\n        compute_groups=[\n            [\n                \"MulticlassAccuracy\",\n                \"MulticlassF1Score\",\n                \"MulticlassPrecision\",\n                \"MulticlassRecall\",\n            ],\n            [\n                \"MulticlassAUROC\",\n            ],\n        ],\n    )\n</code></pre>"},{"location":"reference/core/models/modules/","title":"Modules","text":"<p>Reference information for the model <code>Modules</code> API.</p>"},{"location":"reference/core/models/modules/#eva.models.modules.ModelModule","title":"<code>eva.models.modules.ModelModule</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>The base model module.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>MetricsSchema | None</code> <p>The metric groups to track.</p> <code>None</code> <code>postprocess</code> <code>BatchPostProcess | None</code> <p>A list of helper functions to apply after the loss and before the metrics calculation to the model predictions and targets.</p> <code>None</code> Source code in <code>src/eva/core/models/modules/module.py</code> <pre><code>def __init__(\n    self,\n    metrics: metrics_lib.MetricsSchema | None = None,\n    postprocess: batch_postprocess.BatchPostProcess | None = None,\n) -&gt; None:\n    \"\"\"Initializes the basic module.\n\n    Args:\n        metrics: The metric groups to track.\n        postprocess: A list of helper functions to apply after the\n            loss and before the metrics calculation to the model\n            predictions and targets.\n    \"\"\"\n    super().__init__()\n\n    self._metrics = metrics or self.default_metrics\n    self._postprocess = postprocess or self.default_postprocess\n\n    self.metrics = metrics_lib.MetricModule.from_schema(self._metrics)\n</code></pre>"},{"location":"reference/core/models/modules/#eva.models.modules.ModelModule.default_metrics","title":"<code>default_metrics: metrics_lib.MetricsSchema</code>  <code>property</code>","text":"<p>The default metrics.</p>"},{"location":"reference/core/models/modules/#eva.models.modules.ModelModule.default_postprocess","title":"<code>default_postprocess: batch_postprocess.BatchPostProcess</code>  <code>property</code>","text":"<p>The default post-processes.</p>"},{"location":"reference/core/models/modules/#eva.models.modules.ModelModule.metrics_device","title":"<code>metrics_device: torch.device</code>  <code>property</code>","text":"<p>Returns the device by which the metrics should be calculated.</p>"},{"location":"reference/core/models/modules/#eva.models.modules.HeadModule","title":"<code>eva.models.modules.HeadModule</code>","text":"<p>               Bases: <code>ModelModule</code></p> <p>Neural Net Head Module for training on features.</p> <p>It can be used for supervised (mini-batch) stochastic gradient descent downstream tasks such as classification, regression and segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>head</code> <code>Dict[str, Any] | MODEL_TYPE</code> <p>The neural network that would be trained on the features. If its a dictionary, it will be parsed to an object during the <code>configure_model</code> step.</p> required <code>criterion</code> <code>Callable[..., Tensor]</code> <p>The loss function to use.</p> required <code>backbone</code> <code>MODEL_TYPE | None</code> <p>The feature extractor. If <code>None</code>, it will be expected that the input batch returns the features directly.</p> <code>None</code> <code>optimizer</code> <code>OptimizerCallable</code> <p>The optimizer to use.</p> <code>Adam</code> <code>lr_scheduler</code> <code>LRSchedulerCallable</code> <p>The learning rate scheduler to use.</p> <code>ConstantLR</code> <code>metrics</code> <code>MetricsSchema | None</code> <p>The metric groups to track.</p> <code>None</code> <code>postprocess</code> <code>BatchPostProcess | None</code> <p>A list of helper functions to apply after the loss and before the metrics calculation to the model predictions and targets.</p> <code>None</code> <code>save_head_only</code> <code>bool</code> <p>Whether to save only the head during checkpointing. If False, will also save the backbone (not recommended when frozen).</p> <code>True</code> Source code in <code>src/eva/core/models/modules/head.py</code> <pre><code>def __init__(\n    self,\n    head: Dict[str, Any] | MODEL_TYPE,\n    criterion: Callable[..., torch.Tensor],\n    backbone: MODEL_TYPE | None = None,\n    optimizer: OptimizerCallable = optim.Adam,\n    lr_scheduler: LRSchedulerCallable = lr_scheduler.ConstantLR,\n    metrics: metrics_lib.MetricsSchema | None = None,\n    postprocess: batch_postprocess.BatchPostProcess | None = None,\n    save_head_only: bool = True,\n) -&gt; None:\n    \"\"\"Initializes the neural net head module.\n\n    Args:\n        head: The neural network that would be trained on the features.\n            If its a dictionary, it will be parsed to an object during the\n            `configure_model` step.\n        criterion: The loss function to use.\n        backbone: The feature extractor. If `None`, it will be expected\n            that the input batch returns the features directly.\n        optimizer: The optimizer to use.\n        lr_scheduler: The learning rate scheduler to use.\n        metrics: The metric groups to track.\n        postprocess: A list of helper functions to apply after the\n            loss and before the metrics calculation to the model\n            predictions and targets.\n        save_head_only: Whether to save only the head during checkpointing. If False,\n            will also save the backbone (not recommended when frozen).\n    \"\"\"\n    super().__init__(metrics=metrics, postprocess=postprocess)\n\n    self.head = head  # type: ignore\n    self.criterion = criterion\n    self.backbone = backbone\n    self.optimizer = optimizer\n    self.lr_scheduler = lr_scheduler\n    self.save_head_only = save_head_only\n</code></pre>"},{"location":"reference/core/models/modules/#eva.models.modules.InferenceModule","title":"<code>eva.models.modules.InferenceModule</code>","text":"<p>               Bases: <code>ModelModule</code></p> <p>An lightweight model module to perform inference.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>MODEL_TYPE</code> <p>The network to be used for inference.</p> required Source code in <code>src/eva/core/models/modules/inference.py</code> <pre><code>def __init__(self, backbone: MODEL_TYPE) -&gt; None:\n    \"\"\"Initializes the module.\n\n    Args:\n        backbone: The network to be used for inference.\n    \"\"\"\n    super().__init__(metrics=None)\n\n    self.backbone = backbone\n</code></pre>"},{"location":"reference/core/models/networks/","title":"Networks","text":"<p>Reference information for the model <code>Networks</code> API.</p>"},{"location":"reference/core/models/networks/#eva.models.networks.MLP","title":"<code>eva.models.networks.MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>A Multi-layer Perceptron (MLP) network.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>The number of input features.</p> required <code>output_size</code> <code>int</code> <p>The number of output features.</p> required <code>hidden_layer_sizes</code> <code>Tuple[int, ...] | None</code> <p>A list specifying the number of units in each hidden layer.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout probability for hidden layers.</p> <code>0.0</code> <code>hidden_activation_fn</code> <code>Type[Module] | None</code> <p>Activation function to use for hidden layers. Default is ReLU.</p> <code>ReLU</code> <code>output_activation_fn</code> <code>Type[Module] | None</code> <p>Activation function to use for the output layer. Default is None.</p> <code>None</code> Source code in <code>src/eva/core/models/networks/mlp.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    output_size: int,\n    hidden_layer_sizes: Tuple[int, ...] | None = None,\n    hidden_activation_fn: Type[torch.nn.Module] | None = nn.ReLU,\n    output_activation_fn: Type[torch.nn.Module] | None = None,\n    dropout: float = 0.0,\n) -&gt; None:\n    \"\"\"Initializes the MLP.\n\n    Args:\n        input_size: The number of input features.\n        output_size: The number of output features.\n        hidden_layer_sizes: A list specifying the number of units in each hidden layer.\n        dropout: Dropout probability for hidden layers.\n        hidden_activation_fn: Activation function to use for hidden layers. Default is ReLU.\n        output_activation_fn: Activation function to use for the output layer. Default is None.\n    \"\"\"\n    super().__init__()\n\n    self.input_size = input_size\n    self.output_size = output_size\n    self.hidden_layer_sizes = hidden_layer_sizes if hidden_layer_sizes is not None else ()\n    self.hidden_activation_fn = hidden_activation_fn\n    self.output_activation_fn = output_activation_fn\n    self.dropout = dropout\n\n    self._network = self._build_network()\n</code></pre>"},{"location":"reference/core/models/networks/#eva.models.networks.MLP.forward","title":"<code>forward</code>","text":"<p>Defines the forward pass of the MLP.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output of the network.</p> Source code in <code>src/eva/core/models/networks/mlp.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Defines the forward pass of the MLP.\n\n    Args:\n        x: The input tensor.\n\n    Returns:\n        The output of the network.\n    \"\"\"\n    return self._network(x)\n</code></pre>"},{"location":"reference/core/models/networks/#wrappers","title":"Wrappers","text":""},{"location":"reference/core/models/networks/#eva.models.wrappers.BaseModel","title":"<code>eva.models.wrappers.BaseModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for model wrappers.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> Source code in <code>src/eva/core/models/wrappers/base.py</code> <pre><code>def __init__(self, tensor_transforms: Callable | None = None) -&gt; None:\n    \"\"\"Initializes the model.\n\n    Args:\n        tensor_transforms: The transforms to apply to the output\n            tensor produced by the model.\n    \"\"\"\n    super().__init__()\n\n    self._output_transforms = tensor_transforms\n\n    self._model: Callable[..., torch.Tensor] | nn.Module\n</code></pre>"},{"location":"reference/core/models/networks/#eva.models.wrappers.BaseModel.load_model","title":"<code>load_model</code>  <code>abstractmethod</code>","text":"<p>Loads the model.</p> Source code in <code>src/eva/core/models/wrappers/base.py</code> <pre><code>@abc.abstractmethod\ndef load_model(self) -&gt; Callable[..., torch.Tensor]:\n    \"\"\"Loads the model.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/core/models/networks/#eva.models.wrappers.BaseModel.model_forward","title":"<code>model_forward</code>","text":"<p>Implements the forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor to the model.</p> required Source code in <code>src/eva/core/models/wrappers/base.py</code> <pre><code>def model_forward(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Implements the forward pass of the model.\n\n    Args:\n        tensor: The input tensor to the model.\n    \"\"\"\n    return self._model(tensor)\n</code></pre>"},{"location":"reference/core/models/networks/#eva.models.wrappers.ModelFromFunction","title":"<code>eva.models.wrappers.ModelFromFunction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrapper class for models which are initialized from functions.</p> <p>This is helpful for initializing models in a <code>.yaml</code> configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Callable[..., Module]</code> <p>The path to the callable object (class or function).</p> required <code>arguments</code> <code>Dict[str, Any] | None</code> <p>The extra callable function / class arguments.</p> <code>None</code> <code>checkpoint_path</code> <code>str | None</code> <p>The path to the checkpoint to load the model weights from. This is currently only supported for torch model checkpoints. For other formats, the checkpoint loading should be handled within the provided callable object in . <code>None</code> <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> Source code in <code>src/eva/core/models/wrappers/from_function.py</code> <pre><code>def __init__(\n    self,\n    path: Callable[..., nn.Module],\n    arguments: Dict[str, Any] | None = None,\n    checkpoint_path: str | None = None,\n    tensor_transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initializes and constructs the model.\n\n    Args:\n        path: The path to the callable object (class or function).\n        arguments: The extra callable function / class arguments.\n        checkpoint_path: The path to the checkpoint to load the model\n            weights from. This is currently only supported for torch\n            model checkpoints. For other formats, the checkpoint loading\n            should be handled within the provided callable object in &lt;path&gt;.\n        tensor_transforms: The transforms to apply to the output tensor\n            produced by the model.\n    \"\"\"\n    super().__init__(tensor_transforms=tensor_transforms)\n\n    self._path = path\n    self._arguments = arguments\n    self._checkpoint_path = checkpoint_path\n\n    self.load_model()\n</code></pre>"},{"location":"reference/core/models/networks/#eva.models.wrappers.HuggingFaceModel","title":"<code>eva.models.wrappers.HuggingFaceModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrapper class for loading HuggingFace <code>transformers</code> models.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The model name or path to load the model from. This can be a local path or a model name from the <code>HuggingFace</code> model hub.</p> required <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> <code>model_kwargs</code> <code>Dict[str, Any] | None</code> <p>The arguments used for instantiating the model.</p> <code>None</code> Source code in <code>src/eva/core/models/wrappers/huggingface.py</code> <pre><code>def __init__(\n    self,\n    model_name_or_path: str,\n    tensor_transforms: Callable | None = None,\n    model_kwargs: Dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"Initializes the model.\n\n    Args:\n        model_name_or_path: The model name or path to load the model from.\n            This can be a local path or a model name from the `HuggingFace`\n            model hub.\n        tensor_transforms: The transforms to apply to the output tensor\n            produced by the model.\n        model_kwargs: The arguments used for instantiating the model.\n    \"\"\"\n    super().__init__(tensor_transforms=tensor_transforms)\n\n    self._model_name_or_path = model_name_or_path\n    self._model_kwargs = model_kwargs or {}\n\n    self.load_model()\n</code></pre>"},{"location":"reference/core/models/networks/#eva.models.wrappers.ONNXModel","title":"<code>eva.models.wrappers.ONNXModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrapper class for loading ONNX models.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the .onnx model file.</p> required <code>device</code> <code>Literal['cpu', 'cuda'] | None</code> <p>The device to run the model on. This can be either \"cpu\" or \"cuda\".</p> <code>'cpu'</code> <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> Source code in <code>src/eva/core/models/wrappers/onnx.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    device: Literal[\"cpu\", \"cuda\"] | None = \"cpu\",\n    tensor_transforms: Callable | None = None,\n):\n    \"\"\"Initializes the model.\n\n    Args:\n        path: The path to the .onnx model file.\n        device: The device to run the model on. This can be either \"cpu\" or \"cuda\".\n        tensor_transforms: The transforms to apply to the output tensor produced by the model.\n    \"\"\"\n    super().__init__(tensor_transforms=tensor_transforms)\n\n    self._path = path\n    self._device = device\n\n    self.load_model()\n</code></pre>"},{"location":"reference/core/models/transforms/","title":"Transforms","text":"<p>Reference information for the model <code>Transforms</code> API.</p>"},{"location":"reference/core/models/transforms/#eva.models.transforms.ExtractCLSFeatures","title":"<code>eva.models.transforms.ExtractCLSFeatures</code>","text":"<p>Extracts the CLS token from a ViT model output.</p> <p>Parameters:</p> Name Type Description Default <code>cls_index</code> <code>int</code> <p>The index of the CLS token in the output tensor.</p> <code>0</code> <code>num_register_tokens</code> <code>int</code> <p>The number of register tokens in the model output.</p> <code>0</code> <code>include_patch_tokens</code> <code>bool</code> <p>Whether to concat the mean aggregated patch tokens with the cls token.</p> <code>False</code> Source code in <code>src/eva/core/models/transforms/extract_cls_features.py</code> <pre><code>def __init__(\n    self, cls_index: int = 0, num_register_tokens: int = 0, include_patch_tokens: bool = False\n) -&gt; None:\n    \"\"\"Initializes the transformation.\n\n    Args:\n        cls_index: The index of the CLS token in the output tensor.\n        num_register_tokens: The number of register tokens in the model output.\n        include_patch_tokens: Whether to concat the mean aggregated patch tokens with\n            the cls token.\n    \"\"\"\n    self._cls_index = cls_index\n    self._num_register_tokens = num_register_tokens\n    self._include_patch_tokens = include_patch_tokens\n</code></pre>"},{"location":"reference/core/models/transforms/#eva.models.transforms.ExtractPatchFeatures","title":"<code>eva.models.transforms.ExtractPatchFeatures</code>","text":"<p>Extracts the patch features from a ViT model output.</p> <p>Parameters:</p> Name Type Description Default <code>has_cls_token</code> <code>bool</code> <p>If set to <code>True</code>, the model output is expected to have a classification token.</p> <code>True</code> <code>num_register_tokens</code> <code>int</code> <p>The number of register tokens in the model output.</p> <code>0</code> <code>ignore_remaining_dims</code> <code>bool</code> <p>If set to <code>True</code>, ignore the remaining dimensions of the patch grid if it is not a square number.</p> <code>False</code> Source code in <code>src/eva/core/models/transforms/extract_patch_features.py</code> <pre><code>def __init__(\n    self,\n    has_cls_token: bool = True,\n    num_register_tokens: int = 0,\n    ignore_remaining_dims: bool = False,\n) -&gt; None:\n    \"\"\"Initializes the transformation.\n\n    Args:\n        has_cls_token: If set to `True`, the model output is expected to have\n            a classification token.\n        num_register_tokens: The number of register tokens in the model output.\n        ignore_remaining_dims: If set to `True`, ignore the remaining dimensions\n            of the patch grid if it is not a square number.\n    \"\"\"\n    self._has_cls_token = has_cls_token\n    self._num_register_tokens = num_register_tokens\n    self._ignore_remaining_dims = ignore_remaining_dims\n</code></pre>"},{"location":"reference/core/models/wrappers/","title":"Wrappers","text":"<p>Reference information for the model <code>Wrappers</code> API.</p>"},{"location":"reference/core/models/wrappers/#eva.models.wrappers.BaseModel","title":"<code>eva.models.wrappers.BaseModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for model wrappers.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> Source code in <code>src/eva/core/models/wrappers/base.py</code> <pre><code>def __init__(self, tensor_transforms: Callable | None = None) -&gt; None:\n    \"\"\"Initializes the model.\n\n    Args:\n        tensor_transforms: The transforms to apply to the output\n            tensor produced by the model.\n    \"\"\"\n    super().__init__()\n\n    self._output_transforms = tensor_transforms\n\n    self._model: Callable[..., torch.Tensor] | nn.Module\n</code></pre>"},{"location":"reference/core/models/wrappers/#eva.models.wrappers.BaseModel.load_model","title":"<code>load_model</code>  <code>abstractmethod</code>","text":"<p>Loads the model.</p> Source code in <code>src/eva/core/models/wrappers/base.py</code> <pre><code>@abc.abstractmethod\ndef load_model(self) -&gt; Callable[..., torch.Tensor]:\n    \"\"\"Loads the model.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/core/models/wrappers/#eva.models.wrappers.BaseModel.model_forward","title":"<code>model_forward</code>","text":"<p>Implements the forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor to the model.</p> required Source code in <code>src/eva/core/models/wrappers/base.py</code> <pre><code>def model_forward(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Implements the forward pass of the model.\n\n    Args:\n        tensor: The input tensor to the model.\n    \"\"\"\n    return self._model(tensor)\n</code></pre>"},{"location":"reference/core/models/wrappers/#eva.models.wrappers.ModelFromFunction","title":"<code>eva.models.wrappers.ModelFromFunction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrapper class for models which are initialized from functions.</p> <p>This is helpful for initializing models in a <code>.yaml</code> configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Callable[..., Module]</code> <p>The path to the callable object (class or function).</p> required <code>arguments</code> <code>Dict[str, Any] | None</code> <p>The extra callable function / class arguments.</p> <code>None</code> <code>checkpoint_path</code> <code>str | None</code> <p>The path to the checkpoint to load the model weights from. This is currently only supported for torch model checkpoints. For other formats, the checkpoint loading should be handled within the provided callable object in . <code>None</code> <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> Source code in <code>src/eva/core/models/wrappers/from_function.py</code> <pre><code>def __init__(\n    self,\n    path: Callable[..., nn.Module],\n    arguments: Dict[str, Any] | None = None,\n    checkpoint_path: str | None = None,\n    tensor_transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initializes and constructs the model.\n\n    Args:\n        path: The path to the callable object (class or function).\n        arguments: The extra callable function / class arguments.\n        checkpoint_path: The path to the checkpoint to load the model\n            weights from. This is currently only supported for torch\n            model checkpoints. For other formats, the checkpoint loading\n            should be handled within the provided callable object in &lt;path&gt;.\n        tensor_transforms: The transforms to apply to the output tensor\n            produced by the model.\n    \"\"\"\n    super().__init__(tensor_transforms=tensor_transforms)\n\n    self._path = path\n    self._arguments = arguments\n    self._checkpoint_path = checkpoint_path\n\n    self.load_model()\n</code></pre>"},{"location":"reference/core/models/wrappers/#eva.models.wrappers.HuggingFaceModel","title":"<code>eva.models.wrappers.HuggingFaceModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrapper class for loading HuggingFace <code>transformers</code> models.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The model name or path to load the model from. This can be a local path or a model name from the <code>HuggingFace</code> model hub.</p> required <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> <code>model_kwargs</code> <code>Dict[str, Any] | None</code> <p>The arguments used for instantiating the model.</p> <code>None</code> Source code in <code>src/eva/core/models/wrappers/huggingface.py</code> <pre><code>def __init__(\n    self,\n    model_name_or_path: str,\n    tensor_transforms: Callable | None = None,\n    model_kwargs: Dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"Initializes the model.\n\n    Args:\n        model_name_or_path: The model name or path to load the model from.\n            This can be a local path or a model name from the `HuggingFace`\n            model hub.\n        tensor_transforms: The transforms to apply to the output tensor\n            produced by the model.\n        model_kwargs: The arguments used for instantiating the model.\n    \"\"\"\n    super().__init__(tensor_transforms=tensor_transforms)\n\n    self._model_name_or_path = model_name_or_path\n    self._model_kwargs = model_kwargs or {}\n\n    self.load_model()\n</code></pre>"},{"location":"reference/core/models/wrappers/#eva.models.wrappers.ONNXModel","title":"<code>eva.models.wrappers.ONNXModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrapper class for loading ONNX models.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the .onnx model file.</p> required <code>device</code> <code>Literal['cpu', 'cuda'] | None</code> <p>The device to run the model on. This can be either \"cpu\" or \"cuda\".</p> <code>'cpu'</code> <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> Source code in <code>src/eva/core/models/wrappers/onnx.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    device: Literal[\"cpu\", \"cuda\"] | None = \"cpu\",\n    tensor_transforms: Callable | None = None,\n):\n    \"\"\"Initializes the model.\n\n    Args:\n        path: The path to the .onnx model file.\n        device: The device to run the model on. This can be either \"cpu\" or \"cuda\".\n        tensor_transforms: The transforms to apply to the output tensor produced by the model.\n    \"\"\"\n    super().__init__(tensor_transforms=tensor_transforms)\n\n    self._path = path\n    self._device = device\n\n    self.load_model()\n</code></pre>"},{"location":"reference/core/trainers/functional/","title":"Functional","text":"<p>Reference information for the trainers <code>Functional</code> API.</p>"},{"location":"reference/core/trainers/functional/#eva.core.trainers.functional.run_evaluation_session","title":"<code>eva.core.trainers.functional.run_evaluation_session</code>","text":"<p>Runs a downstream evaluation session out-of-place.</p> <p>It performs an evaluation run (fit and evaluate) on the model multiple times. Note that as the input <code>base_trainer</code> and <code>base_model</code> would be cloned, the input object would not be modified.</p> <p>Parameters:</p> Name Type Description Default <code>base_trainer</code> <code>Trainer</code> <p>The base trainer module to use.</p> required <code>base_model</code> <code>ModelModule</code> <p>The base model module to use.</p> required <code>datamodule</code> <code>DataModule</code> <p>The data module.</p> required <code>n_runs</code> <code>int</code> <p>The amount of runs (fit and evaluate) to perform.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>Whether to verbose the session metrics instead of these of each individual runs and vice-versa.</p> <code>True</code> Source code in <code>src/eva/core/trainers/functional.py</code> <pre><code>def run_evaluation_session(\n    base_trainer: eva_trainer.Trainer,\n    base_model: modules.ModelModule,\n    datamodule: datamodules.DataModule,\n    *,\n    n_runs: int = 1,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Runs a downstream evaluation session out-of-place.\n\n    It performs an evaluation run (fit and evaluate) on the model\n    multiple times. Note that as the input `base_trainer` and\n    `base_model` would be cloned, the input object would not\n    be modified.\n\n    Args:\n        base_trainer: The base trainer module to use.\n        base_model: The base model module to use.\n        datamodule: The data module.\n        n_runs: The amount of runs (fit and evaluate) to perform.\n        verbose: Whether to verbose the session metrics instead of\n            these of each individual runs and vice-versa.\n    \"\"\"\n    recorder = _recorder.SessionRecorder(output_dir=base_trainer.default_log_dir, verbose=verbose)\n    for run_index in range(n_runs):\n        validation_scores, test_scores = run_evaluation(\n            base_trainer,\n            base_model,\n            datamodule,\n            run_id=f\"run_{run_index}\",\n            verbose=not verbose,\n        )\n        recorder.update(validation_scores, test_scores)\n    recorder.save()\n</code></pre>"},{"location":"reference/core/trainers/functional/#eva.core.trainers.functional.run_evaluation","title":"<code>eva.core.trainers.functional.run_evaluation</code>","text":"<p>Fits and evaluates a model out-of-place.</p> <p>Parameters:</p> Name Type Description Default <code>base_trainer</code> <code>Trainer</code> <p>The base trainer to use but not modify.</p> required <code>base_model</code> <code>ModelModule</code> <p>The model module to use but not modify.</p> required <code>datamodule</code> <code>DataModule</code> <p>The data module.</p> required <code>run_id</code> <code>str | None</code> <p>The run id to be appended to the output log directory. If <code>None</code>, it will use the log directory of the trainer as is.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print the validation and test metrics in the end of the training.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[_EVALUATE_OUTPUT, _EVALUATE_OUTPUT | None]</code> <p>A tuple of with the validation and the test metrics (if exists).</p> Source code in <code>src/eva/core/trainers/functional.py</code> <pre><code>def run_evaluation(\n    base_trainer: eva_trainer.Trainer,\n    base_model: modules.ModelModule,\n    datamodule: datamodules.DataModule,\n    *,\n    run_id: str | None = None,\n    verbose: bool = True,\n) -&gt; Tuple[_EVALUATE_OUTPUT, _EVALUATE_OUTPUT | None]:\n    \"\"\"Fits and evaluates a model out-of-place.\n\n    Args:\n        base_trainer: The base trainer to use but not modify.\n        base_model: The model module to use but not modify.\n        datamodule: The data module.\n        run_id: The run id to be appended to the output log directory.\n            If `None`, it will use the log directory of the trainer as is.\n        verbose: Whether to print the validation and test metrics\n            in the end of the training.\n\n    Returns:\n        A tuple of with the validation and the test metrics (if exists).\n    \"\"\"\n    trainer, model = _utils.clone(base_trainer, base_model)\n    model.configure_model()\n    trainer.setup_log_dirs(run_id or \"\")\n    return fit_and_validate(trainer, model, datamodule, verbose=verbose)\n</code></pre>"},{"location":"reference/core/trainers/functional/#eva.core.trainers.functional.fit_and_validate","title":"<code>eva.core.trainers.functional.fit_and_validate</code>","text":"<p>Fits and evaluates a model in-place.</p> <p>If the test set is set in the datamodule, it will evaluate the model on the test set as well.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer module to use and update in-place.</p> required <code>model</code> <code>ModelModule</code> <p>The model module to use and update in-place.</p> required <code>datamodule</code> <code>DataModule</code> <p>The data module.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print the validation and test metrics in the end of the training.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[_EVALUATE_OUTPUT, _EVALUATE_OUTPUT | None]</code> <p>A tuple of with the validation and the test metrics (if exists).</p> Source code in <code>src/eva/core/trainers/functional.py</code> <pre><code>def fit_and_validate(\n    trainer: eva_trainer.Trainer,\n    model: modules.ModelModule,\n    datamodule: datamodules.DataModule,\n    verbose: bool = True,\n) -&gt; Tuple[_EVALUATE_OUTPUT, _EVALUATE_OUTPUT | None]:\n    \"\"\"Fits and evaluates a model in-place.\n\n    If the test set is set in the datamodule, it will evaluate the model\n    on the test set as well.\n\n    Args:\n        trainer: The trainer module to use and update in-place.\n        model: The model module to use and update in-place.\n        datamodule: The data module.\n        verbose: Whether to print the validation and test metrics\n            in the end of the training.\n\n    Returns:\n        A tuple of with the validation and the test metrics (if exists).\n    \"\"\"\n    trainer.fit(model, datamodule=datamodule)\n    validation_scores = trainer.validate(\n        datamodule=datamodule, verbose=verbose, ckpt_path=trainer.checkpoint_type\n    )\n    test_scores = (\n        None\n        if datamodule.datasets.test is None\n        else trainer.test(datamodule=datamodule, verbose=verbose, ckpt_path=trainer.checkpoint_type)\n    )\n    return validation_scores, test_scores\n</code></pre>"},{"location":"reference/core/trainers/functional/#eva.core.trainers.functional.infer_model","title":"<code>eva.core.trainers.functional.infer_model</code>","text":"<p>Performs model inference out-of-place.</p> <p>Note that the input <code>base_model</code> and <code>base_trainer</code> would not be modified.</p> <p>Parameters:</p> Name Type Description Default <code>base_trainer</code> <code>Trainer</code> <p>The base trainer to use but not modify.</p> required <code>base_model</code> <code>ModelModule</code> <p>The model module to use but not modify.</p> required <code>datamodule</code> <code>DataModule</code> <p>The data module.</p> required <code>return_predictions</code> <code>bool</code> <p>Whether to return the model predictions.</p> <code>False</code> Source code in <code>src/eva/core/trainers/functional.py</code> <pre><code>def infer_model(\n    base_trainer: eva_trainer.Trainer,\n    base_model: modules.ModelModule,\n    datamodule: datamodules.DataModule,\n    *,\n    return_predictions: bool = False,\n) -&gt; None:\n    \"\"\"Performs model inference out-of-place.\n\n    Note that the input `base_model` and `base_trainer` would\n    not be modified.\n\n    Args:\n        base_trainer: The base trainer to use but not modify.\n        base_model: The model module to use but not modify.\n        datamodule: The data module.\n        return_predictions: Whether to return the model predictions.\n    \"\"\"\n    trainer, model = _utils.clone(base_trainer, base_model)\n    return trainer.predict(\n        model=model,\n        datamodule=datamodule,\n        return_predictions=return_predictions,\n    )\n</code></pre>"},{"location":"reference/core/trainers/trainer/","title":"Trainers","text":"<p>Reference information for the <code>Trainers</code> API.</p>"},{"location":"reference/core/trainers/trainer/#eva.core.trainers.Trainer","title":"<code>eva.core.trainers.Trainer</code>","text":"<p>               Bases: <code>Trainer</code></p> <p>Core trainer class.</p> <p>This is an extended version of lightning's core trainer class.</p> <p>For the input arguments, refer to ::class::<code>lightning.pytorch.Trainer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Any</code> <p>Positional arguments of ::class::<code>lightning.pytorch.Trainer</code>.</p> <code>()</code> <code>default_root_dir</code> <code>str</code> <p>The default root directory to store the output logs. Unlike in ::class::<code>lightning.pytorch.Trainer</code>, this path would be the prioritized destination point.</p> <code>'logs'</code> <code>n_runs</code> <code>int</code> <p>The amount of runs (fit and evaluate) to perform in an evaluation session.</p> <code>1</code> <code>checkpoint_type</code> <code>Literal['best', 'last']</code> <p>Wether to load the \"best\" or \"last\" checkpoint saved by the checkpoint callback for evaluations on validation &amp; test sets.</p> <code>'best'</code> <code>kwargs</code> <code>Any</code> <p>Kew-word arguments of ::class::<code>lightning.pytorch.Trainer</code>.</p> <code>{}</code> Source code in <code>src/eva/core/trainers/trainer.py</code> <pre><code>@argparse._defaults_from_env_vars\ndef __init__(\n    self,\n    *args: Any,\n    default_root_dir: str = \"logs\",\n    n_runs: int = 1,\n    checkpoint_type: Literal[\"best\", \"last\"] = \"best\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initializes the trainer.\n\n    For the input arguments, refer to ::class::`lightning.pytorch.Trainer`.\n\n    Args:\n        args: Positional arguments of ::class::`lightning.pytorch.Trainer`.\n        default_root_dir: The default root directory to store the output logs.\n            Unlike in ::class::`lightning.pytorch.Trainer`, this path would be the\n            prioritized destination point.\n        n_runs: The amount of runs (fit and evaluate) to perform in an evaluation session.\n        checkpoint_type: Wether to load the \"best\" or \"last\" checkpoint saved by the checkpoint\n            callback for evaluations on validation &amp; test sets.\n        kwargs: Kew-word arguments of ::class::`lightning.pytorch.Trainer`.\n    \"\"\"\n    super().__init__(*args, default_root_dir=default_root_dir, **kwargs)\n\n    self.checkpoint_type = checkpoint_type\n    self.n_runs = n_runs\n\n    self._session_id: str = _logging.generate_session_id()\n    self._log_dir: str = self.default_log_dir\n\n    self.setup_log_dirs()\n</code></pre>"},{"location":"reference/core/trainers/trainer/#eva.core.trainers.Trainer.default_log_dir","title":"<code>default_log_dir: str</code>  <code>property</code>","text":"<p>Returns the default log directory.</p>"},{"location":"reference/core/trainers/trainer/#eva.core.trainers.Trainer.setup_log_dirs","title":"<code>setup_log_dirs</code>","text":"<p>Setups the logging directory of the trainer and experimental loggers in-place.</p> <p>Parameters:</p> Name Type Description Default <code>subdirectory</code> <code>str</code> <p>Whether to append a subdirectory to the output log.</p> <code>''</code> Source code in <code>src/eva/core/trainers/trainer.py</code> <pre><code>def setup_log_dirs(self, subdirectory: str = \"\") -&gt; None:\n    \"\"\"Setups the logging directory of the trainer and experimental loggers in-place.\n\n    Args:\n        subdirectory: Whether to append a subdirectory to the output log.\n    \"\"\"\n    self._log_dir = os.path.join(self.default_root_dir, self._session_id, subdirectory)\n\n    enabled_loggers = []\n    if isinstance(self.loggers, list) and len(self.loggers) &gt; 0:\n        for logger in self.loggers:\n            if isinstance(logger, (pl_loggers.CSVLogger, pl_loggers.TensorBoardLogger)):\n                if not cloud_io._is_local_file_protocol(self.default_root_dir):\n                    loguru.logger.warning(\n                        f\"Skipped {type(logger).__name__} as remote storage is not supported.\"\n                    )\n                    continue\n                else:\n                    logger._root_dir = self.default_root_dir\n                    logger._name = self._session_id\n                    logger._version = subdirectory\n            enabled_loggers.append(logger)\n\n    self._loggers = enabled_loggers or [eva_loggers.DummyLogger(self._log_dir)]\n</code></pre>"},{"location":"reference/core/trainers/trainer/#eva.core.trainers.Trainer.run_evaluation_session","title":"<code>run_evaluation_session</code>","text":"<p>Runs an evaluation session out-of-place.</p> <p>It performs an evaluation run (fit and evaluate) the model <code>self._n_run</code> times. Note that the input <code>base_model</code> would not be modified, so the weights of the input model will remain as they are.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelModule</code> <p>The base model module to evaluate.</p> required <code>datamodule</code> <code>DataModule</code> <p>The data module.</p> required Source code in <code>src/eva/core/trainers/trainer.py</code> <pre><code>def run_evaluation_session(\n    self,\n    model: modules.ModelModule,\n    datamodule: datamodules.DataModule,\n) -&gt; None:\n    \"\"\"Runs an evaluation session out-of-place.\n\n    It performs an evaluation run (fit and evaluate) the model\n    `self._n_run` times. Note that the input `base_model` would\n    not be modified, so the weights of the input model will remain\n    as they are.\n\n    Args:\n        model: The base model module to evaluate.\n        datamodule: The data module.\n    \"\"\"\n    functional.run_evaluation_session(\n        base_trainer=self,\n        base_model=model,\n        datamodule=datamodule,\n        n_runs=self.n_runs,\n        verbose=self.n_runs &gt; 1,\n    )\n</code></pre>"},{"location":"reference/core/utils/multiprocessing/","title":"Multiprocessing","text":"<p>Reference information for the utils <code>Multiprocessing</code> API.</p>"},{"location":"reference/core/utils/multiprocessing/#eva.core.utils.multiprocessing.Process","title":"<code>eva.core.utils.multiprocessing.Process</code>","text":"<p>               Bases: <code>Process</code></p> <p>Multiprocessing wrapper with logic to propagate exceptions to the parent process.</p> <p>Source: https://stackoverflow.com/a/33599967/4992248</p> Source code in <code>src/eva/core/utils/multiprocessing.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the process.\"\"\"\n    multiprocessing.Process.__init__(self, *args, **kwargs)\n\n    self._parent_conn, self._child_conn = multiprocessing.Pipe()\n    self._exception = None\n</code></pre>"},{"location":"reference/core/utils/multiprocessing/#eva.core.utils.multiprocessing.Process.exception","title":"<code>exception</code>  <code>property</code>","text":"<p>Property that contains exception information from the process.</p>"},{"location":"reference/core/utils/multiprocessing/#eva.core.utils.multiprocessing.Process.run","title":"<code>run</code>","text":"<p>Run the process.</p> Source code in <code>src/eva/core/utils/multiprocessing.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Run the process.\"\"\"\n    try:\n        multiprocessing.Process.run(self)\n        self._child_conn.send(None)\n    except Exception as e:\n        tb = traceback.format_exc()\n        self._child_conn.send((e, tb))\n</code></pre>"},{"location":"reference/core/utils/multiprocessing/#eva.core.utils.multiprocessing.Process.check_exceptions","title":"<code>check_exceptions</code>","text":"<p>Check for exception propagate it to the parent process.</p> Source code in <code>src/eva/core/utils/multiprocessing.py</code> <pre><code>def check_exceptions(self) -&gt; None:\n    \"\"\"Check for exception propagate it to the parent process.\"\"\"\n    if not self.is_alive():\n        if self.exception:\n            error, traceback = self.exception\n            sys.stderr.write(traceback + \"\\n\")\n            raise error\n</code></pre>"},{"location":"reference/core/utils/workers/","title":"Workers","text":"<p>Reference information for the utils <code>Workers</code> API.</p>"},{"location":"reference/core/utils/workers/#eva.core.utils.workers.main_worker_only","title":"<code>eva.core.utils.workers.main_worker_only</code>","text":"<p>Function decorator which will execute it only on main / worker process.</p> Source code in <code>src/eva/core/utils/workers.py</code> <pre><code>def main_worker_only(func: Callable) -&gt; Any:\n    \"\"\"Function decorator which will execute it only on main / worker process.\"\"\"\n\n    def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Wrapper function for the decorated method.\"\"\"\n        if is_main_worker():\n            return func(*args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"reference/core/utils/workers/#eva.core.utils.workers.is_main_worker","title":"<code>eva.core.utils.workers.is_main_worker</code>","text":"<p>Returns whether the main process / worker is currently used.</p> Source code in <code>src/eva/core/utils/workers.py</code> <pre><code>def is_main_worker() -&gt; bool:\n    \"\"\"Returns whether the main process / worker is currently used.\"\"\"\n    process = multiprocessing.current_process()\n    return process.name == \"MainProcess\"\n</code></pre>"},{"location":"reference/vision/","title":"Vision","text":"<p>Reference information for the <code>Vision</code> API.</p> <p>If you have not already installed the <code>Vision</code>-package, install it with: <pre><code>pip install 'kaiko-eva[vision]'\n</code></pre></p>"},{"location":"reference/vision/data/","title":"Vision Data","text":"<p>Reference information for the <code>Vision Data</code> API.</p>"},{"location":"reference/vision/data/datasets/","title":"Datasets","text":""},{"location":"reference/vision/data/datasets/#visiondataset","title":"VisionDataset","text":""},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.VisionDataset","title":"<code>eva.vision.data.datasets.VisionDataset</code>","text":"<p>               Bases: <code>MapDataset</code>, <code>ABC</code>, <code>Generic[DataSample]</code></p> <p>Base dataset class for vision tasks.</p>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.VisionDataset.filename","title":"<code>filename</code>  <code>abstractmethod</code>","text":"<p>Returns the filename of the <code>index</code>'th data sample.</p> <p>Note that this is the relative file path to the root.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the data-sample to select.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The filename of the <code>index</code>'th data sample.</p> Source code in <code>src/eva/vision/data/datasets/vision.py</code> <pre><code>@abc.abstractmethod\ndef filename(self, index: int) -&gt; str:\n    \"\"\"Returns the filename of the `index`'th data sample.\n\n    Note that this is the relative file path to the root.\n\n    Args:\n        index: The index of the data-sample to select.\n\n    Returns:\n        The filename of the `index`'th data sample.\n    \"\"\"\n</code></pre>"},{"location":"reference/vision/data/datasets/#classification-datasets","title":"Classification datasets","text":""},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.BACH","title":"<code>eva.vision.data.datasets.BACH</code>","text":"<p>               Bases: <code>ImageClassification</code></p> <p>Dataset class for BACH images and corresponding targets.</p> <p>The dataset is split into train and validation by taking into account the patient IDs to avoid any data leakage.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Path to the root directory of the dataset. The dataset will be downloaded and extracted here, if it does not already exist.</p> required <code>split</code> <code>Literal['train', 'val'] | None</code> <p>Dataset split to use. If <code>None</code>, the entire dataset is used.</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download the data for the specified split. Note that the download will be executed only by additionally calling the :meth:<code>prepare_data</code> method and if the data does not yet exist on disk.</p> <code>False</code> <code>transforms</code> <code>Callable | None</code> <p>A function/transform which returns a transformed version of the raw data samples.</p> <code>None</code> Source code in <code>src/eva/vision/data/datasets/classification/bach.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    split: Literal[\"train\", \"val\"] | None = None,\n    download: bool = False,\n    transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    The dataset is split into train and validation by taking into account\n    the patient IDs to avoid any data leakage.\n\n    Args:\n        root: Path to the root directory of the dataset. The dataset will\n            be downloaded and extracted here, if it does not already exist.\n        split: Dataset split to use. If `None`, the entire dataset is used.\n        download: Whether to download the data for the specified split.\n            Note that the download will be executed only by additionally\n            calling the :meth:`prepare_data` method and if the data does\n            not yet exist on disk.\n        transforms: A function/transform which returns a transformed\n            version of the raw data samples.\n    \"\"\"\n    super().__init__(transforms=transforms)\n\n    self._root = root\n    self._split = split\n    self._download = download\n\n    self._samples: List[Tuple[str, int]] = []\n    self._indices: List[int] = []\n</code></pre>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.PatchCamelyon","title":"<code>eva.vision.data.datasets.PatchCamelyon</code>","text":"<p>               Bases: <code>ImageClassification</code></p> <p>Dataset class for PatchCamelyon images and corresponding targets.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>The path to the dataset root. This path should contain the uncompressed h5 files and the metadata.</p> required <code>split</code> <code>Literal['train', 'val', 'test']</code> <p>The dataset split for training, validation, or testing.</p> required <code>download</code> <code>bool</code> <p>Whether to download the data for the specified split. Note that the download will be executed only by additionally calling the :meth:<code>prepare_data</code> method.</p> <code>False</code> <code>transforms</code> <code>Callable | None</code> <p>A function/transform which returns a transformed version of the raw data samples.</p> <code>None</code> Source code in <code>src/eva/vision/data/datasets/classification/patch_camelyon.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    split: Literal[\"train\", \"val\", \"test\"],\n    download: bool = False,\n    transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initializes the dataset.\n\n    Args:\n        root: The path to the dataset root. This path should contain\n            the uncompressed h5 files and the metadata.\n        split: The dataset split for training, validation, or testing.\n        download: Whether to download the data for the specified split.\n            Note that the download will be executed only by additionally\n            calling the :meth:`prepare_data` method.\n        transforms: A function/transform which returns a transformed\n            version of the raw data samples.\n    \"\"\"\n    super().__init__(transforms=transforms)\n\n    self._root = root\n    self._split = split\n    self._download = download\n</code></pre>"},{"location":"reference/vision/data/datasets/#segmentation-datasets","title":"Segmentation datasets","text":""},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.ImageSegmentation","title":"<code>eva.vision.data.datasets.ImageSegmentation</code>","text":"<p>               Bases: <code>VisionDataset[Tuple[Image, Mask]]</code>, <code>ABC</code></p> <p>Image segmentation abstract dataset.</p> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>Callable | None</code> <p>A function/transforms that takes in an image and a label and returns the transformed versions of both.</p> <code>None</code> Source code in <code>src/eva/vision/data/datasets/segmentation/base.py</code> <pre><code>def __init__(self, transforms: Callable | None = None) -&gt; None:\n    \"\"\"Initializes the image segmentation base class.\n\n    Args:\n        transforms: A function/transforms that takes in an\n            image and a label and returns the transformed versions of both.\n    \"\"\"\n    super().__init__()\n\n    self._transforms = transforms\n</code></pre>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.ImageSegmentation.classes","title":"<code>classes: List[str] | None</code>  <code>property</code>","text":"<p>Returns the list with names of the dataset names.</p>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.ImageSegmentation.class_to_idx","title":"<code>class_to_idx: Dict[str, int] | None</code>  <code>property</code>","text":"<p>Returns a mapping of the class name to its target index.</p>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.ImageSegmentation.load_image","title":"<code>load_image</code>  <code>abstractmethod</code>","text":"<p>Loads and returns the <code>index</code>'th image sample.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the data sample to load.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>An image torchvision tensor (channels, height, width).</p> Source code in <code>src/eva/vision/data/datasets/segmentation/base.py</code> <pre><code>@abc.abstractmethod\ndef load_image(self, index: int) -&gt; tv_tensors.Image:\n    \"\"\"Loads and returns the `index`'th image sample.\n\n    Args:\n        index: The index of the data sample to load.\n\n    Returns:\n        An image torchvision tensor (channels, height, width).\n    \"\"\"\n</code></pre>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.ImageSegmentation.load_mask","title":"<code>load_mask</code>  <code>abstractmethod</code>","text":"<p>Returns the <code>index</code>'th target masks sample.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the data sample target masks to load.</p> required <p>Returns:</p> Type Description <code>Mask</code> <p>The semantic mask as a (H x W) shaped tensor with integer</p> <code>Mask</code> <p>values which represent the pixel class id.</p> Source code in <code>src/eva/vision/data/datasets/segmentation/base.py</code> <pre><code>@abc.abstractmethod\ndef load_mask(self, index: int) -&gt; tv_tensors.Mask:\n    \"\"\"Returns the `index`'th target masks sample.\n\n    Args:\n        index: The index of the data sample target masks to load.\n\n    Returns:\n        The semantic mask as a (H x W) shaped tensor with integer\n        values which represent the pixel class id.\n    \"\"\"\n</code></pre>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.ImageSegmentation.load_metadata","title":"<code>load_metadata</code>","text":"<p>Returns the dataset metadata.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the data sample to return the metadata of. If <code>None</code>, it will return the metadata of the current dataset.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any] | None</code> <p>The sample metadata.</p> Source code in <code>src/eva/vision/data/datasets/segmentation/base.py</code> <pre><code>def load_metadata(self, index: int) -&gt; Dict[str, Any] | None:\n    \"\"\"Returns the dataset metadata.\n\n    Args:\n        index: The index of the data sample to return the metadata of.\n            If `None`, it will return the metadata of the current dataset.\n\n    Returns:\n        The sample metadata.\n    \"\"\"\n</code></pre>"},{"location":"reference/vision/data/datasets/#eva.vision.data.datasets.TotalSegmentator2D","title":"<code>eva.vision.data.datasets.TotalSegmentator2D</code>","text":"<p>               Bases: <code>ImageSegmentation</code></p> <p>TotalSegmentator 2D segmentation dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Path to the root directory of the dataset. The dataset will be downloaded and extracted here, if it does not already exist.</p> required <code>split</code> <code>Literal['train', 'val', 'test'] | None</code> <p>Dataset split to use. If <code>None</code>, the entire dataset is used.</p> required <code>version</code> <code>Literal['small', 'full'] | None</code> <p>The version of the dataset to initialize. If <code>None</code>, it will use the files located at root as is and wont perform any checks.</p> <code>'full'</code> <code>download</code> <code>bool</code> <p>Whether to download the data for the specified split. Note that the download will be executed only by additionally calling the :meth:<code>prepare_data</code> method and if the data does not exist yet on disk.</p> <code>False</code> <code>classes</code> <code>List[str] | None</code> <p>Whether to configure the dataset with a subset of classes. If <code>None</code>, it will use all of them.</p> <code>None</code> <code>class_mappings</code> <code>Dict[str, str] | None</code> <p>A dictionary that maps the original class names to a reduced set of classes. If <code>None</code>, it will use the original classes.</p> <code>reduced_class_mappings</code> <code>optimize_mask_loading</code> <code>bool</code> <p>Whether to pre-process the segmentation masks in order to optimize the loading time. In the <code>setup</code> method, it will reformat the binary one-hot masks to a semantic mask and store it on disk.</p> <code>True</code> <code>decompress</code> <code>bool</code> <p>Whether to decompress the ct.nii.gz files when preparing the data. The label masks won't be decompressed, but when enabling optimize_mask_loading it will export the semantic label masks to a single file in uncompressed .nii format.</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for optimizing the masks &amp; decompressing the .gz files.</p> <code>10</code> <code>transforms</code> <code>Callable | None</code> <p>A function/transforms that takes in an image and a target mask and returns the transformed versions of both.</p> <code>None</code> Source code in <code>src/eva/vision/data/datasets/segmentation/total_segmentator_2d.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    split: Literal[\"train\", \"val\", \"test\"] | None,\n    version: Literal[\"small\", \"full\"] | None = \"full\",\n    download: bool = False,\n    classes: List[str] | None = None,\n    class_mappings: Dict[str, str] | None = _total_segmentator.reduced_class_mappings,\n    optimize_mask_loading: bool = True,\n    decompress: bool = True,\n    num_workers: int = 10,\n    transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initialize dataset.\n\n    Args:\n        root: Path to the root directory of the dataset. The dataset will\n            be downloaded and extracted here, if it does not already exist.\n        split: Dataset split to use. If `None`, the entire dataset is used.\n        version: The version of the dataset to initialize. If `None`, it will\n            use the files located at root as is and wont perform any checks.\n        download: Whether to download the data for the specified split.\n            Note that the download will be executed only by additionally\n            calling the :meth:`prepare_data` method and if the data does not\n            exist yet on disk.\n        classes: Whether to configure the dataset with a subset of classes.\n            If `None`, it will use all of them.\n        class_mappings: A dictionary that maps the original class names to a\n            reduced set of classes. If `None`, it will use the original classes.\n        optimize_mask_loading: Whether to pre-process the segmentation masks\n            in order to optimize the loading time. In the `setup` method, it\n            will reformat the binary one-hot masks to a semantic mask and store\n            it on disk.\n        decompress: Whether to decompress the ct.nii.gz files when preparing the data.\n            The label masks won't be decompressed, but when enabling optimize_mask_loading\n            it will export the semantic label masks to a single file in uncompressed .nii\n            format.\n        num_workers: The number of workers to use for optimizing the masks &amp;\n            decompressing the .gz files.\n        transforms: A function/transforms that takes in an image and a target\n            mask and returns the transformed versions of both.\n\n    \"\"\"\n    super().__init__(transforms=transforms)\n\n    self._root = root\n    self._split = split\n    self._version = version\n    self._download = download\n    self._classes = classes\n    self._optimize_mask_loading = optimize_mask_loading\n    self._decompress = decompress\n    self._num_workers = num_workers\n    self._class_mappings = class_mappings\n\n    if self._classes and self._class_mappings:\n        raise ValueError(\"Both 'classes' and 'class_mappings' cannot be set at the same time.\")\n\n    self._samples_dirs: List[str] = []\n    self._indices: List[Tuple[int, int]] = []\n</code></pre>"},{"location":"reference/vision/data/transforms/","title":"Transforms","text":""},{"location":"reference/vision/data/transforms/#eva.core.data.transforms.dtype.ArrayToTensor","title":"<code>eva.core.data.transforms.dtype.ArrayToTensor</code>","text":"<p>Converts a numpy array to a torch tensor.</p>"},{"location":"reference/vision/data/transforms/#eva.core.data.transforms.dtype.ArrayToFloatTensor","title":"<code>eva.core.data.transforms.dtype.ArrayToFloatTensor</code>","text":"<p>               Bases: <code>ArrayToTensor</code></p> <p>Converts a numpy array to a torch tensor and casts it to float.</p>"},{"location":"reference/vision/data/transforms/#eva.vision.data.transforms.ResizeAndCrop","title":"<code>eva.vision.data.transforms.ResizeAndCrop</code>","text":"<p>               Bases: <code>Compose</code></p> <p>Resizes, crops and normalizes an input image while preserving its aspect ratio.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int | Sequence[int]</code> <p>Desired output size of the crop. If size is an <code>int</code> instead of sequence like (h, w), a square crop (size, size) is made.</p> <code>224</code> <code>mean</code> <code>Sequence[float]</code> <p>Sequence of means for each image channel.</p> <code>(0.5, 0.5, 0.5)</code> <code>std</code> <code>Sequence[float]</code> <p>Sequence of standard deviations for each image channel.</p> <code>(0.5, 0.5, 0.5)</code> Source code in <code>src/eva/vision/data/transforms/common/resize_and_crop.py</code> <pre><code>def __init__(\n    self,\n    size: int | Sequence[int] = 224,\n    mean: Sequence[float] = (0.5, 0.5, 0.5),\n    std: Sequence[float] = (0.5, 0.5, 0.5),\n) -&gt; None:\n    \"\"\"Initializes the transform object.\n\n    Args:\n        size: Desired output size of the crop. If size is an `int` instead\n            of sequence like (h, w), a square crop (size, size) is made.\n        mean: Sequence of means for each image channel.\n        std: Sequence of standard deviations for each image channel.\n    \"\"\"\n    self._size = size\n    self._mean = mean\n    self._std = std\n\n    super().__init__(transforms=self._build_transforms())\n</code></pre>"},{"location":"reference/vision/models/networks/","title":"Networks","text":"<p>Reference information for the vision model <code>Networks</code> API.</p>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.ABMIL","title":"<code>eva.vision.models.networks.ABMIL</code>","text":"<p>               Bases: <code>Module</code></p> <p>ABMIL network for multiple instance learning classification tasks.</p> <p>Takes an array of patch level embeddings per slide as input. This implementation supports batched inputs of shape (<code>batch_size</code>, <code>n_instances</code>, <code>input_size</code>). For slides with less than <code>n_instances</code> patches, you can apply padding and provide a mask tensor to the forward pass.</p> <p>The original implementation from [1] was used as a reference: https://github.com/AMLab-Amsterdam/AttentionDeepMIL/blob/master/model.py</p> Notes <ul> <li>use_bias: The paper didn't use bias in their formalism, but their published example code inadvertently does.</li> <li>To prevent dot product similarities near-equal due to concentration of measure as a consequence of large input embedding dimensionality (&gt;128), we added the option to project the input embeddings to a lower dimensionality</li> </ul> <p>[1] Maximilian Ilse, Jakub M. Tomczak, Max Welling, \"Attention-based Deep Multiple     Instance Learning\", 2018     https://arxiv.org/abs/1802.04712</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>input embedding dimension</p> required <code>output_size</code> <code>int</code> <p>number of classes</p> required <code>projected_input_size</code> <code>int | None</code> <p>size of the projected input. if <code>None</code>, no projection is performed.</p> required <code>hidden_size_attention</code> <code>int</code> <p>hidden dimension in attention network</p> <code>128</code> <code>hidden_sizes_mlp</code> <code>tuple</code> <p>dimensions for hidden layers in last mlp</p> <code>(128, 64)</code> <code>use_bias</code> <code>bool</code> <p>whether to use bias in the attention network</p> <code>True</code> <code>dropout_input_embeddings</code> <code>float</code> <p>dropout rate for the input embeddings</p> <code>0.0</code> <code>dropout_attention</code> <code>float</code> <p>dropout rate for the attention network and classifier</p> <code>0.0</code> <code>dropout_mlp</code> <code>float</code> <p>dropout rate for the final MLP network</p> <code>0.0</code> <code>pad_value</code> <code>int | float | None</code> <p>Value indicating padding in the input tensor. If specified, entries with this value in the will be masked. If set to <code>None</code>, no masking is applied.</p> <code>float('-inf')</code> Source code in <code>src/eva/vision/models/networks/abmil.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    output_size: int,\n    projected_input_size: int | None,\n    hidden_size_attention: int = 128,\n    hidden_sizes_mlp: tuple = (128, 64),\n    use_bias: bool = True,\n    dropout_input_embeddings: float = 0.0,\n    dropout_attention: float = 0.0,\n    dropout_mlp: float = 0.0,\n    pad_value: int | float | None = float(\"-inf\"),\n) -&gt; None:\n    \"\"\"Initializes the ABMIL network.\n\n    Args:\n        input_size: input embedding dimension\n        output_size: number of classes\n        projected_input_size: size of the projected input. if `None`, no projection is\n            performed.\n        hidden_size_attention: hidden dimension in attention network\n        hidden_sizes_mlp: dimensions for hidden layers in last mlp\n        use_bias: whether to use bias in the attention network\n        dropout_input_embeddings: dropout rate for the input embeddings\n        dropout_attention: dropout rate for the attention network and classifier\n        dropout_mlp: dropout rate for the final MLP network\n        pad_value: Value indicating padding in the input tensor. If specified, entries with\n            this value in the will be masked. If set to `None`, no masking is applied.\n    \"\"\"\n    super().__init__()\n\n    self._pad_value = pad_value\n\n    if projected_input_size:\n        self.projector = nn.Sequential(\n            nn.Linear(input_size, projected_input_size, bias=True),\n            nn.Dropout(p=dropout_input_embeddings),\n        )\n        input_size = projected_input_size\n    else:\n        self.projector = nn.Dropout(p=dropout_input_embeddings)\n\n    self.gated_attention = GatedAttention(\n        input_dim=input_size,\n        hidden_dim=hidden_size_attention,\n        dropout=dropout_attention,\n        n_classes=1,\n        use_bias=use_bias,\n    )\n\n    self.classifier = MLP(\n        input_size=input_size,\n        output_size=output_size,\n        hidden_layer_sizes=hidden_sizes_mlp,\n        dropout=dropout_mlp,\n        hidden_activation_fn=nn.ReLU,\n    )\n</code></pre>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.ABMIL.forward","title":"<code>forward</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Tensor with expected shape of (batch_size, n_instances, input_size).</p> required Source code in <code>src/eva/vision/models/networks/abmil.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Args:\n        input_tensor: Tensor with expected shape of (batch_size, n_instances, input_size).\n    \"\"\"\n    input_tensor, mask = self._mask_values(input_tensor, self._pad_value)\n\n    # (batch_size, n_instances, input_size) -&gt; (batch_size, n_instances, projected_input_size)\n    input_tensor = self.projector(input_tensor)\n\n    attention_logits = self.gated_attention(input_tensor)  # (batch_size, n_instances, 1)\n    if mask is not None:\n        # fill masked values with -inf, which will yield 0s after softmax\n        attention_logits = attention_logits.masked_fill(mask, float(\"-inf\"))\n\n    attention_weights = nn.functional.softmax(attention_logits, dim=1)\n    # (batch_size, n_instances, 1)\n\n    attention_result = torch.matmul(torch.transpose(attention_weights, 1, 2), input_tensor)\n    # (batch_size, 1, hidden_size_attention)\n\n    attention_result = torch.squeeze(attention_result, 1)  # (batch_size, hidden_size_attention)\n\n    return self.classifier(attention_result)  # (batch_size, output_size)\n</code></pre>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.decoders.Decoder","title":"<code>eva.vision.models.networks.decoders.Decoder</code>","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for segmentation decoders.</p>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.decoders.Decoder.forward","title":"<code>forward</code>  <code>abstractmethod</code>","text":"<p>Forward pass of the decoder.</p> Source code in <code>src/eva/vision/models/networks/decoders/segmentation/base.py</code> <pre><code>@abc.abstractmethod\ndef forward(self, decoder_inputs: DecoderInputs) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the decoder.\"\"\"\n</code></pre>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.decoders.segmentation.Decoder2D","title":"<code>eva.vision.models.networks.decoders.segmentation.Decoder2D</code>","text":"<p>               Bases: <code>Decoder</code></p> <p>Segmentation decoder for 2D applications.</p> <p>Here the input nn layers will be directly applied to the features of shape (batch_size, hidden_size, n_patches_height, n_patches_width), where n_patches is image_size / patch_size. Note the n_patches is also known as grid_size.</p> <p>Parameters:</p> Name Type Description Default <code>layers</code> <code>Module</code> <p>The layers to be used as the decoder head.</p> required <code>combine_features</code> <code>bool</code> <p>Whether to combine the features from different feature levels into one tensor before applying the decoder head.</p> <code>True</code> Source code in <code>src/eva/vision/models/networks/decoders/segmentation/decoder2d.py</code> <pre><code>def __init__(self, layers: nn.Module, combine_features: bool = True) -&gt; None:\n    \"\"\"Initializes the based decoder head.\n\n    Here the input nn layers will be directly applied to the\n    features of shape (batch_size, hidden_size, n_patches_height,\n    n_patches_width), where n_patches is image_size / patch_size.\n    Note the n_patches is also known as grid_size.\n\n    Args:\n        layers: The layers to be used as the decoder head.\n        combine_features: Whether to combine the features from different\n            feature levels into one tensor before applying the decoder head.\n    \"\"\"\n    super().__init__()\n\n    self._layers = layers\n    self._combine_features = combine_features\n</code></pre>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.decoders.segmentation.Decoder2D.forward","title":"<code>forward</code>","text":"<p>Maps the patch embeddings to a segmentation mask of the image size.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_inputs</code> <code>DecoderInputs</code> <p>Inputs required by the decoder.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor containing scores for all of the classes with shape</p> <code>Tensor</code> <p>(batch_size, n_classes, image_height, image_width).</p> Source code in <code>src/eva/vision/models/networks/decoders/segmentation/decoder2d.py</code> <pre><code>def forward(self, decoder_inputs: DecoderInputs) -&gt; torch.Tensor:\n    \"\"\"Maps the patch embeddings to a segmentation mask of the image size.\n\n    Args:\n        decoder_inputs: Inputs required by the decoder.\n\n    Returns:\n        Tensor containing scores for all of the classes with shape\n        (batch_size, n_classes, image_height, image_width).\n    \"\"\"\n    features, image_size, _ = DecoderInputs(*decoder_inputs)\n    if self._combine_features:\n        features = self._forward_features(features)\n    logits = self._forward_head(features)\n    return self._upscale(logits, image_size)\n</code></pre>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.decoders.segmentation.ConvDecoder1x1","title":"<code>eva.vision.models.networks.decoders.segmentation.ConvDecoder1x1</code>","text":"<p>               Bases: <code>Decoder2D</code></p> <p>A convolutional decoder with a single 1x1 convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>The hidden dimension size of the embeddings.</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes as channels.</p> required Source code in <code>src/eva/vision/models/networks/decoders/segmentation/semantic/common.py</code> <pre><code>def __init__(self, in_features: int, num_classes: int) -&gt; None:\n    \"\"\"Initializes the decoder.\n\n    Args:\n        in_features: The hidden dimension size of the embeddings.\n        num_classes: Number of output classes as channels.\n    \"\"\"\n    super().__init__(\n        layers=nn.Conv2d(\n            in_channels=in_features,\n            out_channels=num_classes,\n            kernel_size=(1, 1),\n        ),\n    )\n</code></pre>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.decoders.segmentation.ConvDecoderMS","title":"<code>eva.vision.models.networks.decoders.segmentation.ConvDecoderMS</code>","text":"<p>               Bases: <code>Decoder2D</code></p> <p>A multi-stage convolutional decoder with upsampling and convolutional layers.</p> <p>This decoder applies a series of upsampling and convolutional layers to transform the input features into output predictions with the desired spatial resolution.</p> <p>This decoder is based on the <code>+ms</code> segmentation decoder from DINOv2 (https://arxiv.org/pdf/2304.07193)</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>The hidden dimension size of the embeddings.</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes as channels.</p> required Source code in <code>src/eva/vision/models/networks/decoders/segmentation/semantic/common.py</code> <pre><code>def __init__(self, in_features: int, num_classes: int) -&gt; None:\n    \"\"\"Initializes the decoder.\n\n    Args:\n        in_features: The hidden dimension size of the embeddings.\n        num_classes: Number of output classes as channels.\n    \"\"\"\n    super().__init__(\n        layers=nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_features, 64, kernel_size=(3, 3), padding=(1, 1)),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(64, num_classes, kernel_size=(3, 3), padding=(1, 1)),\n        ),\n    )\n</code></pre>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.decoders.segmentation.LinearDecoder","title":"<code>eva.vision.models.networks.decoders.segmentation.LinearDecoder</code>","text":"<p>               Bases: <code>Decoder</code></p> <p>Linear decoder.</p> <p>Here the input nn layers will be applied to the reshaped features (batch_size, patch_embeddings, hidden_size) from the input (batch_size, hidden_size, height, width) and then unwrapped again to (batch_size, n_classes, height, width).</p> <p>Parameters:</p> Name Type Description Default <code>layers</code> <code>Module</code> <p>The linear layers to be used as the decoder head.</p> required Source code in <code>src/eva/vision/models/networks/decoders/segmentation/linear.py</code> <pre><code>def __init__(self, layers: nn.Module) -&gt; None:\n    \"\"\"Initializes the linear based decoder head.\n\n    Here the input nn layers will be applied to the reshaped\n    features (batch_size, patch_embeddings, hidden_size) from\n    the input (batch_size, hidden_size, height, width) and then\n    unwrapped again to (batch_size, n_classes, height, width).\n\n    Args:\n        layers: The linear layers to be used as the decoder head.\n    \"\"\"\n    super().__init__()\n\n    self._layers = layers\n</code></pre>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.decoders.segmentation.LinearDecoder.forward","title":"<code>forward</code>","text":"<p>Maps the patch embeddings to a segmentation mask of the image size.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>List[Tensor]</code> <p>List of multi-level image features of shape (batch_size, hidden_size, n_patches_height, n_patches_width).</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>The target image size (height, width).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor containing scores for all of the classes with shape</p> <code>Tensor</code> <p>(batch_size, n_classes, image_height, image_width).</p> Source code in <code>src/eva/vision/models/networks/decoders/segmentation/linear.py</code> <pre><code>def forward(\n    self,\n    features: List[torch.Tensor],\n    image_size: Tuple[int, int],\n) -&gt; torch.Tensor:\n    \"\"\"Maps the patch embeddings to a segmentation mask of the image size.\n\n    Args:\n        features: List of multi-level image features of shape (batch_size,\n            hidden_size, n_patches_height, n_patches_width).\n        image_size: The target image size (height, width).\n\n    Returns:\n        Tensor containing scores for all of the classes with shape\n        (batch_size, n_classes, image_height, image_width).\n    \"\"\"\n    patch_embeddings = self._forward_features(features)\n    logits = self._forward_head(patch_embeddings)\n    return self._cls_seg(logits, image_size)\n</code></pre>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.decoders.segmentation.SingleLinearDecoder","title":"<code>eva.vision.models.networks.decoders.segmentation.SingleLinearDecoder</code>","text":"<p>               Bases: <code>LinearDecoder</code></p> <p>A simple linear decoder with a single fully connected layer.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>The hidden dimension size of the embeddings.</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes as channels.</p> required Source code in <code>src/eva/vision/models/networks/decoders/segmentation/semantic/common.py</code> <pre><code>def __init__(self, in_features: int, num_classes: int) -&gt; None:\n    \"\"\"Initializes the decoder.\n\n    Args:\n        in_features: The hidden dimension size of the embeddings.\n        num_classes: Number of output classes as channels.\n    \"\"\"\n    super().__init__(\n        layers=nn.Linear(\n            in_features=in_features,\n            out_features=num_classes,\n        ),\n    )\n</code></pre>"},{"location":"reference/vision/models/networks/#eva.vision.models.networks.decoders.segmentation.ConvDecoderWithImage","title":"<code>eva.vision.models.networks.decoders.segmentation.ConvDecoderWithImage</code>","text":"<p>               Bases: <code>Decoder2D</code></p> <p>A convolutional that in addition to encoded features, also takes the input image as input.</p> <p>In a first stage, the input features are upsampled and passed through a convolutional layer, while in the second stage, the input image channels are concatenated with the upsampled features and passed through additional convolutional blocks in order to combine the image prior information with the encoded features. Lastly, a 1x1 conv operation reduces the number of channels to the number of classes.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>The hidden dimension size of the embeddings.</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes as channels.</p> required <code>greyscale</code> <code>bool</code> <p>Whether to convert input images to greyscale.</p> <code>False</code> <code>hidden_dims</code> <code>List[int] | None</code> <p>List of hidden dimensions for the convolutional layers.</p> <code>None</code> Source code in <code>src/eva/vision/models/networks/decoders/segmentation/semantic/with_image.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    num_classes: int,\n    greyscale: bool = False,\n    hidden_dims: List[int] | None = None,\n) -&gt; None:\n    \"\"\"Initializes the decoder.\n\n    Args:\n        in_features: The hidden dimension size of the embeddings.\n        num_classes: Number of output classes as channels.\n        greyscale: Whether to convert input images to greyscale.\n        hidden_dims: List of hidden dimensions for the convolutional layers.\n    \"\"\"\n    hidden_dims = hidden_dims or self._default_hidden_dims\n    if len(hidden_dims) != 3:\n        raise ValueError(\"Hidden dims must have 3 elements.\")\n\n    super().__init__(\n        layers=nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            Conv2dBnReLU(in_features, hidden_dims[0]),\n        )\n    )\n    self.greyscale = greyscale\n\n    additional_hidden_dims = 1 if greyscale else 3\n    self.image_block = nn.Sequential(\n        Conv2dBnReLU(hidden_dims[0] + additional_hidden_dims, hidden_dims[1]),\n        Conv2dBnReLU(hidden_dims[1], hidden_dims[2]),\n    )\n\n    self.classifier = nn.Conv2d(hidden_dims[2], num_classes, kernel_size=1)\n</code></pre>"},{"location":"reference/vision/models/wrappers/","title":"Wrappers","text":"<p>Reference information for the model <code>Wrappers</code> API.</p>"},{"location":"reference/vision/models/wrappers/#eva.vision.models.wrappers.TimmModel","title":"<code>eva.vision.models.wrappers.TimmModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model wrapper for <code>timm</code> models.</p> <p>Note that only models with <code>forward_intermediates</code> method are currently supported.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of model to instantiate.</p> required <code>pretrained</code> <code>bool</code> <p>If set to <code>True</code>, load pretrained ImageNet-1k weights.</p> <code>True</code> <code>checkpoint_path</code> <code>str</code> <p>Path of checkpoint to load.</p> <code>''</code> <code>out_indices</code> <code>int | Tuple[int, ...] | None</code> <p>Returns last n blocks if <code>int</code>, all if <code>None</code>, select matching indices if sequence.</p> <code>None</code> <code>model_kwargs</code> <code>Dict[str, Any] | None</code> <p>Extra model arguments.</p> <code>None</code> <code>tensor_transforms</code> <code>Callable | None</code> <p>The transforms to apply to the output tensor produced by the model.</p> <code>None</code> Source code in <code>src/eva/vision/models/wrappers/from_timm.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    pretrained: bool = True,\n    checkpoint_path: str = \"\",\n    out_indices: int | Tuple[int, ...] | None = None,\n    model_kwargs: Dict[str, Any] | None = None,\n    tensor_transforms: Callable | None = None,\n) -&gt; None:\n    \"\"\"Initializes the encoder.\n\n    Args:\n        model_name: Name of model to instantiate.\n        pretrained: If set to `True`, load pretrained ImageNet-1k weights.\n        checkpoint_path: Path of checkpoint to load.\n        out_indices: Returns last n blocks if `int`, all if `None`, select\n            matching indices if sequence.\n        model_kwargs: Extra model arguments.\n        tensor_transforms: The transforms to apply to the output tensor\n            produced by the model.\n    \"\"\"\n    super().__init__(tensor_transforms=tensor_transforms)\n\n    self._model_name = model_name\n    self._pretrained = pretrained\n    self._checkpoint_path = checkpoint_path\n    self._out_indices = out_indices\n    self._model_kwargs = model_kwargs or {}\n\n    self.load_model()\n</code></pre>"},{"location":"reference/vision/models/wrappers/#eva.vision.models.wrappers.TimmModel.load_model","title":"<code>load_model</code>","text":"<p>Builds and loads the timm model as feature extractor.</p> Source code in <code>src/eva/vision/models/wrappers/from_timm.py</code> <pre><code>@override\ndef load_model(self) -&gt; None:\n    \"\"\"Builds and loads the timm model as feature extractor.\"\"\"\n    self._model = timm.create_model(\n        model_name=self._model_name,\n        pretrained=True if self._checkpoint_path else self._pretrained,\n        pretrained_cfg=self._pretrained_cfg,\n        out_indices=self._out_indices,\n        features_only=self._out_indices is not None,\n        **self._model_kwargs,\n    )\n    TimmModel.__name__ = self._model_name\n</code></pre>"},{"location":"reference/vision/utils/io/","title":"IO","text":""},{"location":"reference/vision/utils/io/#eva.vision.utils.io.image","title":"<code>eva.vision.utils.io.image</code>","text":"<p>Image I/O related functions.</p>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.image.read_image","title":"<code>read_image</code>","text":"<p>Reads and loads the image from a file path as a RGB.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the image file.</p> required <p>Returns:</p> Type Description <code>NDArray[uint8]</code> <p>The RGB image as a numpy array (HxWxC).</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>IOError</code> <p>If the image could not be loaded.</p> Source code in <code>src/eva/vision/utils/io/image.py</code> <pre><code>def read_image(path: str) -&gt; npt.NDArray[np.uint8]:\n    \"\"\"Reads and loads the image from a file path as a RGB.\n\n    Args:\n        path: The path of the image file.\n\n    Returns:\n        The RGB image as a numpy array (HxWxC).\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        IOError: If the image could not be loaded.\n    \"\"\"\n    return read_image_as_array(path, cv2.IMREAD_COLOR)\n</code></pre>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.image.read_image_as_tensor","title":"<code>read_image_as_tensor</code>","text":"<p>Reads and loads the image from a file path as a RGB torch tensor.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the image file.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>The RGB image as a torch tensor (CxHxW).</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>IOError</code> <p>If the image could not be loaded.</p> Source code in <code>src/eva/vision/utils/io/image.py</code> <pre><code>def read_image_as_tensor(path: str) -&gt; tv_tensors.Image:\n    \"\"\"Reads and loads the image from a file path as a RGB torch tensor.\n\n    Args:\n        path: The path of the image file.\n\n    Returns:\n        The RGB image as a torch tensor (CxHxW).\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        IOError: If the image could not be loaded.\n    \"\"\"\n    image_array = read_image(path)\n    return functional.to_image(image_array)\n</code></pre>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.image.read_image_as_array","title":"<code>read_image_as_array</code>","text":"<p>Reads and loads an image file as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the image file.</p> required <code>flags</code> <code>int</code> <p>Specifies the way in which the image should be read.</p> <code>IMREAD_UNCHANGED</code> <p>Returns:</p> Type Description <code>NDArray[uint8]</code> <p>The image as a numpy array.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>IOError</code> <p>If the image could not be loaded.</p> Source code in <code>src/eva/vision/utils/io/image.py</code> <pre><code>def read_image_as_array(path: str, flags: int = cv2.IMREAD_UNCHANGED) -&gt; npt.NDArray[np.uint8]:\n    \"\"\"Reads and loads an image file as a numpy array.\n\n    Args:\n        path: The path to the image file.\n        flags: Specifies the way in which the image should be read.\n\n    Returns:\n        The image as a numpy array.\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        IOError: If the image could not be loaded.\n    \"\"\"\n    _utils.check_file(path)\n    image = cv2.imread(path, flags=flags)\n    if image is None:\n        raise IOError(\n            f\"Input '{path}' could not be loaded. \"\n            \"Please verify that the path is a valid image file.\"\n        )\n\n    if image.ndim == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    if image.ndim == 2 and flags == cv2.IMREAD_COLOR:\n        image = image[:, :, np.newaxis]\n\n    return np.asarray(image, dtype=np.uint8)\n</code></pre>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.nifti","title":"<code>eva.vision.utils.io.nifti</code>","text":"<p>NIfTI I/O related functions.</p>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.nifti.read_nifti","title":"<code>read_nifti</code>","text":"<p>Reads and loads a NIfTI image from a file path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the NIfTI file.</p> required <code>slice_index</code> <code>int | None</code> <p>Whether to read only a slice from the file.</p> <code>None</code> <code>use_storage_dtype</code> <code>bool</code> <p>Whether to cast the raw image array to the inferred type.</p> <code>True</code> <p>Returns:</p> Type Description <code>NDArray[Any]</code> <p>The image as a numpy array (height, width, channels).</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>ValueError</code> <p>If the input channel is invalid for the image.</p> Source code in <code>src/eva/vision/utils/io/nifti.py</code> <pre><code>def read_nifti(\n    path: str, slice_index: int | None = None, *, use_storage_dtype: bool = True\n) -&gt; npt.NDArray[Any]:\n    \"\"\"Reads and loads a NIfTI image from a file path.\n\n    Args:\n        path: The path to the NIfTI file.\n        slice_index: Whether to read only a slice from the file.\n        use_storage_dtype: Whether to cast the raw image\n            array to the inferred type.\n\n    Returns:\n        The image as a numpy array (height, width, channels).\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        ValueError: If the input channel is invalid for the image.\n    \"\"\"\n    _utils.check_file(path)\n    image_data: nib.Nifti1Image = nib.load(path)  # type: ignore\n    if slice_index is not None:\n        image_data = image_data.slicer[:, :, slice_index : slice_index + 1]\n\n    image_array = image_data.get_fdata()\n    if use_storage_dtype:\n        image_array = image_array.astype(image_data.get_data_dtype())\n\n    return image_array\n</code></pre>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.nifti.save_array_as_nifti","title":"<code>save_array_as_nifti</code>","text":"<p>Saved a numpy array as a NIfTI image file.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ArrayLike</code> <p>The image array to save.</p> required <code>filename</code> <code>str</code> <p>The name to save the image like.</p> required <code>dtype</code> <code>DTypeLike | None</code> <p>The data type to save the image.</p> <code>int64</code> Source code in <code>src/eva/vision/utils/io/nifti.py</code> <pre><code>def save_array_as_nifti(\n    array: npt.ArrayLike,\n    filename: str,\n    *,\n    dtype: npt.DTypeLike | None = np.int64,\n) -&gt; None:\n    \"\"\"Saved a numpy array as a NIfTI image file.\n\n    Args:\n        array: The image array to save.\n        filename: The name to save the image like.\n        dtype: The data type to save the image.\n    \"\"\"\n    nifti_image = nib.Nifti1Image(array, affine=np.eye(4), dtype=dtype)  # type: ignore\n    nifti_image.to_filename(filename)\n</code></pre>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.nifti.fetch_nifti_shape","title":"<code>fetch_nifti_shape</code>","text":"<p>Fetches the NIfTI image shape from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the NIfTI file.</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>The image shape.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the path does not exist or it is unreachable.</p> <code>ValueError</code> <p>If the input channel is invalid for the image.</p> Source code in <code>src/eva/vision/utils/io/nifti.py</code> <pre><code>def fetch_nifti_shape(path: str) -&gt; Tuple[int]:\n    \"\"\"Fetches the NIfTI image shape from a file.\n\n    Args:\n        path: The path to the NIfTI file.\n\n    Returns:\n        The image shape.\n\n    Raises:\n        FileExistsError: If the path does not exist or it is unreachable.\n        ValueError: If the input channel is invalid for the image.\n    \"\"\"\n    _utils.check_file(path)\n    image = nib.load(path)  # type: ignore\n    return image.header.get_data_shape()  # type: ignore\n</code></pre>"},{"location":"reference/vision/utils/io/#eva.vision.utils.io.nifti.fetch_nifti_axis_direction_code","title":"<code>fetch_nifti_axis_direction_code</code>","text":"<p>Fetches the NIfTI axis direction code from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the NIfTI file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The axis direction codes as string (e.g. \"LAS\").</p> Source code in <code>src/eva/vision/utils/io/nifti.py</code> <pre><code>def fetch_nifti_axis_direction_code(path: str) -&gt; str:\n    \"\"\"Fetches the NIfTI axis direction code from a file.\n\n    Args:\n        path: The path to the NIfTI file.\n\n    Returns:\n        The axis direction codes as string (e.g. \"LAS\").\n    \"\"\"\n    _utils.check_file(path)\n    image_data: nib.Nifti1Image = nib.load(path)  # type: ignore\n    return \"\".join(orientations.aff2axcodes(image_data.affine))\n</code></pre>"},{"location":"user-guide/","title":"User Guide","text":"<p>Here you can find everything you need to install, understand and interact with eva.</p>"},{"location":"user-guide/#getting-started","title":"Getting started","text":"<p>Install eva on your machine and learn how to use eva.</p>"},{"location":"user-guide/#tutorials","title":"Tutorials","text":"<p>To familiarize yourself with eva, try out some of our tutorials.</p> <ul> <li>Go through offline vs. online evaluations to run eva workflows.</li> <li>Train and evaluate a ResNet model from scratch.</li> </ul>"},{"location":"user-guide/#advanced-user-guide","title":"Advanced user guide","text":"<p>Get to know eva in more depth by studying our advanced user guides.</p> <ul> <li>See how to replicate our evaluations of public FM-checkpoints.</li> <li>Learn how to access models in eva's FM backbone model registry.</li> <li>Understand how to use eva's model wrapper API to load models from different formats and sources.</li> <li>Running evaluations using custom models &amp; checkpoints.</li> </ul>"},{"location":"user-guide/advanced/custom_checkpoints/","title":"Evaluations with custom models &amp; checkpoints","text":"<p>The <code>.yaml</code> evaluation config files that eva provides out of the box support loading models from eva's model registry through the <code>eva.vision.models.ModelFromRegistry</code> wrapper as described in the Model Wrapper docs.</p> <p>For evaluating your own custom models &amp; checkpoints, the most flexible way is to create your own set of configs starting from the default ones and replacing the <code>models:</code> section in the <code>.yaml</code> file.</p> <p>However, if your model can be loaded using <code>timm</code>, there is a quicker way using the default configuration files: <pre><code>MODEL_NAME=timm/vit_small_patch16_224 \\\nMODEL_EXTRA_KWARGS='{checkpoint_path: path/to/model.ckpt}' \\\neva predict_fit --config configs/vision/pathology/offline/segmentation/consep.yaml\n</code></pre></p> <p>Note that <code>MODEL_NAME</code> in the above example refers to a wrapper model function in eva's model registry which calls <code>timm.create_model</code> and therefore can load any <code>timm</code> model, while <code>MODEL_EXTRA_KWARGS.model_name</code> refers to the name of the model in timm`s model registry to be loaded.</p>"},{"location":"user-guide/advanced/model_registry/","title":"Backbone Model Registry","text":"<p>eva contains a model registry that provides the most popular FM backbones that are publicly available and which we list in the Leaderboard.</p>"},{"location":"user-guide/advanced/model_registry/#loading-models-through-the-python-api","title":"Loading models through the Python API","text":"<p>The available models can be listed as follows after installing the eva package: <pre><code>from eva.vision.models.networks.backbones import BackboneModelRegistry\n\nmodels = BackboneModelRegistry.list_models()\nprint(models)\n</code></pre></p> <p>This should output a list of the model names such as: <pre><code>['universal/vit_small_patch16_224_random', 'pathology/kaiko_vits16', 'pathology/kaiko_vits8', ...]\n</code></pre></p> <p>A model can then be loaded and instantiated like this:</p> <pre><code>import torch\nfrom eva.vision.models.networks.backbones import BackboneModelRegistry\n\nmodel = BackboneModelRegistry.load_model(\n    model_name=\"universal/vit_small_patch16_224_random\",\n     **{\"out_indices\": 2}\n)\noutput = model(torch.randn(1, 3, 224, 224))\nprint(output.shape)\n# console output:\n# &gt; torch.Size([1, 384])\n</code></pre> <p>In the above example, we load a vit-s model initialized with random weights. The <code>output</code> tensor corresponds to the CLS embedding which for this backbone is a one dimensional tensor of dimension <code>384</code>. For segmentation tasks, we need to access not only the CLS embedding, but entire feature maps. This we can achieve by using the <code>out_indices</code> argument:</p> <pre><code>model = BackboneModelRegistry.load_model(\n    model_name=\"universal/vit_small_patch16_224_random\",\n     **{\"out_indices\": 2}\n)\noutputs = model(torch.randn(1, 3, 224, 224))\nfor output in outputs:\n    print(output.shape)\n# console output:\n# &gt; torch.Size([1, 384, 14, 14])\n# &gt; torch.Size([1, 384, 14, 14])\n</code></pre> <p>The above example returns a <code>list</code> of 4D tensors, each representing the feature map from a different level in the backbone. <code>out_indices=2</code> means that it returns the last two feature maps. This also supports tuples, for instance <code>(-2, -4)</code> returns the penultimate and the forth before the last maps.</p>"},{"location":"user-guide/advanced/model_registry/#run-evaluations-using-backbones-from-the-registry","title":"Run evaluations using backbones from the registry","text":"<p>In the default <code>.yaml</code> config files that eva provides, the backbone is specified as follows:</p> <pre><code>backbone:\n  class_path: eva.vision.models.ModelFromRegistry\n  init_args:\n    model_name: ${oc.env:MODEL_NAME, universal/vit_small_patch16_224_dino}\n    model_kwargs:\n      out_indices: ${oc.env:OUT_INDICES, 1}\n</code></pre> <p>Note that <code>ModelFromRegistry</code> is a model wrapper class, which loads the models through <code>BackboneModelRegistry</code>.</p> <p>By using the <code>MODEL_NAME</code> environment variable, you can run an evaluation with a specific model from the registry, without modifying the default config files: <pre><code>MODEL_NAME=pathology/kaiko_vits16 \\\neva predict_fit --config configs/vision/pathology/offline/segmentation/consep.yaml\n</code></pre></p>"},{"location":"user-guide/advanced/model_registry/#adding-new-models-to-the-registry","title":"Adding new models to the registry","text":"<p>If you want to add a new FM backbone to eva's registry, you'll need to follow these steps:</p> <ol> <li> <p>Implement a Python function that returns your model as a <code>torch.nn.Module</code>. If it's not a native PyTorch model, or if you have made the model already available in public hubs such as torch.hub or huggingface, our model wrapper classes might come in handy.</p> </li> <li> <p>Add your model function to <code>eva.vision.models.networks.backbones</code> together with a <code>@register_model(\"your_model_name\")</code> decorator. Then add an import statement to the <code>__init__</code> file of the corresponding module.</p> </li> <li> <p>Open a PR \ud83d\ude80</p> </li> </ol>"},{"location":"user-guide/advanced/model_wrappers/","title":"Model Wrappers","text":"<p>This document shows how to use eva's Model Wrapper API (<code>eva.models.wrappers</code>) to load different model formats from a series of sources such as PyTorch Hub, HuggingFace Model Hub and ONNX.</p>"},{"location":"user-guide/advanced/model_wrappers/#eva-model-registry","title":"eva model registry","text":"<p>To load models from eva's FM backbone model registry, we provide the <code>ModelFromRegistry</code> wrapper class:</p> <p><pre><code>backbone:\n  class_path: eva.vision.models.wrappers.ModelFromRegistry\n  init_args:\n    model_name: universal/vit_small_patch16_224_dino\n    model_kwargs:\n      out_indices: 1\n</code></pre> The above example loads a vit-s model with weights pretrained on imagenet-1k. Note that by specifying the <code>out_indices=1</code> keyword argument, the model will return a feature map tensor, which is needed for segmentation tasks. If you ommit this argument, it will return the CLS embedding (for classification tasks).</p>"},{"location":"user-guide/advanced/model_wrappers/#pytorch-models","title":"PyTorch models","text":"<p>The eva framework is built on top of PyTorch Lightning and thus naturally supports loading PyTorch models. You just need to specify the class path of your model in the backbone section of the <code>.yaml</code> config file.</p> <pre><code>backbone:\n  class_path: path.to.your.ModelClass\n  init_args:\n    arg_1: ...\n    arg_2: ...\n</code></pre> <p>Note that your <code>ModelClass</code> should subclass <code>torch.nn.Module</code> and implement the <code>forward()</code> method to return an embedding tensor of shape <code>[1, embedding_dim]</code> for classification tasks or a list feature maps of shape <code>[1, embedding_dim, patch_dim, patch_dim]</code> for segmentation.</p>"},{"location":"user-guide/advanced/model_wrappers/#models-from-functions","title":"Models from functions","text":"<p>The wrapper class <code>eva.models.wrappers.ModelFromFunction</code> allows you to load models from Python functions that return torch model instances (<code>nn.Module</code>).</p> <p>You can either use this to load models from your own custom functions, or from public providers such as Torch Hub or <code>timm</code> that expose model load functions.</p>"},{"location":"user-guide/advanced/model_wrappers/#torchhubload","title":"<code>torch.hub.load</code>","text":"<p>The following example shows how to load a dino_vits16 model from Torch Hub using the <code>torch.hub.load</code> function: <pre><code>backbone:\n  class_path: eva.models.wrappers.ModelFromFunction\n  init_args:\n    path: torch.hub.load\n    arguments:\n      repo_or_dir: facebookresearch/dino:main\n      model: dino_vits16\n      pretrained: false\n    checkpoint_path: path/to/your/checkpoint.torch\n</code></pre></p> <p>Note that if a <code>checkpoint_path</code> is provided, <code>ModelFromFunction</code> will automatically initialize the specified model using the provided weights from that checkpoint file.</p>"},{"location":"user-guide/advanced/model_wrappers/#timmcreate_model","title":"<code>timm.create_model</code>","text":"<p>Similar to the above example, we can easily load models using the common vision library <code>timm</code>: <pre><code>backbone:\n  class_path: eva.models.wrappers.ModelFromFunction\n  init_args:\n    path: timm.create_model\n    arguments:\n      model_name: resnet18\n      pretrained: true\n</code></pre></p>"},{"location":"user-guide/advanced/model_wrappers/#timm-models","title":"<code>timm</code> models","text":"<p>While you can load <code>timm</code> models using the <code>ModelFromFunction</code> wrapper class as shown in the example above, we also provide a specific wrapper class:</p> <pre><code>backbone:\n  class_path: eva.vision.models.wrappers.TimmModel\n  init_args:\n    model_name: vit_tiny_patch16_224\n    pretrained: true\n    out_indices=1 # to return the last feature map\n    model_kwargs:\n      dynamic_img_size: true  \n</code></pre>"},{"location":"user-guide/advanced/model_wrappers/#huggingface-models","title":"HuggingFace models","text":"<p>For loading models from HuggingFace Hub, eva provides a custom wrapper class <code>HuggingFaceModel</code> which can be used as follows:</p> <pre><code>backbone:\n  class_path: eva.models.wrappers.HuggingFaceModel\n  init_args:\n    model_name_or_path: owkin/phikon\n    tensor_transforms: \n      class_path: eva.models.networks.transforms.ExtractCLSFeatures\n</code></pre> <p>In the above example, the forward pass implemented by the <code>owkin/phikon</code> model returns an output tensor containing the hidden states of all input tokens. In order to extract the state corresponding to the CLS token only (for classification tasks), we can specify a transformation via the <code>tensor_transforms</code> argument which will be applied to the model output. For segmentation tasks, we can use the <code>ExtractPatchFeatures</code> transformation instead to extract patch feature maps instead.</p>"},{"location":"user-guide/advanced/model_wrappers/#onnx-models","title":"ONNX models","text":"<p><code>.onnx</code> model checkpoints can be loaded using the <code>ONNXModel</code> wrapper class as follows:</p> <pre><code>class_path: eva.models.wrappers.ONNXModel\ninit_args:\n  path: path/to/model.onnx\n  device: cuda\n</code></pre>"},{"location":"user-guide/advanced/model_wrappers/#implementing-custom-model-wrappers","title":"Implementing custom model wrappers","text":"<p>You can also implement your own model wrapper classes, in case your model format is not supported by the wrapper classes that eva already provides. To do so, you need to subclass <code>eva.models.wrappers.BaseModel</code> and implement the following abstract methods: </p> <ul> <li><code>load_model</code>: Returns an instantiated model object &amp; loads pre-trained model weights from a checkpoint if available. </li> <li><code>model_forward</code>: Implements the forward pass of the model and returns the output as a <code>torch.Tensor</code> of shape <code>[embedding_dim]</code></li> </ul> <p>You can take the implementations of <code>ModelFromFunction</code>, <code>HuggingFaceModel</code> and <code>ONNXModel</code> wrappers as a reference.</p>"},{"location":"user-guide/advanced/replicate_evaluations/","title":"Replicate evaluations","text":"<p>To produce the evaluation results presented here, you can run eva with the settings below.</p> <p>The <code>.yaml</code> config files for the different benchmark datasets can be found on GitHub. You will need to download the config files and then in the following commands replace <code>&lt;task.yaml&gt;</code> with the name of the config you want to use.</p> <p>Keep in mind:</p> <ul> <li>Some datasets provide automatic download by setting the argument <code>download: true</code> (either modify the <code>.yaml</code> config file or set the environment variable <code>DOWNLOAD=true</code>), while other datasets need to be downloaded manually beforehand. Please review the instructions in the corresponding dataset documentation.</li> <li>The following <code>eva predict_fit</code> commands will store the generated embeddings to the <code>./data/embeddings</code> directory. To change this location you can alternatively set the <code>EMBEDDINGS_ROOT</code> environment variable.</li> </ul>"},{"location":"user-guide/advanced/replicate_evaluations/#pathology-fms","title":"Pathology FMs","text":""},{"location":"user-guide/advanced/replicate_evaluations/#dino-vit-s16-random-weights","title":"DINO ViT-S16 (random weights)","text":"<p>Evaluating the backbone with randomly initialized weights serves as a baseline to compare the pretrained FMs to a FM that produces embeddings without any prior learning on image tasks. To evaluate, run:</p> <pre><code>MODEL_NAME=\"universal/vit_small_patch16_224_random\" \\\nNORMALIZE_MEAN=\"[0.485,0.456,0.406]\" \\\nNORMALIZE_STD=\"[0.229,0.224,0.225]\" \\\nIN_FEATURES=384 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#dino-vit-s16-imagenet","title":"DINO ViT-S16 (ImageNet)","text":"<p>The next baseline model, uses a pretrained ViT-S16 backbone with ImageNet weights. To evaluate, run:</p> <pre><code>MODEL_NAME=\"universal/vit_small_patch16_224_dino\" \\\nNORMALIZE_MEAN=\"[0.485,0.456,0.406]\" \\\nNORMALIZE_STD=\"[0.229,0.224,0.225]\" \\\nIN_FEATURES=384 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#lunit-dino-vit-s16-tcga-1","title":"Lunit - DINO ViT-S16 (TCGA) [1]","text":"<p>Lunit, released the weights for a DINO ViT-S16 backbone, pretrained on TCGA data on GitHub. To evaluate, run:</p> <pre><code>MODEL_NAME=pathology/lunit_vits16\nNORMALIZE_MEAN=\"[0.70322989,0.53606487,0.66096631]\" \\\nNORMALIZE_STD=\"[0.21716536,0.26081574,0.20723464]\" \\\nIN_FEATURES=384 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#lunit-dino-vit-s8-tcga-1","title":"Lunit - DINO ViT-S8 (TCGA) [1]","text":"<pre><code>MODEL_NAME=pathology/lunit_vits8 \\\nNORMALIZE_MEAN=\"[0.70322989,0.53606487,0.66096631]\" \\\nNORMALIZE_STD=\"[0.21716536,0.26081574,0.20723464]\" \\\nIN_FEATURES=384 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#phikon-owkin-ibot-vit-b16-tcga-2","title":"Phikon (Owkin) - iBOT ViT-B16 (TCGA) [2]","text":"<p>Owkin released the weights for \"Phikon\", a FM trained with iBOT on TCGA data, via HuggingFace. To evaluate, run:</p> <pre><code>MODEL_NAME=pathology/owkin_phikon \\\nNORMALIZE_MEAN=\"[0.485,0.456,0.406]\" \\\nNORMALIZE_STD=\"[0.229,0.224,0.225]\" \\\nIN_FEATURES=768 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#phikon-v2-owkin-dinov2-vit-l16-pancan-xl-9","title":"Phikon-v2 (Owkin) - DINOv2 ViT-L16 (PANCAN-XL) [9]","text":"<p>Owkin released the weights for \"Phikon-v2\", a FM trained with DINOv2 on the PANCAN-XL dataset (450M 20x magnification histology images sampled from 60K WSIs), via HuggingFace. To evaluate, run:</p> <pre><code>MODEL_NAME=pathology/owkin_phikon_v2 \\\nNORMALIZE_MEAN=\"[0.485,0.456,0.406]\" \\\nNORMALIZE_STD=\"[0.229,0.224,0.225]\" \\\nIN_FEATURES=1024 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#uni-mahmoodlab-dinov2-vit-l16-mass-100k-3","title":"UNI (MahmoodLab) - DINOv2 ViT-L16 (Mass-100k) [3]","text":"<p>The UNI FM by MahmoodLab is available on HuggingFace. Note that access needs to  be requested.</p> <pre><code>MODEL_NAME=pathology/mahmood_uni \\\nNORMALIZE_MEAN=\"[0.485,0.456,0.406]\" \\\nNORMALIZE_STD=\"[0.229,0.224,0.225]\" \\\nIN_FEATURES=1024 \\\nHF_TOKEN=&lt;your-huggingace-token-for-downloading-the-model&gt; \\\neva predict_fit --config configs/vision/phikon/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#uni2-h-mahmoodlab-dinov2-vit-g14-3","title":"UNI2-h (MahmoodLab) - DINOv2 ViT-G14 [3]","text":"<p>The UNI2-h FM by MahmoodLab is available on HuggingFace. Note that access needs to  be requested.</p> <pre><code>MODEL_NAME=pathology/mahmood_uni2_h \\\nNORMALIZE_MEAN=\"[0.485,0.456,0.406]\" \\\nNORMALIZE_STD=\"[0.229,0.224,0.225]\" \\\nIN_FEATURES=1536 \\\nHF_TOKEN=&lt;your-huggingace-token-for-downloading-the-model&gt; \\\neva predict_fit --config configs/vision/phikon/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#kaikoai-dino-vit-s16-tcga-4","title":"kaiko.ai - DINO ViT-S16 (TCGA) [4]","text":"<p>To evaluate kaiko.ai's FM with DINO ViT-S16 backbone, pretrained on TCGA data  and available on GitHub, run:</p> <pre><code>MODEL_NAME=pathology/kaiko_vits16 \\\nNORMALIZE_MEAN=\"[0.5,0.5,0.5]\" \\\nNORMALIZE_STD=\"[0.5,0.5,0.5]\" \\\nIN_FEATURES=384 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#kaikoai-dino-vit-s8-tcga-4","title":"kaiko.ai - DINO ViT-S8 (TCGA) [4]","text":"<p>To evaluate kaiko.ai's FM with DINO ViT-S8 backbone, pretrained on TCGA data  and available on GitHub, run:</p> <pre><code>MODEL_NAME=pathology/kaiko_vits8 \\\nNORMALIZE_MEAN=\"[0.5,0.5,0.5]\" \\\nNORMALIZE_STD=\"[0.5,0.5,0.5]\" \\\nIN_FEATURES=384 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#kaikoai-dino-vit-b16-tcga-4","title":"kaiko.ai - DINO ViT-B16 (TCGA) [4]","text":"<p>To evaluate kaiko.ai's FM with DINO ViT-B16 backbone, pretrained on TCGA data  and available on GitHub, run:</p> <pre><code>MODEL_NAME=pathology/kaiko_vitb16 \\\nNORMALIZE_MEAN=\"[0.5,0.5,0.5]\" \\\nNORMALIZE_STD=\"[0.5,0.5,0.5]\" \\\nIN_FEATURES=768 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#kaikoai-dino-vit-b8-tcga-4","title":"kaiko.ai - DINO ViT-B8 (TCGA) [4]","text":"<p>To evaluate kaiko.ai's FM with DINO ViT-B8 backbone, pretrained on TCGA data  and available on GitHub, run:</p> <pre><code>MODEL_NAME=pathology/kaiko_vitb8 \\\nNORMALIZE_MEAN=\"[0.5,0.5,0.5]\" \\\nNORMALIZE_STD=\"[0.5,0.5,0.5]\" \\\nIN_FEATURES=768 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#kaikoai-dinov2-vit-l14-tcga-4","title":"kaiko.ai - DINOv2 ViT-L14 (TCGA) [4]","text":"<p>To evaluate kaiko.ai's FM with DINOv2 ViT-L14 backbone, pretrained on TCGA data  and available on GitHub, run:</p> <pre><code>MODEL_NAME=pathology/kaiko_vitl14 \\\nNORMALIZE_MEAN=\"[0.5,0.5,0.5]\" \\\nNORMALIZE_STD=\"[0.5,0.5,0.5]\" \\\nIN_FEATURES=1024 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#h-optimus-0-bioptimus-vit-g14-5","title":"H-optimus-0 (Bioptimus) - ViT-G14 [5]","text":"<p>Bioptimus released their H-optimus-0 which was trained on a collection of 500,000 H&amp;E slides. The model weights were released on HuggingFace.</p> <pre><code>MODEL_NAME=pathology/bioptimus_h_optimus_0 \\\nNORMALIZE_MEAN=\"[0.707223,0.578729,0.703617]\" \\\nNORMALIZE_STD=\"[0.211883,0.230117,0.177517]\" \\\nIN_FEATURES=1536 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#prov-gigapath-dinov2-vit-g14-6","title":"Prov-GigaPath - DINOv2 ViT-G14 [6]","text":"<p>To evaluate the Prov-Gigapath model, available on HuggingFace, run:</p> <pre><code>MODEL_NAME=pathology/prov_gigapath \\\nNORMALIZE_MEAN=\"[0.485,0.456,0.406]\" \\\nNORMALIZE_STD=\"[0.229,0.224,0.225]\" \\\nIN_FEATURES=1536 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#hibou-b-histai-dinov2-vit-b14-1m-slides-7","title":"hibou-B (hist.ai) - DINOv2 ViT-B14 (1M Slides) [7]","text":"<p>To evaluate hist.ai's FM with DINOv2 ViT-B14 backbone, pretrained on a proprietary dataset of one million slides, available for download on HuggingFace, run: </p> <pre><code>MODEL_NAME=pathology/histai_hibou_b \\\nNORMALIZE_MEAN=\"[0.7068,0.5755,0.722]\" \\\nNORMALIZE_STD=\"[0.195,0.2316,0.1816]\" \\\nIN_FEATURES=768 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#hibou-l-histai-dinov2-vit-l14-1m-slides-7","title":"hibou-L (hist.ai) - DINOv2 ViT-L14 (1M Slides) [7]","text":"<p>To evaluate hist.ai's FM with DINOv2 ViT-L14 backbone, pretrained on a proprietary dataset of one million slides, available for download on HuggingFace, run: </p> <pre><code>MODEL_NAME=pathology/histai_hibou_l \\\nNORMALIZE_MEAN=\"[0.7068,0.5755,0.722]\" \\\nNORMALIZE_STD=\"[0.195,0.2316,0.1816]\" \\\nIN_FEATURES=1024 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#virchow2-paigeai-dinov2-vit-h14-31m-slides-8","title":"Virchow2 (paige.ai) - DINOv2 ViT-H14 (3.1M Slides) [8]","text":"<p>To evaluate paige.ai's FM with DINOv2 ViT-H14 backbone, pretrained on a proprietary dataset of 3.1M million slides, available for download on HuggingFace, run:</p> <pre><code>MODEL_NAME=paige/virchow2 \\\nNORMALIZE_MEAN=\"[0.485,0.456,0.406]\" \\\nNORMALIZE_STD=\"[0.229,0.224,0.225]\" \\\nIN_FEATURES=1280 \\\neva predict_fit --config configs/vision/pathology/offline/&lt;task&gt;.yaml\n</code></pre>"},{"location":"user-guide/advanced/replicate_evaluations/#references","title":"References","text":"<p>[1]: Kang, Mingu, et al. \"Benchmarking self-supervised learning on diverse pathology datasets.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.</p> <p>[2]: Filiot, Alexandre, et al. \"Scaling self-supervised learning for histopathology with masked image modeling.\" medRxiv (2023): 2023-07.</p> <p>[3]: Chen: Chen, Richard J., et al. \"A general-purpose self-supervised model for computational pathology.\" arXiv preprint arXiv:2308.15474 (2023).</p> <p>[4]: Aben, Nanne, et al. \"Towards Large-Scale Training of Pathology Foundation Models.\" arXiv preprint arXiv:2404.15217 (2024).</p> <p>[5]: Saillard, et al. \"H-optimus-0\" https://github.com/bioptimus/releases/tree/main/models/h-optimus/v0 (2024).</p> <p>[6]: Xu, Hanwen, et al. \"A whole-slide foundation model for digital pathology from real-world data.\" Nature (2024): 1-8.</p> <p>[7]: Nechaev, Dmitry, Alexey Pchelnikov, and Ekaterina Ivanova. \"Hibou: A Family of Foundational Vision Transformers for Pathology.\" arXiv preprint arXiv:2406.05074 (2024).</p> <p>[8]: Zimmermann, Eric, et al. \"Virchow 2: Scaling Self-Supervised Mixed Magnification Models in Pathology.\" arXiv preprint arXiv:2408.00738 (2024).</p> <p>[9]: Filiot, Alexandre, et al. \"Phikon-v2, A large and public feature extractor for biomarker prediction.\" arXiv preprint arXiv:2409.09173 (2024).</p> <p>[10]: Chen, Richard J., et al. \"Towards a general-purpose foundation model for computational pathology.\" Nature Medicine 30.3 (2024): 850-862.</p>"},{"location":"user-guide/getting-started/how_to_use/","title":"How to use eva","text":"<p>Before starting to use eva, it's important to get familiar with the different workflows, subcommands and configurations.</p>"},{"location":"user-guide/getting-started/how_to_use/#eva-subcommands","title":"eva subcommands","text":"<p>To run an evaluation, we call: <pre><code>eva &lt;subcommand&gt; --config &lt;path-to-config-file&gt;\n</code></pre></p> <p>The eva interface supports the subcommands: <code>predict</code>, <code>fit</code> and <code>predict_fit</code>.</p> <ul> <li><code>fit</code>: is used to train a decoder for a specific task and subsequently evaluate the performance. This can be done online or offline *</li> <li><code>predict</code>: is used to compute embeddings for input images with a provided FM-checkpoint. This is the first step of the offline workflow</li> <li><code>predict_fit</code>: runs <code>predict</code> and <code>fit</code> sequentially. Like the <code>fit</code>-online run, it runs a complete evaluation with images as input.</li> </ul>"},{"location":"user-guide/getting-started/how_to_use/#online-vs-offline-workflows","title":"* online vs. offline workflows","text":"<p>We distinguish between the online and offline workflow:</p> <ul> <li>online: This mode uses raw images as input and generates the embeddings using a frozen FM backbone on the fly to train a downstream head network.</li> <li>offline: In this mode, embeddings are pre-computed and stored locally in a first step, and loaded in a 2nd step from disk to train the downstream head network.</li> </ul> <p>The online workflow can be used to quickly run a complete evaluation without saving and tracking embeddings. The offline workflow runs faster (only one FM-backbone forward pass) and is ideal to experiment with different decoders on the same FM-backbone.</p>"},{"location":"user-guide/getting-started/how_to_use/#run-configurations","title":"Run configurations","text":""},{"location":"user-guide/getting-started/how_to_use/#config-files","title":"Config files","text":"<p>The setup for an eva run is provided in a <code>.yaml</code> config file which is defined with the <code>--config</code> flag.</p> <p>A config file specifies the setup for the trainer (including callback for the model backbone), the model (setup of the trainable decoder) and data module. </p> <p>The config files for the datasets and models that eva supports out of the box, you can find on GitHub. We recommend that you inspect some of them to get a better understanding of their structure and content.</p>"},{"location":"user-guide/getting-started/how_to_use/#environment-variables","title":"Environment variables","text":"<p>To customize runs, without the need of creating custom config-files, you can overwrite the config-parameters listed below by setting them as environment variables.</p> Type Description <code>MODEL_NAME</code> <code>str</code> The name of the backbone model to load from the model registry. (e.g. pathology/kaiko_vitb8) facebookresearch/dino FM is evaluated <code>OUT_INDICES</code> <code>int</code> | <code>tuple[int]</code> | <code>None</code> The indices of the feature maps to select. E.g. <code>1</code> outputs last feature map of the backbone, <code>3</code> outputs the last three feature maps, and <code>(-2, -4)</code> returns the penultimate and the forth before the last maps. Currently this is only used for segmentation tasks. <code>DATA_ROOT</code> <code>str</code> The location of where the datasets will be downloaded to / loaded from during evaluation. <code>DOWNLOAD</code> <code>bool</code> Whether to automatically download the dataset (make sure to review the license of the dataset first and note that not all datasets support this) . <code>OUTPUT_ROOT</code> <code>str</code> The directory to store logging outputs and evaluation results <code>EMBEDDINGS_ROOT</code> <code>str</code> The directory to store the computed embeddings during <code>eva predict</code>. <code>IN_FEATURES</code> <code>int</code> The input feature dimension (embedding) <code>N_RUNS</code> <code>int</code> Number of <code>fit</code> runs to perform in a session, defaults to 5 <code>MAX_STEPS</code> <code>int</code> Maximum number of training steps (if early stopping is not triggered) <code>BATCH_SIZE</code> <code>int</code> Batch size for a training step <code>PREDICT_BATCH_SIZE</code> <code>int</code> Batch size for a predict step <code>LR_VALUE</code> <code>float</code> Learning rate for training the decoder <code>MONITOR_METRIC</code> <code>str</code> The metric to monitor for early stopping and final model checkpoint loading <code>MONITOR_METRIC_MODE</code> <code>str</code> \"min\" or \"max\", depending on the <code>MONITOR_METRIC</code> used <code>REPO_OR_DIR</code> <code>str</code> GitHub repo with format containing model implementation, e.g. \"facebookresearch/dino:main\" <code>TQDM_REFRESH_RATE</code> <code>str</code> Determines at which rate (in number of batches) the progress bars get updated. Set it to 0 to disable the progress bar. <code>N_DATA_WORKERS</code> <code>str</code> How many subprocesses to use for the torch dataloaders. Set to <code>null</code> to use the number of cpu cores. <code>METRICS_DEVICE</code> <code>str</code> Specifies the device on which to compute the metrics. If not set, will use the same device as used for training. <code>CHECKPOINT_TYPE</code> <code>str</code> Set to \"best\" or \"last\", to select which checkpoint to load for evaluations on validation &amp; test sets after training. <code>PATIENCE</code> <code>int</code> Number of checks with no improvement after which training will be stopped (early stopping)."},{"location":"user-guide/getting-started/installation/","title":"Installation","text":"<ul> <li> <p>Create and activate a virtual environment with Python 3.10+</p> </li> <li> <p>Install eva and the eva-vision package with:</p> </li> </ul> <pre><code>pip install \"kaiko-eva[vision]\"\n</code></pre>"},{"location":"user-guide/getting-started/installation/#run-eva","title":"Run eva","text":"<p>Now you are all set and you can start running eva with: <pre><code>eva &lt;subcommand&gt; --config &lt;path-to-config-file&gt;\n</code></pre> To learn how the subcommands and configs work, we recommend you familiarize yourself with How to use eva and then proceed to running eva with the Tutorials.</p>"},{"location":"user-guide/tutorials/evaluate_resnet/","title":"Train and evaluate a ResNet","text":"<p>If you read How to use eva and followed the Tutorials to this point, you might ask yourself why you would not always use the offline workflow to run a complete evaluation. An offline-run stores the computed embeddings and runs faster than the online-workflow which computes a backbone-forward pass in every epoch.</p> <p>One use case for the online-workflow is the evaluation of a supervised ML model that does not rely on a backbone/head architecture. To demonstrate this, let's train a ResNet 18 from PyTorch Image Models (timm).</p> <p>To do this we need to create a new config-file:</p> <ul> <li>Create a new folder: <code>configs/vision/resnet18</code></li> <li>Create a copy of <code>configs/vision/dino_vit/online/bach.yaml</code> and move it to the new folder.</li> </ul> <p>Now let's adapt the new <code>bach.yaml</code>-config to the new model:</p> <ul> <li>remove the <code>backbone</code>-key from the config. If no backbone is specified, the backbone will be skipped during inference.</li> <li>adapt the model-head configuration as follows:</li> </ul> <p><pre><code>     head:\n      class_path: eva.models.ModelFromFunction\n      init_args:\n        path: timm.create_model\n        arguments:\n          model_name: resnet18\n          num_classes: &amp;NUM_CLASSES 4\n          drop_rate: 0.0\n          pretrained: false\n</code></pre> To reduce training time, let's overwrite some of the default parameters. Run the training &amp; evaluation with: <pre><code>OUTPUT_ROOT=logs/resnet/bach \\\nMAX_STEPS=50 \\\nLR_VALUE=0.01 \\\neva fit --config configs/vision/resnet18/bach.yaml\n</code></pre> Once the run is complete, take a look at the results in <code>logs/resnet/bach/&lt;session-id&gt;/results.json</code> and check out the tensorboard with <code>tensorboard --logdir logs/resnet/bach</code>. How does the performance compare to the results observed in the previous tutorials?</p>"},{"location":"user-guide/tutorials/offline_vs_online/","title":"Offline vs. online evaluations","text":"<p>In this tutorial we run eva with the three subcommands <code>predict</code>, <code>fit</code> and <code>predict_fit</code>, and take a look at the difference between offline and online workflows.</p>"},{"location":"user-guide/tutorials/offline_vs_online/#before-you-start","title":"Before you start","text":"<p>If you haven't downloaded the config files yet, please download them from GitHub.</p> <p>For this tutorial we use the BACH classification task which is available on Zenodo and is distributed under Attribution-NonCommercial-ShareAlike 4.0 International license.</p> <p>To let eva automatically handle the dataset download, set <code>download: true</code> in <code>configs/vision/pathology/offline/classification/bach.yaml</code> (you may also enable automatic download by setting the environment variable <code>DOWNLOAD=true</code>). Additionally, you can set <code>DATA_ROOT</code> to configure the location of where the dataset will be downloaded to / loaded from during evaluation (the default is <code>./data</code> which will be used in the following examples).</p> <p>Before doing so, please make sure that your use case is compliant with the dataset license. Note that not all datasets support automatic download.</p>"},{"location":"user-guide/tutorials/offline_vs_online/#offline-evaluations","title":"Offline evaluations","text":""},{"location":"user-guide/tutorials/offline_vs_online/#1-compute-the-embeddings","title":"1. Compute the embeddings","text":"<p>First, let's use the <code>predict</code>-command to download the data and compute embeddings. In this example we use a randomly initialized <code>dino_vits16</code> as backbone.</p> <p>Open a terminal in the folder where you installed eva and run:</p> <pre><code>MODEL_NAME=universal/vit_small_patch16_224_random \\\nEMBEDDINGS_ROOT=./data/embeddings/dino_vits16_random \\\neva predict --config configs/vision/pathology/offline/classification/bach.yaml\n</code></pre> <p>Executing this command will:</p> <ul> <li>Download and extract the BACH dataset to <code>./data/bach</code> (if it has not already been downloaded to this location). This will take a few minutes.</li> <li>Compute the embeddings for all input images with the specified FM-backbone and store them in the <code>EMBEDDINGS_ROOT</code> along with a <code>manifest.csv</code> file.</li> </ul> <p>Once the session is complete, verify that:</p> <ul> <li>The raw images have been downloaded to <code>./data/bach/ICIAR2018_BACH_Challenge</code></li> <li>The embeddings have been computed and are stored in <code>$EMBEDDINGS_ROOT/$MODEL_NAME/bach</code></li> <li>The <code>manifest.csv</code> file that maps the filename to the embedding, target and split has been created in the same embeddings directory.</li> </ul>"},{"location":"user-guide/tutorials/offline_vs_online/#2-evaluate-the-fm","title":"2. Evaluate the FM","text":"<p>Now we can use the <code>fit</code>-command to evaluate the FM on the precomputed embeddings.</p> <p>To ensure a quick run for the purpose of this exercise, we overwrite some of the default parameters. Run eva to fit the decoder classifier with:</p> <pre><code>MODEL_NAME=universal/vit_small_patch16_224_random \\\nEMBEDDINGS_ROOT=./data/embeddings/dino_vits16_random \\\nN_RUNS=2 \\\nMAX_STEPS=20 \\\nLR_VALUE=0.1 \\\neva fit --config configs/vision/pathology/offline/classification/bach.yaml\n</code></pre> <p>Executing this command will:</p> <ul> <li>Fit a downstream head (single layer MLP) on the BACH-train split, using the computed embeddings and provided labels as input.</li> <li>Evaluate the trained model on the validation split and store the results.</li> </ul> <p>Once the session is complete:</p> <ul> <li>Check the evaluation results in <code>logs/$MODEL_NAME/offline/bach/&lt;session-id&gt;/results.json</code>. (The <code>&lt;session-id&gt;</code> consists of a timestamp and a hash that is based on the run configuration.)</li> <li>Take a look at the training curves with the Tensorboard. Open a new terminal, activate the environment and run: <pre><code>tensorboard --logdir logs/$MODEL_NAME/offline/bach\n</code></pre></li> </ul>"},{"location":"user-guide/tutorials/offline_vs_online/#3-run-a-complete-offline-workflow","title":"3. Run a complete offline-workflow","text":"<p>With the <code>predict_fit</code>-command, the two steps above can be executed with one command. Let's do this, but this time let's use an FM pretrained from ImageNet.</p> <p>Go back to the terminal and execute: <pre><code>MODEL_NAME=universal/vit_small_patch16_224_dino \\\nEMBEDDINGS_ROOT=./data/embeddings/dino_vits16_imagenet \\\nN_RUNS=2 \\\nMAX_STEPS=20 \\\nLR_VALUE=0.1 \\\neva predict_fit --config configs/vision/pathology/offline/classification/bach.yaml\n</code></pre></p> <p>Once the session is complete, inspect the evaluation results as you did in Step 2. Compare the performance metrics and training curves. Can you observe better performance with the ImageNet pretrained encoder?</p>"},{"location":"user-guide/tutorials/offline_vs_online/#online-evaluations","title":"Online evaluations","text":"<p>Alternatively to the offline workflow from Step 3, a complete evaluation can also be computed online. In this case we don't save and track embeddings and instead fit the ML model (encoder with frozen layers + trainable decoder) directly on the given task.</p> <p>As in Step 3 above, we again use a <code>dino_vits16</code> pretrained from ImageNet. </p> <p>Run a complete online workflow with the following command: <pre><code>MODEL_NAME=universal/vit_small_patch16_224_dino \\\nN_RUNS=1 \\\nMAX_STEPS=20 \\\nLR_VALUE=0.1 \\\neva fit --config configs/vision/pathology/online/classification/bach.yaml\n</code></pre></p> <p>Executing this command will:</p> <ul> <li>Fit a complete model - the frozen FM-backbone and downstream head - on the BACH-train split. (The download step will be skipped if you executed Step 1 or 3 before.)</li> <li>Evaluate the trained model on the val split and report the results</li> </ul> <p>Once the run is complete:</p> <ul> <li>Check the evaluation results in <code>logs/$MODEL_NAME/online/bach/&lt;session-id&gt;/results.json</code> and compare them to the results of Step 3. Do they match?</li> <li>You might have noticed that the online run took considerably longer than the offline run. That's because in the offline mode we compute the embeddings only once in the <code>predict</code> step and then store them to disk, while in online mode we calculate them in every training epoch of the evaluation again.</li> </ul>"}]}