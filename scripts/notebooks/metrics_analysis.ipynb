{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from eva.core.models.wrappers import _utils\n",
    "from eva.vision.callbacks.loggers.batch.segmentation import _draw_semantic_mask, _overlay_mask\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional\n",
    "from eva.core import metrics\n",
    "from lightning_fabric.utilities import cloud_io\n",
    "from eva.vision.models.networks.decoders.segmentation import ConvDecoderMS, PVTFormerDecoder\n",
    "from eva.vision.models import modules, wrappers\n",
    "from eva.vision.models.networks import adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/mnt/localdisk/data/eva/models/consep/uni_convdecoder.ckpt\"\n",
    "model_1 = modules.SemanticSegmentationModule(\n",
    "    encoder=None,\n",
    "    decoder=ConvDecoderMS(in_features=1024, num_classes=5),\n",
    "    criterion=None\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "model_1.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model_1.encoder = wrappers.ModelFromRegistry(\n",
    "    model_name=\"pathology/mahmood_uni\",\n",
    "    model_kwargs={\"out_indices\": 1, \"hf_token\": hf_token}\n",
    ")\n",
    "\n",
    "_ = model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/mnt/localdisk/data/eva/models/consep/uni_adapter_pvtformer_last.ckpt\"\n",
    "\n",
    "model_2 = modules.SemanticSegmentationModule(\n",
    "    encoder=adapters.ViTAdapter(\n",
    "        vit_backbone=wrappers.ModelFromRegistry(\n",
    "                model_name=\"pathology/mahmood_uni\",\n",
    "                model_kwargs={\"hf_token\": hf_token}\n",
    "        ),\n",
    "        deform_num_heads=8,\n",
    "        freeze_vit=True\n",
    "    ),\n",
    "    decoder=PVTFormerDecoder(in_features=[1024]*3, num_classes=5),\n",
    "    criterion=None\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "\n",
    "model_2.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "_ = model_2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eva.vision import datasets\n",
    "from eva.vision.data.wsi.patching import samplers\n",
    "from eva.vision.data import transforms\n",
    "\n",
    "dataset = datasets.CoNSeP(\n",
    "    root=\"/mnt/localdisk/data/datasets/consep\",\n",
    "    split=\"val\",\n",
    "    sampler=samplers.ForegroundGridSampler(max_samples=25),\n",
    "    transforms=transforms.ResizeAndCrop(size=224, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    ")\n",
    "dataset.configure()\n",
    "\n",
    "image, target, metadata = dataset[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = 10\n",
    "MODELS = [model_1, model_2]\n",
    "\n",
    "dice_metric = metrics.GeneralizedDiceScore(num_classes=5, input_format=\"index\", weight_type=\"linear\", per_class=False, include_background=False)\n",
    "\n",
    "def _preprocess_image(image: torch.Tensor):\n",
    "    image = image - image.min()\n",
    "    image = image / image.max()\n",
    "    return image\n",
    "\n",
    "def _one_hot(tensor: torch.Tensor, num_classes: int=5):\n",
    "    return functional.one_hot(tensor, num_classes=5).permute(2, 0, 1)\n",
    "\n",
    "for i in range(N_IMAGES):\n",
    "    image, target, metadata = dataset[i]\n",
    "    image = _preprocess_image(image)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2 + len(MODELS), figsize=(10, 3))\n",
    "    for j, model in enumerate(MODELS):\n",
    "        prediction = model(image.unsqueeze(0), to_size=(224, 224))\n",
    "        prediction = torch.argmax(prediction, dim=1)\n",
    "\n",
    "        image_with_mask = _overlay_mask(image, prediction.squeeze())\n",
    "\n",
    "        fig.axes[0].imshow(image.permute(1, 2, 0))\n",
    "        fig.axes[1].imshow(_draw_semantic_mask(target).permute(1, 2, 0))\n",
    "        fig.axes[2+j].imshow(_draw_semantic_mask(prediction).permute(1, 2, 0))\n",
    "\n",
    "        dice_value = dice_metric(_one_hot(prediction.squeeze()), _one_hot(target))\n",
    "\n",
    "        fig.axes[2+j].set_title(f\"Dice: {dice_value:.2f}\")\n",
    "\n",
    "        for ax in fig.axes:\n",
    "            ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
